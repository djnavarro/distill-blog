---
title: "Data types in Arrow and R" # <---- UPDATE ME
description:
  Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, 
  consectetur, adipisci velit # <---- UPDATE ME
author:
  - first_name: "Danielle"
    last_name: "Navarro"
    url: https://djnavarro.net
    affiliation: Voltron Data
    affiliation_url: https://voltrondata.com
    orcid_id: 0000-0001-7648-6578
date: 2022-01-27
preview: img/cover.jpg  # <---- UPDATE ME 
creative_commons: CC BY
citation_url: https://blog.djnavarro.net/data-types-in-arrow-and-r 
repository_url: https://github.com/djnavarro/distill-blog/
output:
  distill::distill_article:
    self_contained: false
    toc: true
params:
  slug: data-types-in-arrow-and-r
  date: 2022-01-27
  repo: djnavarro/distill-blog
  site: https://blog.djnavarro.net/
---

<!----

checklist:
- check the "update me" messages in YAML above
- initialise the _renv folder with refinery::renv_new("name of post folder")
- populate the lockfile with refinery::renv_snapshot("name of post folder")
- update the _renv folder from snapshot with refinery::restore("name of post folder")

---->


<!--------------- setup post ----------------->

```{r setup, include=FALSE}
tz <- .sys.timezone
.sys.timezone <- "UTC"
knitr::opts_chunk$set(echo = TRUE)
refinery::renv_load(paste(params$date, params$slug, sep = "_"))
```


<!--------------- post ----------------->

> Manuals for translating one language into another can be set up in divergent ways, all compatible with the totality of speech dispositions, yet incompatible with one another <br>
> &nbsp; &nbsp; -- William Van Orman Quine, 1960, [Word and Object](https://en.wikipedia.org/wiki/Word_and_Object)


<!-- 
TODO: 

Insert the point from conversation with Jon. The code the read_* functions is
complicated. It handles everything for you because "reading the data" is a 
constrained task. That means the developers can optimise almost everything. 

In contrast, the code underpinning schema() etc is simple. It takes care of 
choices for you when those choices are "obvious", but it leaves all the edge
cases for you to deal with. There are a lot of weird edge cases because 
"translating between languages" is underdetermined and can only ever be 
approximate. The developers can help make it easier, but the user has to be
the final arbiter, and it helps to understand what each language is "doing". 

-->

Consider this piece of magic

```{r}
library(tibble)
library(dplyr)
library(arrow)

magicians <- read_csv_arrow("magicians.csv")
magicians

arrowmagicks <- arrow_table(magicians)
arrowmagicks
```

```{r, echo=FALSE, message=FALSE}
conflicted::conflict_prefer("timestamp", "arrow")
```

The `magicians` data set is a "data frame" (a tibble, technically) stored in R. The `arrowmagicks` data set, however, is a pointer to a data structure stored in Arrow. That data structure is a "Table" object. Arrow Tables are roughly analogous to data frames -- both represent tabular data with columns that may be of different types -- but they are not the same. An act of *translation* has occurred. Similarly when we look at the individual columns we see that something similar has happened. "Integer" columns in R are mapped to "int32" columns in Arrow, "Date" columns in R become "date32" columns in Arrow, and so on. Even without knowing much about the data types in the two languages, you can infer a lot from the names! In this case you can reasonably (and correctly) infer that the translation from R to Arrow is a sensible one. Better yet, in this case, when we pull the `arrowmagicks` data back into R we recover the original data:

```{r}
collect(arrowmagicks)
```

In this example the translation back and forth "just works". You really don't have to think too much about the subtle differences in how Arrow and R "think about the world" and how their data structures are organised. And in general that's what we *want* in a multi-language toolbox: we want the data analyst to be thinking about the data, not the cross-linguistic subtleties of the data structures! 

## Defining schemas

That being said, it's also valuable to give the data analyst flexibility. The **arrow** package makes very sensible default choices about how to translate an R data structure into an Arrow data structure, but those choices can never be more than defaults because of the fundamental fact that the languages are inherently different. The quote about the [indeterminacy of translation](https://en.wikipedia.org/wiki/Indeterminacy_of_translation) at the top of this post was originally written about natural languages, but I think it applies in programming too. There's no "single" rulebook that tells you how to translate between R and Arrow: there can't be. 

Suppose that I knew that there would in fact be a "Season 5.1648" coming, consisting of a single episode whose title corresponded to the full text of the [Treaty of Westphalia](https://is.muni.cz/el/1423/podzim2008/MVZ430/um/Treaty-of-Westphalia.pdf) or several gigabytes worth of Wikipedia text.^[I mean, if this were a Fiona Apple discography this would be a real possibility right?] Knowing that this new data point is coming, I'd perhaps want my Arrow data to encode `season` as a numeric variable, and I'd probably want to recognise that the text in the `title` field can be quite long. To do that I can specify an explicit *schema* that tells the **arrow** package how to translate my R data set into Arrow. I can do this with the `schema()` function:

```{r}
translation <- schema(
  season = float64(), # not the default
  episode = int32(),
  title = large_utf8(), # not the default
  air_date = date32(),
  rating = float64(),
  viewers = float64()
)
```

Now I can use my schema to govern the translation:

```{r}
arrowmagicks2 <- arrow_table(magicians, schema = translation)
arrowmagicks2
```

This is of course a very silly example. But the underlying issue is fairly serious. 

## Why mapping languages is hard

Blah blah I used to teach categorisation and mental representation. Organising the world into concepts (or data structures) is hard. We define ontologies that impose order on a chaotic world. A famous attempt to do this in 1668 by John Wilkins in [An Essay Towards a Real Character, and a Philosophical Language](https://www.google.com.au/books/edition/_/BCCtZjBtiEYC?hl=en&gbpv=1). A small snippet taken from the section "On Beasts":

> BEASTS, may be distinguished by their several shapes, properties, uses, food, their tameness or wildness, etc. into such as are either
> 
> - VIVIPAROUS; producing living young.
> 
>     - WHOLE FOOTED, the *soles* of whose *feet* is are undivided, being used chiefly for *Carriage*. I.
>     - CLOVEN FOOTED. II. <br>
>       *Clawed*, or *multifidous*; the end of whose *feet* is branched out into *toes*; whether
>       
>         - NOT RAPACIOUS. III.
>         - RAPACIOUS; living upon the prey of other *Animals*; having generally *six short pointed* incisors, or *cutting teeth*, and *two long fangs* to hold their prey; whether the
>         
>             - CAT-KIND; having a *roundish head*. IV.
>             - DOG-KIND; whose *heads* are *more oblong*. V.
>             
> - OVIPAROUS; breeding *Eggs*. VI.

It goes on to elaborate. As a scientific taxonomy it's... interesting. But even psychologically it betrays the peculiarities of the author. Wilkins had the noble ambition of defining a "universal language", but as a 17th century English gentleman there are certain deficiencies! I note as an Australian that there's no distinction between placental and marsupial mammals here! But of course the problems are more general. In 1952 the Argentinian author Jorge Luis Borges published an essay called [The Analytical Language of John Wilkins](https://ccrma.stanford.edu/courses/155/assignment/ex1/Borges.pdf) and describes a classification system from an fictitious "Celestial Emporium of Benevolent Knowledge":

> In its remote pages it is written that the animals are divided into: (a) belonging to the emperor, (b) embalmed, (c) tame, (d) sucking pigs, (e) sirens, (f) fabulous, (g) stray dogs, (h) included in the present classification, (i) frenzied, (j) innumerable, (k) drawn with a very fine camelhair brush, (l) et cetera, (m) having just broken the water pitcher, (n) that from a long way off look like flies

It's pretty unlikely that any human language would produce a classification system quite as chaotic as Borges' fictional example, but the point is well made. Actual classification systems used in different languages and cultures are very different to one another and often feel very alien when translated. Michel Foucault actually refers to this Borges passage in the preface to his famous work [The Order of Things: An Archaeology of the Human Sciences](https://en.wikipedia.org/wiki/The_Order_of_Things) on how different cultures and historical periods viewed the world from fundamentally different perspectives. According to Foucault, Borges essay 

> shattered ... all the familiar landmarks of thought --- our thought, the thought that bears the stamp of our age and our geography --- breaking up all the ordered surfaces and all the planes with which we are accustomed to tame the wild profusion of existing things



## Translating data types

So how do we construct data structures in Arrow that are appropriate translations of data structures in R, and vice versa? For the moment, let's keep things simple and assume that the mapping we're interested in has the same format as the one described at the start of the post: in R we want to encode the data as a data frame, and in Arrow we want to encode it as a Table. If so, the translation question is really a matter of how we encode the columns.

In the rest of this post I'll use a lot of diagrams showing the default mappings that the **arrow** package uses when converting data columns from R to Arrow and vice versa. In each case I'll show R data types on the left hand side (against a blue background) and Arrow data types on the right hand side (against an orange background), as shown below:

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Illustration of the graphical convention used in the later   diagrams, showing R on the left side (against a blue background) and Arrow on the right side (against an orange background)."

knitr::include_graphics("img/visual-convention.png", dpi = 100)
```

<aside><br>Instead of using alt-text, I've included verbose and descriptive figure captions that verbally describe the content of each diagram</aside>

## Logical types

<!-- 
TODO: add discussion of two-valued and three-valued logics here
-->

I'll start with the simplest possible case: a data frame with one column of logical data. Logical data in R can take on three possible values: `TRUE` and `FALSE` are the two allowed truth values, and `NA` is used to denote missing data. 

```{r}
df <- tibble(values = c(TRUE, FALSE, NA))
df
```

Arrow has an analogous "boolean" type, which has truth values `true` and `false`. Just like R, missing values are allowed, and are represented as `null`. Arrow booleans and R logicals are essentially equivalent to one another, so by default the **arrow** package will map an R logical to an Arrow boolean and vice versa.

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Default mappings for logical types"

knitr::include_graphics("img/logical-types.png", dpi = 100)
```

So let's do it:

```{r}
tb <- arrow_table(df)
tb
```

It's mildly annoying this doesn't print out the actual values. Happily the **arrow** package supplies a `$` operator for Arrow Table objects so we can do this:

```{r}
tb$values
```

Okay, that makes sense. The "ChunkedArray" bit might seem a little unfamiliar to R users -- and I'll come back to it later -- but for now we can imagine it's something similar to an R vector. 

This also tells us a little more about the relationship between R data frames and Arrow Tables. In R, a data frame is a collection of equal-length vectors; in Arrow, a Table is a collection of equal-length chunked arrays. This is convenient for me because I can use the `chunked_array()` function for the rest of this section. Instead of going through that elaborate process of creating a tibble and then calling `arrow_table()` I could have just done this:

```{r}
values <- c(TRUE, FALSE, NA)
chunked_array(values)
```


<!-- 
TODO: foreshadow that the NA, null thing will be explained later
-->

In fact, I could have done this in an even more primitive way and passed a scalar value to Arrow. That's not something you usually find yourself needing to do, so the **arrow** package doesn't have an equivalent of `chunked_array()` that applies to scalars. However, most of the heavy lifting for `chunked_array()` is actually being done by the R6 object `ChunkedArray`.^[In general R6 classes use UpperCamelCase, and the snake_case functions with the same name provide an interface to the R6 class that feels more familiar to an R user.] There is a `Scalar` R6 object that we can use to import scalars:

```{r}
Scalar$create(TRUE, type = boolean())
```

In the same way that `chunked_array()` is an alias for `ChunkedArray$create()`, for the purpose of this post I'm going to define `scalar()` as an alias for `Scalar$create()`

```{r}
scalar <- function(x, type = NULL) {
  Scalar$create(x, type)
}

scalar(TRUE, type = boolean())
```


## Integer types

Integers are a little more complicated: where base R provides just the one integer type, Arrow provides eight. 

To make sense of the different types, it helps to take a moment to think about how integers are represented in a binary format. Let's suppose we allocate 8 bits to specify an integer. If we do that, then there are $2^8 = 256$ unique binary patterns we can create with these bits. Because of this, there is a fundamental constraint: no matter how we choose to set it up, 8-bit integers can only represent 256 distinct numbers. Technically, we could choose any 256 numbers we like, but in practice there are only two schemes used for 8-bit integers: *unsigned* 8-bit integers ("uint8") use those bits to represent integers from 0 to 255, whereas *signed* 8-bit integers ("int8") can represent integers from -128 to 127. 

<!-- 
TODO:

Mathematically: 
signed = the integers, Z
unsigned = the naturals, N
-->

More generally, an unsigned n-bit integer can represent integers from 0 to $2^n - 1$, whereas a signed n-bit integer can represent integers from $-2^{n-1}$ to $2^{n-1} - 1$. Here's what that looks like for all the integer types supported by Arrow:

| Description     | Name   | Smallest Value       |        Largest Value |
| --------------- | -----: | -------------------: | -------------------: |
| 8 bit unsigned  | uint8  | 0                    |                  255 |
| 16 bit unsigned | uint16 | 0                    |                65535 |
| 32 bit unsigned | uint32 | 0                    |           4294967295 |
| 64 bit unsigned | uint64 | 0                    | 18446744073709551615 |
| 8 bit signed    | int8   | -128                 |                  127 |
| 16 bit signed   | int16  | -32768               |                32767 |
| 32 bit signed   | int32  | -2147483648          |           2147483647 |
| 64 bit signed   | int64  | -9223372036854775808 |  9223372036854775807 |

On the R side, the integer type supplied by base R is a 32 bit signed integer, and has a natural one-to-one mapping to the Arrow int32 type. Because of this, the **arrow** default is to convert an R integer to an Arrow int32 and vice versa. 

What about the other seven Arrow types? This is where it gets a little trickier. The table above illustrates that some integer types are fully contained within others: unsurprisingly, every number representable by int16 can also be represented by int32, so we can say that the int16 numbers are fully "contained" by (i.e. are a proper subset of) the int32 numbers. Similarly, uint16 is contained by unit32. There are many cases where an unsigned type is contained by a signed type: for instance, int32 contains all the uint16 numbers. However, because the unsigned integers cannot represent negative numbers, the reverse is never true. So we can map out the relationships between the different types like this: 

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Containment relationships between the integer types."

knitr::include_graphics("img/integer-types-03.png", dpi = 350)
```

Whenever type A contains type B, it's possible to transform an object of type B into an object of type A without losing information or requiring any special handling. R integers are 32 bit signed integers, which means it's possible to convert Arrow data of types int32, int16, int8, uint16, and uint8 to R integers completely painlessly. So for these data types the **arrow** defaults give us this relationship:

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Default mappings for some integer types"

knitr::include_graphics("img/integer-types-01.png", dpi = 100)
```

Other integer types are messier. To keep things nice and simple, what we'd *like* to do is to map the Arrow uint32, uint64, and int64 types onto the R integer type. Sometimes that's possible: if all the stored values fall within the range of values representable by R integers (i.e., are between -2147483648 and 2147483647) then we can do this, and that's what **arrow** does by default. However, if there are values that "overflow" this range, then **arrow** will import the data as a different type, as shown below:

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Default mappings for other integer types"

knitr::include_graphics("img/integer-types-02.png", dpi = 100)
```

If this diagram looks ugly and unhelpful, well... yeah, I agree. Translations become messy when the boxes in one language don't quite match up to the content expressed in another. Sometimes it's just easier to see the system in action, so let's write a little helper function:

```{r}
translate_integer <- function(value, type) {
  value_arrow <- scalar(value, type)
  value_r <- value_arrow$as_vector()
  return(c(
    arrow = value_arrow$type$name,
    r = class(value_r)
  ))
}
```

The `translate_integer()` function takes an integer `value` and an `type` as input, and it returns a vector that tells you what Arrow type was created (this should be identical to `type`), and what R class gets returned when we import that Arrow object back into R. 

To illustrate, let's encode the number 10 as an unsigned 8-bit integer in Arrow and then see what happens when that gets pulled back into R. This is an easy case because uint8 always maps back to integer:

```{r}
translate_integer(value = 10, type = uint8())
```

Next, let's see how **arrow** handles the three types that are a little awkward for R to represent as integers. First, let's see what happens when all the values are small enough that R actually *can* represent them as integers:

```{r}
translate_integer(value = 10, type = uint32())
translate_integer(value = 10, type = uint64())
translate_integer(value = 10, type = int64())
```

Okay, that makes sense. If the numbers *can* be represented using the R integer class then that's what **arrow** will do. 

Now let's increase the number to something that R cannot represent as an integer, and see what happens:

```{r}
translate_integer(value = 3000000000, type = uint32())
translate_integer(value = 3000000000, type = uint64())
translate_integer(value = 3000000000, type = int64())
```

The last result might be a little surprising to many R users. The integer64 class is supplied by the **bit64** package, and provides an R class that behaves like 64-bit integers in R. 

Just to complicate matters, there's a special option for uint64 that you can use to force **arrow** to always return integer64:

```{r}
options(arrow.int64_downcast = FALSE)
translate_integer(value = 10, type = int64())
```

Anyway. That's the whole story. Just need to make it coherent!

Before moving on, allow me to clean up state:

```{r}
options(arrow.int64_downcast = NULL)
```



## Numeric types

In the last section I talked about the rather extensive range of data types that Arrow has to represent integers. Sure, there's a practical benefit to having all these different data types, but at the same time its wild that we even *need* so many different data structures to represent something so simple. Integers aren't complicated things. We learn them as kids even before we go to school, and we get taught the arithmetic rules to operate on them very early in childhood. 

The problem, though, is that there are *a lot* of integers. It's a tad inconvenient sometimes, but the set of integers is infinite in size,^[Rest assured, dear reader, while I am entirely aware of the distinction between countably and uncountably infinite sets, and have been forced to learn more about the cardinality of transfinite numbers than any human deserves to endure, I will not be discussing any of that in this post. The word "infinite" is perfectly serviceable for our purposes, and if anyone even *tries* to discuss this with me further on twitter I will be forced to engage the services of a very unpleasant lawyer...] so it doesn't matter how many bits you allocate to your "int" type, there will always be integers that your machine cannot represent. But this is obvious, so why am I saying it? Mostly to foreshadow that things get worse when we enter the desert of the reals...

The real numbers correspond to our intuitive concept of the continuous number line. Just like the integers, the real line extends infinitely far in both directions, but unlike the integers the reals are continuous: for any two real numbers -- no matter how close they are to each other -- there is *always* another real number in between. This, quite frankly, sucks. Because the moment you accept that this is true, something ugly happens. If I accept that there must exist a number between 1.01 and 1.02, which I'll call 1.015, then I have to accept that there is a number between 1.01 and 1.015, which I'll call 1.0075, and then I have to accept that... oh shit this is going to go on forever. In other words, the reals have the obnoxious property that there between any two real numbers there are an infinity of other real numbers. 

Try shoving all *that* into your finite-precision machine. 

### Floating point numbers

Stepping away from the mathematics for a moment, most of us already know how programming languages attempt to solve the problem. They use [floating point numbers](https://en.wikipedia.org/wiki/Floating-point_arithmetic) as a crude tool to approximate the real numbers using a finite-precision machine, and it... sort of works, as long as you you never forget that floating point numbers don't always obey the normal rules of arithmetic. I imagine most people reading this post already know this but for those that don't, I'll show you the most famous example:

```{r}
0.1 + 0.2 == 0.3
```

This is not a bug in R. It happens because `0.1`, `0.2`, and `0.3` are *not* real numbers in the mathematical sense. Rather, they are encoded in R as objects of type double, and a double is a 64-bit floating point number that adheres to the [IEEE 754](https://en.wikipedia.org/wiki/IEEE_754) standard. It's a bit beyond the scope of this post to dig into the IEEE standard, but it does help a lot to have a general sense of *how* a floating point number (approximately) encodes a real number. 

To that end, I'll quickly write a `unpack_double()` function that extracts the raw 64-bit binary representation of a numeric value in R, and a print method that will allow me to display the results in a (fairly) clean way: 

```{r}
unpack_double <- function(x) {
  binary <- numToBits(x)    # little-endian binary representation
  structure(
    list(
      sign = as.integer(binary[64]),        # 1-bit sign
      exponent = as.integer(binary[63:53]), # 11-bit exponent
      fraction = as.integer(binary[52:1])  # 52-bit binary fraction
    ),
    class = "unpacked_double"
  )
}

print.unpacked_double <- function(x, ...) {
  sign <- as.character(x$sign)
  exponent <- paste(as.character(x$exponent), collapse = "")
  fraction <- paste(as.character(x$fraction), collapse = "")
  cat(sign, exponent, fraction, "\n")
}
```  

Armed with this, let's take a look at the format. To start out, I'll do the most boring thing possible and show you the binary representation of `0` as a floating point number. You will, I imagine, be entirely unshocked to discover that it is in fact a sequence of 64 zeros:

```{r}
unpack_double(0)
```

Truly amazing. Really, the only thing that matters here is to notice the spacing. The sequence of 64 bits are divided into three meaningful chunks. The first bit^[Sigh. Technically, this is the last bit. R uses a [little endian](https://en.wikipedia.org/wiki/Endianness) representation here, but for the love of all that is good and precious in this world please let me simplify a few things okay?] represents the *sign*: is this a positive number (first bit equals 0) or a negative number (first bit equals 1), where zero is treated as if it were a positive number. The next 11 bits are used to specify an *exponent*: you can think of these bits as if they describe a signed "int11" type, and can be used to store any number between -1022 and 1023.^[Okay, if you were reading closely earlier you might be thinking this is wrong and the range should be -1023 to 1024. The reason it's not is that those to values are reserved for "special" numbers.]. The remaining 53 bits are used to represent a binary fraction. 

To see this in action, let's see how `16` and `-8` are encoded. These are both whole numbers in binary, and they're both powers of two. One is positive and the other is negative, so you can see that the sign bit is different; and they're different powers of two so you can see that the exponent is different. But since they're both *exact* powers of 2 (in the same way that 10, 100, and 1000 are all exact powers of 10), there's nothing in the fraction component at all:

```{r, results='hold'}
unpack_double(16)
unpack_double(-8)
```

In a binary representation these are "round numbers". The same is true for numbers like `.25` and `.0625` which are exact *negative* powers of 2 (i.e. $2^{-2} = .25$ and $2^{-4} = .0625$). These are also round numbers in a binary encoding:

```{r, results='hold'}
unpack_double(.25)
unpack_double(.0625)
```

Now look what happens when I try the numbers in the famous example:

```{r, results='hold'}
unpack_double(.1)
unpack_double(.2)
unpack_double(.3)
```

Although these are clean numbers with a simple, finite *decimal* expansion, they are a lot messier in binary! Because of that, these numbers aren't represented exactly, and truncation errors occur. The errors are very small, but they exist:

```{r}
0.1 + 0.2 - 0.3
```

I promise there is a point to this story, honest, and it's about to come back around to the "data structures in R versus Arrow" topic.

No, really!


<!-- 
TODO:
Mathematically: floating points as approximations to the reals. uncountable infinity is a bitch
-->



```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Default mappings for numeric types"

knitr::include_graphics("img/numeric-types.png", dpi = 100)
```


## Character types

Strings are an interesting case. R uses a single data type to represent strings (character vectors) but Arrow has two types, known as strings and large strings. When using the **arrow** package, Arrow strings are specified using the `utf8()` function, and large strings correspond to the `large_utf8()` type. The default mapping is to assume that an R character vector maps onto the Arrow `utf8()` type, as shown below:

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Default mappings for character types"
knitr::include_graphics("img/character-types.png", dpi = 100)
```

There's a little more than meets the eye here though, and you might be wondering about the difference between "strings" and "large strings" in Arrow, and when you might prefer one to the other. As you might expect, the large string type is suitable when you're storing large amounts of text, but to understand it properly I need to talk in more depth about how R and Arrow store strings, and I'll use this partial list of people that -- according to the lyrics of [Jung Talent Time](https://genius.com/Tism-jung-talent-time-lyrics) by TISM -- were perhaps granted slightly more fame than they had earned on merit

```
Bert Newton
Billy Ray Cyrus
Warwick Capper
Uri Geller
Samantha Fox
```

### [R] The character class

Suppose I want to store this as a character vector in R, storing only the family names for the sake of brevity and visual clarity. 

```{r}
jung_talent <- c("Newton", "Cyrus", "Capper", "Geller", "Fox")
```

Each element of the `jung_talent` vector is a variable-length string, and is stored internally by R as an array of individual characters^[As noted in the R internals manual, the specific data structure is referred to as a [CHARSXP](https://cran.r-project.org/doc/manuals/r-release/R-ints.html#Encodings-for-CHARSXPs). For the purposes of the current post I'm pretending that character strings are always encoded as UTF-8 because there's no need to complicate matters by talking about things like Latin-1, but be aware that R does support those things. If you're looking for a good overview of what UTF-8 encoding is all about, this blog post on [how unicode works](https://deliciousbrains.com/how-unicode-works/) is helpful.] So, to a first approximation, your mental model of how R stores the `jung_talent` variable might look something like this:

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Simplified representation of how character vectors are represented in R"
knitr::include_graphics("img/character-types-02.png", dpi = 100)
```

Here, the `jung_talent` variable is an object^[More strictly, it is a symbol that points to an object. R makes a distinction between the symbol and the object to which it links, and allows multiple labels to point at the same object. There's an excellent discussion of this in [Chapter 3 of Advanced R](https://adv-r.hadley.nz/names-values.html)] that contains five elements shown as the orange boxes. Internally, each of those orange boxes is itself an array of individual characters shown as the purple boxes. As a description of what R actually does this is a bit of an oversimplification because it ignores the [global string pool](https://adv-r.hadley.nz/names-values.html#character-vectors), but it will be sufficient for the current purposes. 

The key thing to understand conceptually is that R treats the elements of a character vector as the fundamental unit. The `jung_talent` vector is constructed from five distinct strings, `"Newton"`, `"Cyrus"`, etc. The `"Newton"` string is assigned to position 1, the `"Cyrus"` string is assigned to position 2, and so on. 

### [Arrow] The string (utf8) type

The approach taken in Arrow is rather different. Instead of carving up the character vector into strings (and internally treating the strings as character arrays), it concatenates everything into one long buffer. The text itself is dumped into one long string, like this:

```
NewtonCyrusCapperGellerFox
```

The first element of this buffer -- the letter `"N"` -- is stored at "offset 0" (indexing in Arrow starts at 0), the second element is stored at offset 1, and so on. This long array of text is referred to as the data buffer, and it does not specify where the boundaries between array elements are. Those are stored separately. If I were to create an Arrow string array called `jung_talent_arrow`, it would be comprised of a data buffer, and an "offset buffer" that specifies the positions at which each element of the string array begins. In other words, we'd have a mental model that looks a bit like this:

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Simplified representation of how character vectors are represented in Arrow"
knitr::include_graphics("img/character-types-03.png", dpi = 150)
```

How are each of these buffers encoded?

- The contents of the data buffer are stored as UTF-8 text, which is itself a variable length encoding: some characters are encoded using only 8 bits while others require 32 bits. This [blog post on unicode](https://deliciousbrains.com/how-unicode-works/) is a nice explainer.

- The contents of the offset buffer are stored as unsigned integers, either 32 bit or 64 bit, depending on which of the two Arrow string array types you're using. I'll unpack this in the next section.

Now that we've got a sense of the differences between how R and Arrow encode character data, we're almost ready move some text from R to Arrow. But before we do that, there's a little terminology to explain. A little confusingly, there's some inconsistency in how the types are referred to in the Arrow C++ documentation and the **arrow** R package documentation. In the [list of data types](https://arrow.apache.org/docs/cpp/api/datatype.html) for the underlying C++ library you'll find references to "string" types and "large_string". However, in [list of data types](https://arrow.apache.org/docs/r/reference/data-type.html) documentation for the **arrow** R package you'll see the same data types referred to as "utf8" and "large_utf8". These refer to the same thing. What **arrow** calls "large_utf8" is the same thing as what Arrow C++ calls a "large_string". The reason why **arrow** uses a different term is simply because **arrow** is an R package, and the word "string" already means something subtly different in R.

Sheesh. That was long. Let's give ourselves a small round of applause for surviving, and now actually *do* something. We'll port the `jung_talent` vector over to Arrow. 

```{r}
jung_talent_arrow <- chunked_array(jung_talent)
jung_talent_arrow
jung_talent_arrow$type
```

### [Arrow] The large_string (large_utf8) type

okay, so why do I care? well, if you're using 32-bit integers to encode the offset, that imposes a limit on the total number of characters that can be included. The total size of the string array is capped at 2GiB because you don't have a way of referring to positions in the string larger than that. if you use 64-bit integers to encode the offset, that limit goes away (I mean technically there is a limit of course because 64-bit is still finite precision but it's *quite* a lot bigger). 


```{r}
jung_talent_arrow_big <- chunked_array(jung_talent, type = large_utf8())
jung_talent_arrow_big
jung_talent_arrow_big$type
```


## Date/time types

<!--
TODO
Quine has a smartarse quote about time being lingustically special
-->


Next up on our tour of data types are dates and times. Internally, R and Arrow both adopt the convention of measuring time in terms of the time elapsed since a specific moment in time known as the [unix epoch](https://en.wikipedia.org/wiki/Unix_time). The unix epoch is the time 00:00:00 UTC on 1 January 1970. It was a Thursday.

Despite agreeing on fundamentals, there are some oddities in the particulars. Base R has three date/time classes (Date, POSIXct, and POSIXlt), and while Arrow also has three date/time classes (date32, date64, and timestamp), the default mappings between them are a little puzzling unless you are deeply familiar with what all these data types are and what they represent. I'll do the deep dive in a moment, but to give you the big picture here's how the mapping works:

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Default mappings for date/time types"
knitr::include_graphics("img/date-types.png", dpi = 100)
```




### [R] The Date class

On the R side of things, a Date object is represented internally as a numeric value, counting the number of days since the unix epoch. Here is today as a Date:

```{r}
today <- Sys.Date()
today
```

If I use `unclass()` to see what it looks like under the hood:

```{r}
unclass(today)
```

Fundamentally, a Date object is just a number: it counts the number of days that have elapsed since a fixed date. It does not care what the year is, what the month is, or what day of the month it is. It does not care how the date is displayed to the user. All those things are supplied by the `print()` method, and are not part of the Date itself.

### [R] The POSIXct class

A date is a comparatively simple thing. When we want to represent dates and time together, we nee we need to know the time of day, and we might need to store information about the timezone as well (more on that later). Base R has two different classes for representing this, POSIXct and POSIXlt. These names used to confuse me a lot. [POSIX](https://en.wikipedia.org/wiki/POSIX) stands for "portable operating system interface", and it's a set of standards used to help operating systems remain compatible with each other. In this context though, it's not very meaningful: all it says "yup we use unix time." 

The more important part of the name is actually the ["ct" versus "lt"](https://stackoverflow.com/questions/44778721/what-do-ct-and-lt-in-posixct-and-posixlt-mean) part. Let's start with POSIXct. The "ct" in POSIXct stands for "calendar time": internally, R stores the number of seconds that have elapsed since 1970-01-01 00:00 UTC.

```{r}
now <- Sys.time()
now
```

If I peek under the hood using `unclass()` here's what I see:

```{r}
unclass(now)
```

There are no attributes attached to this object, it is simply a count of the number of seconds since that particular moment in time. However, it doesn't necessarily have to be this way: a POSIXct object is permitted to have a "tzone" attribute, a character string that specifies the timezone that is used when printing the object will be preserved when it is converted to a POSIXlt.

Nevertheless, when I created the `now` object by calling `Sys.time()`, no timezone information was stored in the object. The fact that it appears when I print out `now` occurs because the `print()` method for POSIXct objects prints the time with respect to a particular timezone. The default is to use the system timezone, which you can check by calling `Sys.timezone()`, but you can override this behaviour by specifying the timezone explicitly (for a list of timezone names, see `OlsonNames()`). So if I wanted to print the time in Berlin, I could do this:

```{r}
print(now, tz = "Europe/Berlin")
```

If you want to record the timezone as part of your POSIXct object rather than relying on the print method to do the work, you can do so by setting the `tzone` attribute. To illustrate this, let's pretend I'm in Tokyo:

```{r}
attr(now, "tzone") <- "Asia/Tokyo"
now
```

The important thing here is that the timezone is metadata used to change the how the time is displayed. Changing the timezone does not alter the number of seconds stored in the `now` object:

```{r}
unclass(now)
```

### [R] The POSIXlt class

What about POSIXlt? It turns out that this is a quite different kind of data structure, and it "thinks" about time in a very different way. The "lt" in POSIXlt stands for "local time", and internally a POSIXlt object is a list that stores information about the time in a way that more closely mirrors how humans think about it. Here's what `now` looks like when I coerce it to a POSIXlt object:

```{r}
now_lt <- as.POSIXlt(now)
now_lt
```

It *looks* the same, but this is an illusion produced by the `print()` method. Internally, the `now_lt` object is a very different kind of thing. To see this, lets see what happens if we print it as if it were a regular list:

```{r}
unclass(now_lt)
```

As you can see, this object separately stores the year (counted from 1900), the month (where January is month 0 and December is month 11), the day of the month (starting at day 1), etc.^[A little counterintuitively, the value of `sec` ranges from 0 to 61, presumably because leap seconds are a thing. I am *not* going to torture myself with that one today.] The timezone is stored, as is the day of the week (Sunday is day 0), it specifies whether daylight savings time is in effect, and so on. Time, as represented in the POSIXlt class, uses a collection of categories that are approximately the same as those that humans use when we talk about time. 

It is not a compact representation, and it's useful for quite different things than POSIXct. What matters for the current purposes is that POSIXlt is, fundamentally, a list structure, and is not in any sense a "timestamp".


### [Arrow] The date32 type

Okay, now let's pivot over to the Arrow side and see what we have to work with. The date32 type is similar -- but not identical -- to the R Date class. Just like the R Date class, it counts the number of days since 1970-01-01. To see this, let's create an analog of the `today` Date object inside Arrow, and represent it as a date32 type:

```{r}
today_date32 <- scalar(today, type = date32())
today_date32
```

We can expose the internal structure of this object by casting it to an integer:

```{r}
today_date32$cast(int32())
```

This is the same answer we got earlier when I used `unclass()` to take a peek at the internals of the `today` object. That being said, there is a subtle difference: in Arrow, the date32 type is explicitly a 32-bit integer. If you read through the help documentation for date/time classes in R you'll see that R has something a little more complicated going on. The details don't matter for this post, but you should be aware that Dates (and POSIXct objects) are stored as doubles. Unlike Arrow, they aren't stored as integers:

```{r, results='hold'}
typeof(today)
typeof(now)
```

In any case, given that the Arrow date32 type and the R Date class are so similar to each other in structure and intended usage, it is natural to map R Dates to Arrow date32 types and vice versa, and that's what the **arrow** package does by default. 

### [Arrow] The date64 type

The date64 type is similar to the date32 type, but instead of storing the number of days since 1970-01-01 as a 32-bit integer, it stores the number of milliseconds since 1970-01-01 00:00:00 UTC as a 64-bit integer. It's similar to the POSIXct class in R, except that (1) it uses milliseconds instead of seconds; (2) the internal storage is a 64-bit integer, not a 32-bit double; and (3) it does not have metadata and cannot represent timezones.

As you might have guessed, the date64 type in Arrow isn't very similar to the Date class in R. Because it represents time at the millisecond level, the intended use of the date64 class is in situations where you want to keep track of units of time smaller than one day. Sure, I *can* create date64 objects from R Date objects if I want to...

```{r}
scalar(today, date64())
```

...but this is quite wasteful. Why use a 64-bit representation that tracks time at the millisecond level when all I'm doing is storing the date? Although POSIXct and date64 aren't exact matches, they're more closely related to each other than Date and date64. So let's create an Arrow analog of `now` as a date64 object:

```{r}
now_date64 <- scalar(now, date64())
now_date64
```

The output is *printed* as a date, but it is a little bit misleading because it doesn't give you a good sense of the level of precision in the data. Again we can peek under the hood by explicitly casting this to a 64-bit integer:

```{r}
now_date64$cast(int64())
```

This isn't a count of the number of days since the unix epoch, it's a count of the number of milliseconds. It is essentially the same number, divided by 1000, as the one we obtained when I typed `unclass(now)`. However, there's a puzzle here that we need to solve. Let's take another look at `unclass(now)`:

```{r}
unclass(now)
```

This might strike you as very weird. On the face of it, what has happened is that I have taken `now` (which ostensibly represents time at "second-level" precision), ported it over to Arrow, and created an object `now_date64` that apparently knows what millisecond it is???? How is that possible? Does Arrow have magic powers? 

Not really. R is playing tricks here. Remember how I said that POSIXct objects are secretly doubles and not integers? Well, this is where that becomes relevant. It's quite hard to get R to confess that a POSIXct object actually knows the time at a more precise level than "to the nearest second" but you can get it do to so by coercing it to a POSIXlt object and then taking a peek at the `sec` variable:

```{r}
as.POSIXlt(now)$sec
```

Aha! The first few digits of the decimal expansion are the same ones stored as the least significant digits in `now_date64`. The data was there all along. Even though `unclass(now)` produces an output that has been rounded to the nearest second, the original `now` variable is indeed a double, and it *does* store the time a higher precision! Ultimately, the accuracy of the time depends on the system clock itself, but the key thing to not here is that even though POSIXct times are almost always displayed to the nearest second, they do have the ability to represent more precise times. 

Because of this, the default behaviour in **arrow** is to convert date64 types (64-bit integers interpreted as counts of milliseconds) to POSIXct classes (which are secretly 32-bit doubles interpreted as counts of seconds). 

Right. Moving on.

### [Arrow] The timestamp type

The last of the Arrow date/time types is the timestamp. The core data structure is a 64-bit integer used to count the number of time units that have passed since the unix epoch, and this is associated with two additional pieces of metadata: the time unit used (e.g., "seconds", "milliseconds, "microseconds", "nanoseconds"), and the timezone. As with the POSIXct class in R, the timezone metadata is optional, but the time unit is necessary. The default is to use microseconds (i.e., `unit = "us"`):

```{r, results='hold'}
scalar(now)
scalar(now, timestamp(unit = "us"))
```

Alternatively, we could use seconds:

```{r}
scalar(now, timestamp(unit = "s"))
```

It's important to recognise that changing the unit does more than change the precision at which the timestamp is printed. It changes "the thing that is counted", so the numbers that get stored in the timestamp are quite different depending on the unit. Compare the numbers that are stored when the units are seconds versus when the units are nanoseconds:

```{r, results='hold'}
scalar(now, timestamp(unit = "s"))$cast(int64())
scalar(now, timestamp(unit = "ns"))$cast(int64())
```

Okay, what about timezone. Recall that `now` has a timezone attached to it, because I explicitly recorded the `tzone` attribute earlier. Admittedly I lied and I said I was in Tokyo and not in Sydney, but still, that information is in the `now` object:

```{r}
now
```

When I print the R object it displays the time in the relevant time zone. The output for the Arrow object doesn't do that: the time *as displayed* is shown in UTC. However, that doesn't mean that the metadata isn't there:

```{r}
now_timestamp <- scalar(now)
now_timestamp$type
```

I mention this because this caused me a considerable amount of panic at one point when I thought my timezone information had been lost when importing data from POSIXct into Arrow. Nothing was lost, it is simply that the **arrow** R package prints all timestamps in the corresponding UTC time regardless of what timezone is specified in the metadata. 

There is, however, a catch. This worked last time *because* I was diligent and ensured that my `now` variable encoded the timezone. By default, a POSIXct object created by `Sys.time()` will not include the timezone. It's easy to forget this because the `print()` method for POSIXct objects will inspect the system timezone if the POSIXct object doesn't contain any timezone information, so it can often *look* like you have a timezone stored in your POSIXct object when actually you don't. When that happens, Arrow can't help you. Because the POSIXct object does not have a timezone (all appearances to the contrary), the object that arrives in Arrow won't have a timezone either. Here's what I mean:

```{r}
# a POSIXct object with no timezone
new_now <- Sys.time() # has no time zone...
new_now               # ... but appears to!

# an Arrow timestamp with no timezone
new_now_timestamp <- scalar(new_now)
new_now_timestamp$type
```

The take-home message in all this is that if you're going to be working in both Arrow and R, and using the **arrow** package for data interchange, you'd be well advised to be careful with your POSIXct objects and timezones. They are trickier than they look, and can lead to subtle translation errors if you are not careful! 

### Um, but what about POSIXlt?

At long last we come to POSIXlt, which has no clear analog in Arrow. The key idea behind POSIXlt is to represent temporal information in terms of multiple different units: days, weeks, years, seconds, timezones, and so on. It's a very different kind of thing to a POSIXct object in R or a timestamp in Arrow. In R terms, it's essentially a list, and as a consequence the default behaviour in **arrow** is to import it as a struct (which serves essentially the same purpose). Here's how that looks:

```{r}
scalar(now_lt)
```

The struct object contains named fields that are identical to their POSIXlt equivalents, and whose types have been translated according to the default mappings: `sec` is a double, `min` is an int32, `hour` is an int32, `zone` is a string, and so on.

This arrangement, where POSIXct maps to timestamp and POSIXlt maps to struct, makes perfect sense when you think about the underlying data structures that POSIXct and POSIXlt encode. Where things can be tricky for the R user is in the "mental account keeping". In order to be helpful, R displays POSIXct and POSIXlt objects in exactly the same way:

```{r, results='hold'}
now
now_lt
```

Not only that, because POSIXct and POSIXlt are both subclasses of the POSIXt class, R allows you to perform temporal arithmetic on objects of different types:

```{r}
now - now_lt
```

This is very convenient from a data analysis perspective, since calculations performed with date/time classes "just work" even though POSIXct objects are secretly doubles and POSIXlt objects are secretly lists. However, all this happens invisibly. In much the same way that it's easy to forget that POSIXct objects may not encode a timezone even though they *look* like they do, it can be easy to forget that POSIXct and POSIXlt are fundamentally different objects, and they map onto quite different data structures in Arrow. 

## Duration types

Any discussion of temporal data is incomplete without a discussion of duration types, which are used to describe a length of time without reference to any fixed origin. The figure below shows the default mappings used by **arrow**:

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Default mappings for duration types"
knitr::include_graphics("img/duration-types.png", dpi = 100)
```


### [R] The difftime class

In base R, the difference between two date/time objects is stored as a difftime object. To give a better illustration of a difftime object, let's create `diff`, a variable that stores the amount of time elapsed between executing the R markdown chunk that first computed the `now` variable, and executing the R markdown chunk below:

```{r}
new_now <- Sys.time()
rmd_time <- new_now - now
rmd_time
```

Now let's take a look at how it's actually stored:

```{r}
unclass(rmd_time)
```

The duration is represented as a numeric variable, and the `"units"` attribute is used to specify the time unit that it represents: "secs", "mins", "hours", "days" or "weeks". Unless the user specifies exactly which unit is to be used, R will attempt to make a sensible choice. For instance, if I were to do this,

```{r}
hedy_lamarr <- as.POSIXct("1914-11-09 19:30:00", tz = "Europe/Vienna")
hedy_age <- now - hedy_lamarr
hedy_age
```

I would learn that it has been `r round(c(hedy_age))` days since [Hedy Lamarr](https://en.wikipedia.org/wiki/Hedy_Lamarr) was born.^[Yes, I looked up her [time and location of birth](https://www.astro.com/astro-databank/Lamarr,_Hedy) on an astrology website. I am, after all, queer. It is one of the traditions of our people.] More to the point, R has guessed that the length of time is sufficiently long that seconds aren't the appropriate encoding unit:

```{r}
unclass(hedy_age)
```

### [Arrow] The duration type

The difftime class in R has a natural analog in Arrow, the duration type. As usual though, they are not *exactly* equivalent to one another. An R difftime object stores the value as a double, so it has no problems storing 0.23 as the value and setting the units to be seconds. This doesn't work very well in Arrow because the value is stored as a signed 64 bit integer (int64), and a value of 0.23 seconds will simply round down to a duration of zero seconds. When importing my duration data into Arrow, then, I should be careful to ensure I choose a higher precision unit. If I don't, things can go a little awry:

```{r}
rmd_time_arrow <- scalar(rmd_time)
rmd_time_arrow
rmd_time_arrow$type
```

What I really needed to is specify a higher level of precision. If I want to import a duration into Arrow rounded to the nearest microsecond, I could do this:

```{r}
rmd_time_arrow <- scalar(rmd_time, duration(unit = "us"))
rmd_time_arrow
rmd_time_arrow$type
```


### [R] The hms::hms class

So where are we up to in our voyage through the world of dates, times, and durations in the R world? We've talked about situations where you can specify a fixed date (with the Date class) and situations where you can specify a fixed moment in time (with POSIXct and POSIXlt classes). We've also talked about situations where you can specify an amount of time without fixing it to a specific date or time (with the difftime class). What we haven't talked about is how to store the time of day. In base R you can talk about a date without needing to specify a time, or you can talk about times and dates together, but what you can't do is specify a time on its own without a date. 

The **hms** package fixes this by supplying the hms class. Internally, it's just a difftime object that counts the number of seconds elapsed since midnight. As I type these words the current time is 14:05:25, and I could create an hms object representing this like so:

```{r}
hms_time <- hms::hms(seconds = 25, minutes = 5, hours = 14)
hms_time
```

The nice thing about hms times is that they inherit from difftime, which we can see by checking the class vector for our `hms_time` object

```{r}
class(hms_time)
```

Just to show that there really isn't anything fancy going on, let's strip the class attribute away and let R print out the raw object. As the output here shows, an hms object has the same underlying structure as a regular difftime object:

```{r}
unclass(hms_time)
```

### [Arrow] The time32 and time64 types

What about Arrow? 

At a technical level, it would be perfectly possible to translate an hms object in R into an Arrow duration object, but that feels slightly unnatural. The entire reason why the hms class exists in R is that we -- the human users -- attach special meaning to the "duration of time that has elapsed since midnight on an arbitrary day". We call it the time of day, and while technically it is possible to represent the time of day as a duration (or an hms as a difftime), human beings like to treat special things as special for a reason. 

Because of this, Arrow supplies two data types that are roughly analogous to the hms class in R, called time32 and time64. The time32 type stores the time of day as a signed 32-bit integer, which represents the number of seconds (or alternatively, milliseconds) since midnight. By default, the **arrow** package will translate an hms object to a time32 type, using seconds as the unit:

```{r}
hms_time32_s <- scalar(hms_time)
hms_time32_s
hms_time32_s$type
```

To switch to milliseconds, I would use a command like this:

```{r}
hms_time32_ms <- scalar(hms_time, time32(unit = "ms"))
hms_time32_ms
```

Notice that the output shows the time in a little more resolution. I find this a helpful touch, since it provides a visual cue letting me know what the unit is. But just to confirm, let's inspect the type explicitly:

```{r}
hms_time32_ms$type
```

If you need to represent the time of day at a higher degree of precision, you'll want to use the time64 type, which (shockingly!) represents the time of day as a signed 64-bit integer. When using the time64 class you can choose microseconds (`unit = "us"`) or nanoseconds (the default, `unit = "ns"`) as the unit:

```{r}
hms_time64_us <- scalar(hms_time, time64())
hms_time64_us
hms_time64_us$type
```

In essence, the **arrow** defaults are set up such that if you choose `time32()` when going from R to Arrow without specifying a unit, you will end up with the lowest precision representation of time (rounded to the nearest second), whereas if you do the same with `time64()` you end up with the highest precision (nanosecond level) representation. When going the other way, **arrow** will map time32 types and time64 types to hms objects, and the end result is that the time of day will be stored as a 32-bit double. 

## Other types

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Default mappings for null types"
knitr::include_graphics("img/null-types.png", dpi = 100)
```

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Default mappings for raw types"
knitr::include_graphics("img/raw-types.png", dpi = 100)
```

```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Default mappings for factor types"
knitr::include_graphics("img/factor-types.png", dpi = 100)
```


```{r}
#| echo = FALSE,
#| fig.align = "center",
#| fig.cap = "Default mappings for data frame types"
knitr::include_graphics("img/dataframe-types.png", dpi = 100)
```


## Resources

- https://arrow.apache.org/docs/r/articles/arrow.html
- https://arrow.apache.org/docs/cpp/api/datatype.html



```{r, echo=FALSE}
.sys.timezone <- tz
```

<!--------------- appendices ----------------->

```{r, echo=FALSE}
refinery::insert_appendix(
  repo_spec = params$repo, 
  name = paste(params$date, params$slug, sep = "_")
)
```


<!--------------- miscellanea ----------------->

```{r redirect, echo=FALSE}
refinery::insert_netlify_redirect(
  slug = params$slug, 
  date = params$date
)
```




