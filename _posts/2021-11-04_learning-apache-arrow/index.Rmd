---
title: "Learning Apache Arrow (for R users)" 
description:
  Or, how this R user learned to stop worrying and love a standardised in-memory columnar data format
author:
  - first_name: "Danielle"
    last_name: "Navarro"
    url: https://djnavarro.net
    affiliation: UNSW Sydney
    affiliation_url: https://unsw.edu.au
    orcid_id: 0000-0001-7648-6578
date: 2021-11-04
preview: preview-image.jpg  # <---- UPDATE ME 
creative_commons: CC BY
citation_url: https://blog.djnavarro.net/learning-apache-arrow 
repository_url: https://github.com/djnavarro/distill-blog/
output:
  distill::distill_article:
    self_contained: false
    toc: true
params:
  slug: learning-apache-arrow
  date: 2021-11-04
  repo: djnavarro/distill-blog
  site: https://blog.djnavarro.net/
---

<!----

checklist:
  - check the "update me" messages in YAML above
  - initialise the _renv folder with refinery::renv_new("name of post folder")
  - populate the lockfile with refinery::renv_snapshot("name of post folder")
  - update the _renv folder from snapshot with refinery::restore("name of post folder")

---->


<!--------------- setup post ----------------->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# possible renv/arrow issue?
# refinery::renv_load(paste(params$date, params$slug, sep = "_"))
```


<!--------------- post ----------------->

If you're like me and spend far too much time on [#rstats](https://twitter.com/home?hashtag=rstats) twitter you may have come across people talking about how to work with large data sets in R, and in particular how to solve the issues associated with data serialisation, analysing columnar data stored in memory, and so on. Perhaps you hear people talking about [Parquet files](https://parquet.apache.org), [Apache Arrow](https://arrow.apache.org), and the [arrow package for R](https://arrow.apache.org/docs/r/), but you're not really sure what they're about and are curious? If that's you, then--

*So we're just writing obvious "I want a job in tech, please hire me!" blog posts now?*

Oh shush. It's fun and useful too, you know.

*Okay fine, but at least be transparent about what you're doing?*

Sheesh, what do you think this fake dialogue is for if not making the subtext blatant? Now could you please stop interrupting me and let me talk about Apache Arrow? It is in fact a more interesting subject than our pending unemployment. 

*Yeah, see how you feel about that in December babe...*

Sigh.


## Introduction

Okay, where was I? Ah yes...

If you're like me and spend far too much time on [#rstats](https://twitter.com/home?hashtag=rstats) twitter you may have come across people talking about how to work with large data sets in R, and in particular how to solve the issues associated with data serialisation, analysing columnar data stored in memory, and so on. Perhaps you hear people talking about [Parquet files](https://parquet.apache.org), [Apache Arrow](https://arrow.apache.org), and the [arrow package for R](https://arrow.apache.org/docs/r/), but you're not really sure what they're about and are curious? If that's you, then this blog post is designed to help you get started. 

### Wait... do I actually care?

Let's start at the beginning, with the most important question of all: do you actually need to care about this? This is going to be a long post, so let's make sure you're reading it for the right reasons!

For a lot of people, the answer to the "do I care?" question is going to be "probably not -- or at least not right now". For example, if all your data sets are small and rectangular, then you're probably working with CSV files and not encountering a lot of problems. Your current workflow uses `read.csv()` or `readr::read_csv()` to import data, and everything is fine. Sure, the CSV format has some problems, but it's simple and it works. If that is you, then right now you don't need to worry about this. 

But perhaps that's not you, or maybe that won't be you forever. You might be working with larger data sets, either now or in the future, and when that happens you might need to care.

### Okay... so what's the problem?

Thanks for a great question! Here are a few scenarios to think about.

- *Scenario 1:* Let's suppose you have a big rectangular data set. An enormous table, basically, and currently it's stored as a file on your disk. The format of that file could be a plain CSV, a compressed CSV, or it could be something fancier like a Parquet file (I'll come back to those later). It might be a couple of billion rows or so, the kind of thing that you can store on disk but is too big to fit into memory, so it's not going to be very easy to read this thing into R as a data frame! But your boss wants you to analyse it in R anyway. That's awkward. R likes to store things in memory. Eek.

- *Scenario 2:* Okay, maybe your data isn't that big and it fits in memory, but it's still pretty big, and you need to do something complicated with it. Maybe your analysis needs to start in R but then continue in Python. Or something like that. In your head, you're thinking okay first I have to read the whole dataset into memory in R, and then it has to be transferred to Python which will have to read its own copy, and... gosh that sounds slow and inefficient. Ugh.

- *Scenario 3:* Honestly, you're just tired of having to deal with the fact that every language has its own idiosyncratic way of storing data sets in memory and it's exhausting to have to keep learning new things and you really wish there were some standardised way that programming languages represent data in memory and you'd like a single toolkit that you can use regardless of what language you're in. Sigh...

In any of these scenarios, Arrow might be useful to you.

### Fiiiiiine, I'll keep reading... tell me what Arrow is

Yaaaaay! [Green Arrow](https://en.wikipedia.org/wiki/Green_Arrow) is a superhero in the DC Comics universe, whose real name is Oliver Queen. He was the subject of an unintentionally hilarious TV show, and--

### Sigh. *Apache* Arrow please?

Oh right. Apache Arrow is a standardised, language-independent format for storing table-like data in-memory, using a columnar format and supporting zero-copy reads without serialisation overhead.

### I hate you

Sorry. Let's unpack each of those terms: 

- Arrow is a **standardised and language-independent format**. It's the same thing regardless of what programming language you're using: a data set accessed from R with Arrow has the same format as the a data set accessed in Python.
- Arrow is used to store **table-like data**, very similar to a data frame or tibble.
- Arrow refers to the **in-memory** format: it's not talking about how the data are stored on disk, and it's not talking about file formats. It's all about how a loaded data set is represented in memory.
- Arrow (and the related Parquet file format) uses **columnar format**. Unlike a CSV file, which stores the data row-wise, it represents the data column-wise: this turns out to be a much more efficient way to represent data when you need to subset the data (e.g., by using `dplyr::filter()` in R or the `WHERE` clause in SQL). 
- Arrow supports **zero-copy reads without serialisation overhead**, which... um...

### Explain serialisation please

Oh right, yeah. [Serialisation](https://en.wikipedia.org/wiki/Serialization) is one of those terms that those fancy data people know all about, but a regular R user might not be *quite* as familiar with. It's worth unpacking this a bit because it's helpful for understanding the problem that Arrow solves. 

Okay, here's a data frame shown the way we typically think about it from the R user perspective:

```{r read-manifest}
art <- readr::read_csv("https://djnavarro.net/series-water-colours/manifest.csv")
art
```

It's a rectangular data stored in memory as the `art` tibble, and like all tibbles and data frames it's basically a list that serves as a container for a collection of vectors: `path`, `resolution`, `series` and so on. If you really wanted to torture yourself you could go look at the [R Internals Manual](https://cran.r-project.org/doc/manuals/r-release/R-ints.pdf) to see how this data structure is implemented, but it's not important here. The key thing here is that the "thing" stored in memory is a structured object.

However, notice that in the code chunk above, I had to load the data from a file. The data had to be transferred over a channel (in this case the internet) to reach R, and what gets transferred is just a series of bytes. A structured object is *serialised* into a sequence of bytes, that sequence of bytes is transmitted (or stored as a file), and that sequence of bytes is then *unserialised* at the other end and transformed back into a structured object. Base R has a `serialize()` function that you can use to convert the `art` tibble into a raw vector:

```{r serialise-art}
bytes <- serialize(art, connection = NULL)
```
Here's the first 100 bytes:

```{r show-serialised}
bytes[1:100]
```

Similarly, there's an `unserialize()` function that will take the raw vector as input and reconstruct the tibble for you:

```{r unserialise-art}
unserialize(bytes)
```

For a small data set like this one, it doesn't take R very long to serialise or unserialise. The "serialisation overhead" isn't a big deal. But when the data set is very large, this is not a trivial operation and you don't want to do this very often. That's a problem when a large data set needs to be passed around between multiple platforms. Loading the a CSV into R incurs a serialisation cost; transferring a copy of the data from R to Python incurs a serialisation cost. This happens because R and Python have different structured representations: an data frame in R is a different kind of thing to a panda in Python, so the data has to be serialised, transferred, and then unserialised at the other end in order to pass the data from one to another. 

Wouldn't it be nice if we could avoid that? What if there was just *one* data structure representing the table in-memory, and R and Python could both agree to use it? That would remove the need to copy and transfer the data, right? And in doing so, it would eliminate those pesky serialisation costs incurred every time. If *only* there were a standardised, language-independent format for storing table-like data in-memory, using a columnar format and supporting zero-copy reads without serialisation overhead...

**[Apache Arrow enters stage left, looking unreasonably pleased]**

"Hi!"


## Overview of Arrow


## Installing Arrow

Installing Apache Arrow on your local machine as an R user is either extremely easy or very tiresome, depending almost entirely on whether you're on Linux. If you're using Windows or Mac OS, you shouldn't need to do anything except install the arrow package in the usual way. 

```{r, eval=FALSE}
install.packages("arrow")
```

If you're on Linux, there may not be any precompiled C++ binaries for your system, so you'll have to do it yourself. On my system this was quite time consuming, and the first couple of times I tried it I was convinced that nothing was actually happening because I wasn't seeing a progress bar or anything, and being impatient I killed the install process before it was finished. If you're like me and need visual confirmation that something is happening, there's an `ARROW_R_DEV` environment variable you can set that will make the process more verbose:

```{r, eval=FALSE}
Sys.setenv(ARROW_R_DEV = TRUE)
install.packages("arrow")
```

This way you get to see all the C++ build information scrolling by on the screen during the installation process. It doesn't make for very exciting viewing, but at least you have visual confirmation that everything is working!

There are quite a few ways you can customise the installation process, and they're all documented on the [installation page](https://arrow.apache.org/docs/r/articles/install.html). One particularly useful thing to do is to set `LIBARROW_MINIMAL` to false, which ensures that arrow will install a bunch of optional features like compression libraries and AWS S3 support. It takes longer but you get more stuff! So the actual installation code I used was this:

```{r, eval=FALSE}
Sys.setenv(
  ARROW_R_DEV = TRUE,
  LIBARROW_MINIMAL = FALSE
)
install.packages("arrow")
```

This may take quite a long time if you're compiling from source so you may want to go make a cup of tea or something while it installs. At the end, hopefully, you'll have a working version of the package:

```{r, message=FALSE}
library(arrow)
```

You can use the `arrow_info()` function to obtain information about your installation:

```{r}
arrow_info()
```




<!--------------- appendices ----------------->

```{r, echo=FALSE}
#refinery::insert_appendix(
#  repo_spec = params$repo, 
#  name = paste(params$date, params$slug, sep = "_")
#)
```


<!--------------- miscellanea ----------------->

```{r redirect, echo=FALSE}
refinery::insert_netlify_redirect(
  slug = params$slug, 
  date = params$date
)
```




