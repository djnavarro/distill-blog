---
title: "Using with Amazon S3 with R"
description:
  Somehow I was convinced that using Amazon S3 would be a 
  supremely difficult thing to learn, kind of like learning
  git and GitHub for the first time. Thankfully, it's not
  like that at all. With the help of the aws.s3 package,
  you can manage cloud data storage from R with surprisingly
  little pain.
author:
  - first_name: "Danielle"
    last_name: "Navarro"
    url: https://djnavarro.net
    affiliation: Voltron Data
    affiliation_url: https://voltrondata.com
    orcid_id: 0000-0001-7648-6578
date: 2022-03-17
preview: preview-image.jpg  
creative_commons: CC BY
citation_url: https://blog.djnavarro.net/using-aws-s3-in-r 
repository_url: https://github.com/djnavarro/distill-blog/
output:
  distill::distill_article:
    self_contained: false
    toc: true
    css: "strapless.css"
params:
  slug: using-aws-s3-in-r
  date: 2022-03-17
  repo: djnavarro/distill-blog
  site: https://blog.djnavarro.net/
---

<!----

checklist:
- check the "update me" messages in YAML above
- initialise the _renv folder with refinery::renv_new("name of post folder")
- populate the lockfile with refinery::renv_snapshot("name of post folder")
- update the _renv folder from snapshot with refinery::restore("name of post folder")

---->


<!--------------- setup post ----------------->

```{r setup, include=FALSE}
renv::use(lockfile = here::here(
  "_renv", "_posts", paste(params$date, params$slug, sep = "_"), "renv.lock")
)
#refinery::renv_load(paste(params$date, params$slug, sep = "_"))
knitr::opts_chunk$set(echo = TRUE)

tiny_herb_url <- function(object) {
  bucket <- "tiny-herbs"
  region <- "ap-southeast-2"
  url <- paste0(
    "https://", bucket, ".", "s3-", 
    region, ".amazonaws.com/", object
  )
  return(url)
}
```


<!--------------- post ----------------->

I have a shameful confession to make, one that may shock and surprise you. Although I am an R user, data scientist, and developer of many years experience, I've never used Amazon Web Services. I've never spun up a virtual machine on "Amazon EC2" (whatever _that_ is), I don't know what "AWS Lambda" is, and all I know about "Amazon S3" is that fancy data science people use it to stored data.^[Okay fine, I used "Amazon Mechanical Turk" a lot as part of my academic work, but that's hardly a core data science skill.] Or something. Every time people start talking about it my eyes glaze over and my impostor syndrome flares up to tell me that I am an incompetent fraud. A *true* data scientist is born knowing how to spin up EC2 instances,^[Again, I cannot stress how much I do not know about AWS. I guess that's the term for virtual machines. Whatever. This isn't a post about EC2] and if baby doesn't post her drawings on S3 then she's already falling behind, etc etc. My anxiety becomes too much, so I go do something girly like nonparametric Bayes instead. 

My personal "tragedy"^[It's deeply important to me that you read this knowing that I was singing [*Tragedy*](https://www.youtube.com/watch?v=OiwDHHcHPh0) by Steps at the time I wrote this, complete with dramatic hand gestures] notwithstanding, I suspect my situation is not entirely uncommon. Back in my academic days, I knew very few people who used Amazon Web Services (a.k.a. AWS) for much of anything. It wasn't needed, so it wasn't knowledge that people acquired. Now that I'm working in an industry setting I'm finding that it's _so_ widely used that it's almost assumed knowledge. _Everyone_ knows this stuff, so there's not a lot said about why you might care, or how to get started using these tools if you decided that you do care. 

Today I decided to do something about this, starting by teaching myself how to use Amazon's Simple Storage Service (a.k.a S3). With the help of the [**aws.s3** package](https://github.com/cloudyr/aws.s3) authored by Thomas Leeper and currently maintained by Simon Urbanek, it's surprisingly easy to do.

In this post I'll walk you through the process.

```{r}
library(dplyr)
library(readr)
library(purrr)
library(stringr)
library(tibble)
library(magick)
library(aws.s3)
```

```{r echo=FALSE}
tiny_herb_url("NSW687458.jpg") %>% 
  knitr::include_graphics()
```

## What is S3 and why do I care?


```{r echo=FALSE}
tiny_herb_url("NSW29246.jpg") %>% 
  knitr::include_graphics()
```

## Downloading public data from S3

While browsing through the registry of open data sets listed on the S3 website I came across the [National Herbarium of NSW data set](https://registry.opendata.aws/nsw-herbarium/). As described on the website:

> The National Herbarium of New South Wales is one of the most significant scientific, cultural and historical botanical resources in the Southern hemisphere. The 1.43 million preserved plant specimens have been captured as high-resolution images and the biodiversity metadata associated with each of the images captured in digital form. Botanical specimens date from year 1770 to today, and form voucher collections that document the distribution and diversity of the world's flora through time, particularly that of NSW, Austalia and the Pacific. The data is used in biodiversity assessment, systematic botanical research, ecosystem conservation and policy development. The data is used by scientists, students and the public.

Okay I love this and I want to play with it. But how do I do that? The listing for the data set posts the following metadata:

```
Resources on AWS

Description
Herbarium Collection Image files
Resource type
S3 Bucket
Amazon Resource Name (ARN)
arn:aws:s3:::herbariumnsw-pds
AWS Region
ap-southeast-2
AWS CLI Access (No AWS account required)
aws s3 ls --no-sign-request s3://herbariumnsw-pds/
```

### Finding the bucket

Okay, so is there a bucket? I'll use the `bucket_exists()` function to verify that there is in fact a public data set located on AWS at "s3://herbariumnsw-pds/", and of course there...

```{r, eval=FALSE}
bucket_exists("s3://herbariumnsw-pds/")
```
```{r, echo=FALSE, cache=TRUE}
x <- bucket_exists("s3://herbariumnsw-pds/")
attributes(x) <- NULL
x
```

...isn't? What???? 

I've made a very common mistake here, and forgotten to specify the region. S3 is very picky about regions and you need to tell it explicitly which one to use. The National Herbarium is an Australian institution and the data are stored in Amazon's Sydney data center: that's the "ap-southeast-2" region. Let's try that again:

```{r, eval=FALSE}
bucket_exists("s3://herbariumnsw-pds/", region = "ap-southeast-2")
```
```{r, echo=FALSE, cache=TRUE}
x2 <- bucket_exists("s3://herbariumnsw-pds/", region = "ap-southeast-2")
attributes(x2) <- NULL
x2
```

Much better!

Oh and one more thing. I lied slightly in the output... if you go do this yourself, you'll find that the results are a little bit more verbose than simply printing `TRUE` or `FALSE`. The actual output comes with a lot of additional metadata that get stored as attributes. So what you'll actually see is something like this:

```{r, cache=TRUE}
bucket_exists(
  bucket = "s3://herbariumnsw-pds/", 
  region = "ap-southeast-2")
```

### Listing bucket contents

There are a lot of objects stored in this bucket, so I'll just download the first 20,000 records:

```{r get-bucket, cache=TRUE}
herbarium_files <- get_bucket_df(
  bucket = "s3://herbariumnsw-pds/", 
  region = "ap-southeast-2", 
  max = 20000
) %>% 
  as_tibble()
```

Now let's take a peek:

```{r inspect-herbarium-files}
glimpse(herbarium_files)
```

### Downloading files

```{r download-readme, cache=TRUE}
save_object(
  object = "ReadMe.txt",
  bucket = "s3://herbariumnsw-pds/", 
  region = "ap-southeast-2",
  file = "herbarium/ReadMe.txt"
)
```

As you might hope, the README does in fact tell you something about how it's organised:

> Image data are organized by NSW specimen barcode number. For example, the file for Dodonaea lobulata recorded on 1968-09-07 = NSW 041500 can be accessed via the URI 
https://herbariumnsw-pds.s3-ap-southeast-2.amazonaws.com/images/NSW041500.jp2
>
A zipped csv containing the biocollections metadata for the images is available as a DarwinCore Archive at:
https://herbariumnsw-pds.s3-ap-southeast-2.amazonaws.com/dwca-nsw_avh-v1.0.zip

So the real place to start is with this metadata file, `dwca-nsw_avh-v1.0.zip`

```{r download-metadata, cache=TRUE}
save_object(
  object = "dwca-nsw_avh-v1.0.zip",
  bucket = "s3://herbariumnsw-pds/", 
  region = "ap-southeast-2",
  file = "herbarium/dwca-nsw_avh-v1.0.zip"
) 
```

So now we have the data. After extracting from the zip file, we end up with a 421MB text file called `occurrence.txt` that turns out to be a tab separated value file:

```{r read-data, cache=TRUE}
herbarium <- read_tsv("herbarium/dwca-nsw_avh-v1.0/occurrence.txt")
herbarium
```

### Wrangling the data

We can do some simple data wrangling:

```{r data-wrangling}
herbarium %>% 
  filter(country == "Australia") %>% 
  count(stateProvince)
```


Okay, within NSW what are the unique regions

```{r nsw-regions}
herbarium %>% 
  filter(country == "Australia", stateProvince == "New South Wales") %>% 
  count(locality) %>% 
  arrange(desc(n))
```

```{r}
newtowners <- herbarium %>% 
  filter(
    country == "Australia", 
    stateProvince == "New South Wales", 
    locality %>% str_detect("Newtown")
  )

newtowners$catalogNumber
```

I wonder if I can write code to extract these images?


```{r echo=FALSE}
tiny_herb_url("NSW676197.jpg") %>% 
  knitr::include_graphics()
```


### Scripting the download

Okay, now I want to pull the images for these records. First, I'm going to construct the paths. I am *not* going to download the jp2 files because they're about 100MB each. That's why this is on S3, by the way. There are `r nrow(herbarium)` distinct records, and each digitised record has a high resolution jp2 file as well as a regular lossy-encoded jpg. Let's go with the jpgs shall we?

```{r, cache=TRUE, warning=TRUE}
filepaths <- newtowners$catalogNumber %>% 
  str_remove_all(" ") %>% 
  str_c("images/", ., ".jpg")

save_herbarium_image <- function(file) {
  
  ok <- object_exists(
    object = file,
    bucket = "s3://herbariumnsw-pds/", 
    region = "ap-southeast-2"
  )
  
  if(!ok) {
    warning("object '", file, "' does not exist in this bucket", call. = FALSE)
    return(invisible(NULL))
  }
  
  save_object(
    object = file,
    bucket = "s3://herbariumnsw-pds/", 
    region = "ap-southeast-2",
    file = paste0("herbarium/", file)
  )
}

walk(filepaths, save_herbarium_image)
```


### Behold the pretty pictures!

Even the jpg files are fairly high resolution and I don't want this post to be too big, so I'll shrink them for this post

```{r}
shrink_herbarium_image <- function(file) {
  img_from <- file.path("herbarium", "images", file)
  img_to <- file.path("herbarium", "tiny_images", file)
  image_read(img_from) %>% 
    image_resize(geometry_size_pixels(width = 1000)) %>% 
    image_write(img_to)
  gc() # prevents cache problem
}

list.files("herbarium/images") %>% 
  walk(shrink_herbarium_image)
```

Here they are:

```{r, echo=FALSE}
tibble(images = list.files("herbarium/tiny_images", full.names = TRUE)) %>% 
bs4cards::cards(
  image = images, 
  width = 3,
  border_width = 0,
  border_radius = 0
)
```

## Signing up for an AWS account

## Manipulating your S3 storage from R


### Storing your AWS credentials in R

Add these lines to .Renviron

```
AWS_ACCESS_KEY_ID=<your access key id>
AWS_SECRET_ACCESS_KEY=<your secret key>
AWS_DEFAULT_REGION=ap-southeast-2
```

Your default region might be different from mine!

### Creating a new bucket


```{r put-bucket, eval=FALSE}
put_bucket("tiny-herbs")
```
```{r}
TRUE
```

Okay, so does it exist?

```{r check-new-bucket, cache=TRUE}
bucket_exists("s3://tiny-herbs/") 
```

It does, and notice that both `put_bucket()` and `bucket_exists()` have respected my default region setting. When I called `put_bucket()`, the **aws.s3** package supplied the region from my default and so the bucket was created in Sydney (i.e., "ap-southeast-2"), and it did the same again when I used `bucket_exists()` to look for the buckets.

So what's in the bucket?

```{r, eval=FALSE}
get_bucket_df("s3://tiny-herbs/") %>% 
  as_tibble()
```
```{r, echo=FALSE}
readRDS("empty_bucket.rds")
```

Nothing! Well that makes sense. 


### Managing access control

One thing though... is this private or public? This is governed by the Access Control List (ACL) settings. By default, S3 buckets are set to private. You can read and write to them, but no-one else has any access at all. Let's soften that slightly, and allow anyone to read from the "tiny-herbs" bucket. I could have done that from the beginning by setting `acl = "public-read"` when I called `put_bucket()`. However, because I "forgot" to do that earlier, I'll change it now using `put_acl()`

```{r, cache=TRUE}
put_acl(
  bucket = "s3://tiny-herbs/",
  acl = "public-read"
)
```

Now everyone has read access to the bucket.^[You can specify different ACL settings for each object, if you want to. The `put_acl()` function also has an `object` argument that allows you to control the setting for a single object in a bucket.]

### Adding objects to your bucket

```{r}
post_herbarium_image <- function(file) {
  cat("uploading", file, "...")
  out <- put_object(
    file = paste0("herbarium/tiny_images/", file),
    object = file, 
    bucket = "s3://tiny-herbs/",
    acl = "public-read"
  )
  if(out == TRUE) {
    cat("success!\n")
    return(invisible(TRUE))
  }
  cat("failed\n")
  return(invisible(FALSE))
}
```

```{r}
list.files("herbarium/tiny_images") %>% 
  walk(post_herbarium_image)
```

Now what do we have:

```{r}
get_bucket_df("s3://tiny-herbs/") %>% 
  as_tibble()
```

### URLs for objects in public buckets

```{r}
tiny_herb_url <- function(object) {
  bucket <- "tiny-herbs"
  region <- "ap-southeast-2"
  url <- paste0(
    "https://", bucket, ".", "s3-", 
    region, ".amazonaws.com/", object
  )
  return(url)
}
tiny_herb_url("NSW121207.jpg")
```

In fact...

```{r}
tiny_herb_url("NSW121207.jpg") %>% 
  knitr::include_graphics()
```


### Other functionality?

- Copying objects between buckets
- Deleting objects and buckets
- Control tagging metadata for the bucket
- Control cross-origin resource sharing configuration 
- Configure an S3 bucket to work as a website
- ... and more

<!--------------- appendices ----------------->

```{r, echo=FALSE}
refinery::insert_appendix(
  repo_spec = params$repo, 
  name = paste(params$date, params$slug, sep = "_")
)
```


<!--------------- miscellanea ----------------->

```{r redirect, echo=FALSE}
refinery::insert_netlify_redirect(
  slug = params$slug, 
  date = params$date
)
```




