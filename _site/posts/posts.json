[
  {
    "path": "posts/2022-02-11_r-scripts-for-twitter-blocks/",
    "title": "R scripts for twitter mutes and blocks",
    "description": "Social media safety tools like muting and blocking are often misused, but for people who are targeted for harassment they are powerful and important. This is a brief tutorial illustrating how to partially automate twitter mutes and blocks from R. The intended use is defensive, to help people minimise the impact if they are targeted by large scale harassment.",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2022-02-11",
    "categories": [],
    "contents": "\n\nContents\nWarning: off-label usage\nSetting up\nWrite a block/mute function\nPreparing to scale\nPractice safe cancellation\nCheck your quotas my loves\nBe chatty babes\n\nScaling up\nEpilogue\n\n\n\n\nTwitter is a complicated place. Iâ€™ve met some of my closest friends through twitter, itâ€™s the place where I keep in touch with my professional community, and itâ€™s an important part of my work as a developer advocate at Voltron Data. But itâ€™s also a general purpose social media site, and there is a lot of content out there I prefer not to see. In particular, because Iâ€™m transgender and have absolutely no desire to participate in or even view the public debate that surrounds trans lives, I want to keep that kind of content off my twitter feed. This is particularly salient to me today as a member of the Australian LGBT community. Most people who follow me on twitter probably wouldnâ€™t be aware of it, but itâ€™s been a rough week for LGBT folks in Australia courtesy of a rather intense political fight over LGBT rights and the ostensible (and in my personal opinion, largely fictitious) conflict with religious freedom. The details of Australian politics donâ€™t matter for this post, however. What matters is that these kinds of political disputes necessarily spill over into my twitter feed, and it is often distressing. Events like this one are quite commonplace in my online life, and as a consequence Iâ€™ve found it helpful to partially automate my use of twitter safety features such as muting and blocking. Because my experience is not unique, I thought it might be useful to write a short post talking about the the scripts I use to manage twitter safety features from R using the rtweet package.\nWarning: off-label usage\nLetâ€™s make one thing very clear at the outset. A lot of what Iâ€™m going to be doing in this blog post is â€œoff labelâ€ use of the rtweet package. Youâ€™ll see me use it in ways that the writers of the package didnâ€™t really intend it to be used (I think), and youâ€™ll see me dig into the internals of the package and rely on unexported functions.\nThis is almost always a bad idea.\nIf you havenâ€™t seen it, Hadley Wickham gave a very good talk about maintaining R packages as part the rstudio::global 2021 conference. At about the 19 minute mark he talks about the â€œoff labelâ€ metaphor. In the context of medication, â€œoff labelâ€ refers to any use of a medication that itâ€™s not officially approved for. It might work, but there could be unknown consequences because maybe it hasnâ€™t been fully explored in this context. When applied to software, â€œoff labelâ€ use means youâ€™re doing something with a function or package that the designer doesnâ€™t really intend. Your code might work now, but if youâ€™re relying on â€œincidentalâ€ properties of the function to achieve your desired ends, youâ€™re taking a risk. Package maintainers will usually go to considerable lengths to make sure that updates to their packages donâ€™t break your code when youâ€™re using it for its intended purposeâ€¦ but if youâ€™re doing something â€œoff labelâ€ thereâ€™s a good chance that the maintainers wonâ€™t have thought about your particular use case and they might unintentionally break your code.\nIn short: you go off-label at your own risk. In this particular instance it is a risk Iâ€™ve chosen to take and Iâ€™m perfectly happy to fix my scripts if (or, letâ€™s be realistic, when) future updates to rtweet cause them to break. Or possibly abandon my scripts. I knew the risks when I went off-label.\nBut if you follow me along this path you need to be aware of the risk tooâ€¦ donâ€™t go off-label lightly! In my case I didnâ€™t do this on a whim: I chose this path about a year ago out of personal desperation, and Iâ€™ve had to rewrite the scripts a lot in that time. So, please be careful.\nSetting up\nThe first step is to make sure you have the developer version of rtweet: the scripts Iâ€™ve been using rely on the dev version of the package and wonâ€™t work with the current CRAN version. To be precise, Iâ€™m currently using rtweet version 0.7.0.9011. If you donâ€™t have it, this is the command to install:\n\n\nremotes::install_github(\"ropensci/rtweet\")\n\n\n\nThe second step is to authenticate so that rtweet can access private information about your twitter account. The good news here is that the authentication mechanism in the dev version of rtweet is a little more streamlined than it used to be. You only need to authenticate once on your machine, and the command is as simple as this:\n\n\nauth_setup_default()\n\n\n\nWith that, you should be ready to start!\nWrite a block/mute function\nOur first task will be to write a function that can be used either to block or to mute a twitter account. A little whimsically I decided to call it cancel_user(). Quite obviously the name is a personal joke, since it does not â€œcancelâ€ anyone: the only thing blocking or muting accomplishes is to give you a little distance from the account youâ€™re muting or blocking.\nThe reason for wanting one function that can switch between muting and blocking is that I typically run every process twice, once on my primary account (where, with one exception, I donâ€™t block anyone but mute extensively) and once on my private account (where I block quite aggressively). Iâ€™d like to be able to reuse my code in both contexts, so Iâ€™ll design the core function to handle both blocks and mutes. Hereâ€™s the code:\n\n\ncancel_user <- function(user, type) {\n  api <- c(\n    \"block\" = \"/1.1/blocks/create\",\n    \"mute\" = \"/1.1/mutes/users/create\"\n  )\n  rtweet:::TWIT_post(\n    token = NULL,\n    api = api[type],\n    params = list(user_id = user)\n  )\n}\n\n\n\nThereâ€™s quite a bit to unpack here.\nFirst notice that I have called the internal function rtweet:::TWIT_post(). This is the clearest indication that Iâ€™m working off-label. If I were interested only in muting users and never blocking, Iâ€™d be able to do this without going off-label because rtweet has an exported function called post_mute() that you can use to mute an account. However, there is no corresponding post_block() function (possibly for good reasons) so Iâ€™ve written cancel_user() as my personal workaround.\nSecond, letâ€™s look at the interface to the function. Unlike the more sophisticated functions provided by rtweet this is a bare-bones interface. The user argument must be the numerical identifier corresponding to the account you want to block/mute, and type should be either be \"mute\" or \"block\" depending on which action you wish to take.\nFinding the numeric user id code for any given user is straightforward with rtweet. It provides a handy lookup_users() function that you can employ for this. The actual output of the function is a data frame containing a lot of public information about the user, but the relevant information is the user_id variable. So, if you hate me enough to want to mute or block me on twitter, Iâ€™ll make it easy on you. Hereâ€™s my numeric user id:\n\n\nlookup_users(\"djnavarro\")$user_id\n\n\n\n\n[1] \"108899342\"\n\nAs it turns out, for the particular scripts I use, I rarely need to rely on lookup_users() but it is a very handy thing to know about.\nPreparing to scale\nAs written thereâ€™s nothing particularly wrong with the cancel_user() function, but itâ€™s also not very useful. I can use it to block or mute an individual account, sure, but if that were the problem I wanted to solve it would be a lot easier to do that using the block/mute buttons on twitter. I donâ€™t need to write an R function to do that.\nThe only real reason to implement this as an R function is if you intend to automate it in some fashion and repeat the operation on a scale that would be impossible to do manually. To give a sense of the scale at which Iâ€™ve had to implement this I currently have about 220000 accounts blocked from my private account, and a similar number muted from my main account. Thereâ€™s no way I could possibly to that manually, so Iâ€™m going to need to be a little more thoughtful about my cancel_user() function.\nPractice safe cancellation\nThe first step in making sure the function works well â€œat scaleâ€1 is error handling. If I have a list of 50000 account I want to block but for one reason or another cancel_user() throws an error on the 5th account, I donâ€™t want to prevent R from attempting to block the remaining 49995 accounts. Better to catch the error and move on. My preferred way to do this is to use purrr::safely():\n\n\ncancel_safely <- purrr::safely(cancel_user)\n\n\n\nThe cancel_safely() function operates the same way as cancel_user() with one exception. It never throws an error: it always returns a list with two elements, result and error. One of these is always NULL. If cancel_user() throws an error then result is NULL and error contains the error object; if it doesnâ€™t then error is null and result contains the output from cancel_user().\nNot surprisingly, the cancel_safely() function is much more useful when weâ€™re trying to block or mute large numbers of accounts on twitter.\nCheck your quotas my loves\nOne thing that has always puzzled me about the twitter API is that it places rate limits on how many mutes you can post in any 15 minute period, but doesnâ€™t seem to impose any limits on the number of blocks you can post. Iâ€™m sure they have their reasons for doing it, but itâ€™s inconvenient. One consequence of this is that there are lots of tools that exist already for blocking large numbers of accounts. You donâ€™t actually need to write a custom R script for that! But if you want to mute large numbers of accounts, itâ€™s a lot harder: you have to write a script that keeps posting mutes until the rate limits are exceeded, then pauses until they reset, and then starts posting mutes again. Speaking from experience, this takes a very long time. As a purely practical matter, you donâ€™t want to be in the business of muting large numbers of accounts unless you are patient and have a very good reason. In my case, I did.\nIn any case, one thing weâ€™ll need to write a rate_exceeeded() function that returns TRUE if weâ€™ve hit the rate limit and FALSE if we havenâ€™t. Thatâ€™s actually pretty easy to do, as it turns out, because any time our attempt to mute (or block) fails, the cancel_safely() function will catch the error and capture the error message. So all we have to do to write a rate_exceeded() function is to check to see if thereâ€™s an error message, and if there is a message, see if that message informs us that the rate limite has been exceeded. This function accomplishes that goal:\n\n\nrate_exceeded <- function(out) {\n  if(is.null(out$error)) return(FALSE)\n  if(grepl(\"limit exceeded\", out$error$message)) return(TRUE)\n  return(FALSE)\n}\n\n\n\nBecause blocks are not rate limited, in practice this function only applies when youâ€™re trying to mute accounts.\nBe chatty babes\nThe last step in preparing the cancellation function to work well at scale is to make it chatty. In practice, a mass block/mute operation is something you leave running in its own R session, so you want it to leave behind an audit trail that describes its actions. A moderately verbose function is good here. You could make this as sophisticated as you like, but I find this works nicely for me:\n\n\ncancel_verbosely <- function(user, type) {\n\n  # notify user attempt has started\n  msg <- c(\n    \"block\" = \"blocking user id\",\n    \"mute\" = \"muting user id\"\n  )\n  withr::local_options(scipen = 14)\n  cli::cli_process_start(paste(msg[type], user))\n\n  # make the attempt; wait 5 mins if rate limits \n  # exceeded and try again\n  repeat {\n    out <- cancel_safely(user, type)\n    if(rate_exceeded(out)) {\n      Sys.sleep(300)\n    } else {\n      break\n    }\n  }\n\n  # notify user of the outcome\n  if(is.null(out$result)) {\n    cli::cli_process_failed()\n  } else{\n    cli::cli_process_done()\n  }\n}\n\n\n\nHereâ€™s what the output looks like when it successfully blocks a user. Not fancy, but it shows one line per account, specifies whether the action was a block or a mute, and makes clear whether the attempt succeeded or failed:\nâœ“ blocking user id 15xxxx66 ... done\n(where, in the real output the user id for the blocked account is of course not censored). In this function Iâ€™ve used the lovely cli package to create pretty messages at the R command line, but thereâ€™s nothing stopping you from using simpler tools if youâ€™d prefer.\nScaling up\nNow that we have a version of our block/mute function that is suitable for use on a larger scale, itâ€™s time to put it into practice. Letâ€™s say I have a list of 50000 users represented as numeric ids and I want to block (or mute) all these accounts. To do this, Iâ€™ll need a vectorised version of cancellation function. Thanks to the functional programming tools in the purrr package, this is not too difficult. Hereâ€™s the cancel_users() function that I use:\n\n\ncancel_users <- function(users, type) {\n  msg <- c(\"block\" = \"blocking \", \"mute\" = \"muting \")\n  cat(msg[type], length(users), \" users\\n...\")\n  purrr::walk(users, cancel_verbosely, type = type)\n}\n\n\n\nWhen given a vector of user ids, the cancel_users() function will attempt to block/mute them all one at a time. When rate limits are exceeded it will pause and wait for them to reset, and then continue with the process. For mass muting in particular it can take a long time, so itâ€™s the kind of thing you run in its own session while you go do something else with your life. If you want to be clever about it you can make it a background job and sink the output to a log file but honestly Iâ€™m usually too lazy to bother with that: all Iâ€™m trying to do is sanitise my twitter experience, Iâ€™m not deploying production code here.\nThe trickier question is â€œwhere do I get this block list from?â€\nHere, Iâ€™m not going to be too specific, for a couple of reasons. Firstly, I donâ€™t want to be in the business of teaching people how to track down hidden networks of users embedded in social media. Thatâ€™s not something Iâ€™m comfortable doing. Secondly, if youâ€™re doing this defensively (i.e., youâ€™re protecting yourself from attack) then you probably already know something about where the attacks are coming from. You already have your own list of key names, because theyâ€™re the people who keep harassing you. Really, your only goal is to block them and their followers, because the thing thatâ€™s happening is theyâ€™re targeting you and theyâ€™re using their follower base as a weapon. Right? I mean if thatâ€™s not the situation your in, and what youâ€™re actually trying to do is seek out a hidden population to potentially target themâ€¦ yeah Iâ€™m not sure I want to be telling you the other tricks I know. So letâ€™s keep it very simple.\nThe easiest trick in the book (and, honestly, one of the most powerful when youâ€™re trying to block out harassment from a centralised â€œhub-and-spokesâ€ network), is simply to find every account that follows more than \\(k\\) of the \\(n\\) of the key actors, and block/mute them. Actually, in the case of â€œastroturfâ€ organisations that donâ€™t have real grassroots support, you can probably just pick a few of the big names and block (or mute) all their followers. That will eliminate the vast majority of the horribly traffic that youâ€™re trying to avoid. (Yes, I am speaking from experience here!)\nThe rtweet package makes this fairly painless courtesy of the get_followers() function. Twitter makes follower lists public whenever the account itself is public, so you can use get_followers() to return a tibble that contains the user ids for all followers of a particular account.2 Hereâ€™s an example showing how you an write a wrapper around get_followers() and use it to block/mute everyone who follows a particular account:\n\n\ncancel_followers <- function(user, type = \"block\", n_max = 50000, precancelled = numeric(0)) {\n\n  followers <- get_followers(user, n = n_max, retryonratelimit = TRUE)\n  followers <- followers$from_id\n\n  uncancelled <- setdiff(followers, precancelled)\n  uncancelled <- sort(as.numeric(uncancelled))\n\n  cancel_users(uncancelled, type = type)\n}\n\n\n\nNote the precancelled argument to this function. If you have a vector of numeric ids containing users that youâ€™ve already blocked/muted, thereâ€™s no point wasting time and bandwidth trying to block them again, so the function will ignore anything on that list. You could use the same idea to build a whitelist of accounts that would never get blocked or muted regardless of who they follow.\nWeâ€™re almost at the end of the post. Thereâ€™s only one other thing I want to show here, and thatâ€™s how to extract a list of all the accounts you currently have muted or blocked. Again this particular bit of functionality isnâ€™t exposed by rtweet directly, so youâ€™ll notice that Iâ€™ve had to go off-label again and call an unexported function!\n\n\nlist_cancelled <- function(type, n_max, ...) {\n  api <- c(\n    \"block\" = \"/1.1/blocks/ids\",\n    \"mute\" = \"/1.1/mutes/users/ids\"\n  )\n  params <- list(\n    include_entities = \"false\",\n    skip_status = \"true\"\n  )\n  resp <- rtweet:::TWIT_paginate_cursor(NULL, api[type], params, n = n_max, ...)\n  users <- unlist(lapply(resp, function(x) x$ids))\n  return(users)\n}\n\n\n\nIâ€™m not going to expand on this one, other than to mention that when you get to the point where you have hundreds of thousands of blocked or muted accounts, itâ€™s handy to use a function like this from time to time, and to save the results locally so that you can be a little more efficient whenever you next need to refresh your block/mute lists.\nEpilogue\nI wrote this post in two minds. On the one hand, the rtweet developers made a decision not to support blocklists for a reason, and presumably the twitter developers have some reason for making it difficult to mute large numbers of accounts. Itâ€™s very rarely a good idea to write code that works against the clear intent of the tools youâ€™re relying on. It is almost certain to break later on.\nOn the other hand, this is functionality that I personally need. On my primary account Iâ€™ve made the deliberate decision not to block anyone3 but to keep my twitter feed free of a particular type of content I have had to mute an extremely large number of accounts. Twitter makes that difficult to do, but with the help of these scripts I managed to automate the process. After a month or two, with a little manual intervention, the problematic content was gone from my feed, and I was able to get back to doing my job. So, if anyone else finds themselves in a similar situation, hopefully this blog post will help.\n\n\nLast updated\n2022-02-12 11:11:37 AEDT\nDetails\nsource code, R environment\n\n\n\n\n\n\nI mean, what exactly do we mean by â€œat scaleâ€ here? In the context of data wrangling, a hundred thousand anything is rarely considered â€œat scaleâ€. But mute/blocks on twitter are usually measured in the tens or hundreds at most. Doing things at the hundreds of thousands scale is a big step up from the typical use case, and as noted later, because of how the twitter API handles mutes itâ€™s something that can take months to complete.â†©ï¸\nThis is not unrelated to the reason why I keep my follows hidden behind private lists. In public it looks like I donâ€™t follow anyone but using private lists I actually follow several hundred people! Some time ago I had some unpleasant experiences with people using that information to target me, so now I use private lists exclusively. Being a trans woman on the internet is fuuuuuuuun.â†©ï¸\nThere is one exception to this rule, but thatâ€™s a personal matter.â†©ï¸\n",
    "preview": "posts/2022-02-11_r-scripts-for-twitter-blocks/img/jeremy-bezanger-Nh1tBGgEcG4-unsplash.jpg",
    "last_modified": "2022-02-12T11:11:37+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-18_binding-arrow-to-r/",
    "title": "Binding Apache Arrow to R",
    "description": "I've been learning how to program with Apache Arrow inside R, and also I have been watching the SyFy show \"The Magicians\" obsessively. For no sensible reason I wrote a blog post that combines these two fixations",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2022-01-18",
    "categories": [],
    "contents": "\n\nContents\nWhy am I writing this?\nWhat is Arrow?\nUsing Arrow in R\nUsing â€œarrowplyrâ€ bindings\nCalling â€œarrow-prefixâ€ functions\nA slightly-evil digression\nPart one: Object oriented programming\nPart two: Metaprogramming\nCooperative magic\n\nCalling libarrow directly\nArrow expressions\nWriting arrowplyr functions\nEpilogue: Whereâ€™s the rest of the owl?\n\n\n\n\nSo I have a new job.\nIn my previous job as an academic, a large part of my work â€“ my favourite part, if Iâ€™m honest â€“ involved creating open access resources to help people use modern open source tools for data analysis. In my totally different role in developer relations at Voltron Data, a large part of my work involves, um â€¦ [checks job description] â€¦ creating open access resources to help people use modern open source tools for data analysis. Well okay then!\nIâ€™d better get on that, I suppose?\n\n\n\nFigure 1: Eliot Waugh. All I can say at this point is that thanks to his magnificent performance I have developed a terribly awkward crush on Hale Appleman. Image via giphy, copyright syfy\n\n\n\nYes I have been binge watching The Magicians lately. My preemptive apologies to everyone for the gif spam.\nIâ€™ve been in my current role for a little over a week (or had been when I started writing this post!), and today my first contribution to Apache Arrow was merged. It was very modest contribution: I wrote some code that determines whether any given year is a leap year. It precisely mirrors the behaviour of the leap_year() function in the lubridate package, except that it can be applied to Arrow data and it will behave itself when used in the context of a dplyr pipeline (more on that later). The code itself is not complicated, but it relies on a little magic and a deeper understanding of Arrow than I possessed two weeks ago.\n\nThroughout this post Iâ€™ll use boldface to refer to specific R packages like dplyr or C++ libraries like libarrow\nThis post is the story of how I learned Arrow magic. âœ¨ ğŸ¹ âœ¨\n\nWhy am I writing this?\n\n\nThe danger of sublimated trauma is a major theme in our story  Â  Â  â€“ The Great God Ember (The Magicians: Season 2, Episode 3)\n\nIt might seem peculiar that Iâ€™m writing such a long post about such a tiny contribution to an open source project. After all, it doesnâ€™t actually take a lot of work to figure out how to detect leap years. You can do it in one line of R code:\n\n\n(year %% 4 == 0) & ((year %% 100 != 0) | (year %% 400 == 0))\n\n\n\nThis is a logical expression corresponding to the following rules. If the year is divisible by 4 then it is a leap year (e.g., 1996 was a leap year). But thereâ€™s an exception: if year is divisible by 100 then it isnâ€™t a leap year (e.g., 1900 wasnâ€™t a leap year). But thereâ€™s also an exception to the exception: if year is divisible by 400 then it is a leap year (e.g., 2000 was a leap year). Yes, the process of mapping the verbally stated rules onto a logical expression is kind of annoying, but itâ€™s not conceptually difficult or unusual. There is no magic in leap year calculation, no mystery that needs unpacking and explaining.\n\nAll this assumes years are counted using the Gregorian calendar. There are, of course, other calendars\nThe magic comes in when you start thinking about what the arrow package actually does. It lets you write perfectly ordinary R code for data manipulation that returns perfectly ordinary R data structures, even though the data have never been loaded into R and all the computation is performed externally using Apache Arrow. The code you write with arrow looks and feels like regular R code, but almost none of the work is being done by R. This is deep magic, and it is this magic that needs to be demystified.\n\n\n\nFigure 2: Two key moments in â€œThe Magiciansâ€ when Julia Wicker discovers she can do magic, defying the expectations of others. One moment occurs at the start of Season 1 as a novice, after she had been told she failed the magic exams at Brakebills University; another moment occurs at the end of Season 2 after all magic has supposedly been turned off by the Old Gods or something. The parallel between the two moments is striking. Oh and Quentin Coldwater is in both scenes too I guess. Whatevs. Image via giphy, copyright syfy\n\n\n\nI have three reasons for wanting to unpack and explain this magic.\nThe first reason is personal: Iâ€™ve been a professional educator for over 15 years and it has become habit. The moment I learn a new thing my first impulse is to work out how to explain it to someone else.\nThe second reasons is professional: I work for Voltron Data now, and part of my job is to make an educational contribution to the open source Apache Arrow project. Arrow is a pretty cool project, but thereâ€™s very little value in magnificent software if you donâ€™t help people learn how to take advantage of it!\nThe third reason is ethical: a readable tutorial/explanation lowers barriers to entry. I mean, letâ€™s be honest: the only reason I was able to work up the courage to contribute to Apache Arrow is that I work for a company that is deeply invested in open source software and in the Arrow project specifically. I had colleagues and friends I could ask for advice. If I failed I knew they would be there to help me. I had a safety net.\nThe last of these is huuuuuuugely important from a community point of view. Not everyone has the safety net that I have, and it makes a big difference. In a former life Iâ€™ve been on the other side of this divide: Iâ€™ve been the person with no support, nobody to ask for help, and Iâ€™ve run afoul of capricious gatekeeping in the open source world. It is a deeply unpleasant experience, and one I would not wish upon anyone else. We lose good people when this happens, and I really donâ€™t want that!\nThe quote from the beginning of this section, the one about the danger of sublimated trauma, is relevant here: if we want healthy user communities it is our obligation on the inside to provide safe environments and delightful experiences. Our job is to find and remove barriers to entry. We want to provide that â€œsafety netâ€ that ensures that even if you fall (because we all fall sometimes), you donâ€™t get hurt. Failing safely at something can be a learning experience; suffering trauma, however, is almost never healthy. So yeah, this matters to me. I want to take what Iâ€™ve learned now that Iâ€™m on the inside and make that knowledge more widely accessible.\n\nEveryone deserves a safety net when first learning to walk the tightropes. Itâ€™s not a luxury, itâ€™s a necessity\nBefore diving in, I should say something about the â€œassumed knowledgeâ€ for this post.\nIâ€™ll do my best to explain R concepts as I go, but the post does assume that the reader is comfortable in R and knows how to use dplyr for data manipulation. If you need a refresher on these topics, I cannot recommend â€œR for data scienceâ€ highly enough. Itâ€™s a fabulous resource!\nOn the Arrow side it would help a little if you have some vague idea of what Arrow is about. I will of course explain as I go, but if youâ€™re looking for a post that starts at the very beginning, I wrote a post on â€œGetting started with Apache Arrowâ€ that does exactly this and discusses a lot of the basics.\nFinally, a tiny R warning: later in the post I will do a little excursion into object oriented programming and metaprogramming in R, which will be familiar to some but not all readers. If youâ€™re not comfortable with these topics, you should still be okay to skim those sections and still get the important parts of this post. Itâ€™s not essential to understand the main ideas.\n\n\n\nFigure 3: The Great God Ember. Capricious, chaotic, and utterly unreliable unless what youâ€™re looking for is a whimsical death. Pretty much the opposite of what weâ€™d hope for in a healthy open source community really! He is, however, a very entertaining character. Image via giphy, copyright syfy\n\n\n\n\nWhat is Arrow?\n\nIn case you decided not to read the introductory â€œGetting started with Apache Arrowâ€ post, hereâ€™s an extremely condensed version. Apache Arrow is a standard and open source library that represents tabular data efficiently in memory. More generally it refers to a collection of tools used to work with Arrow data. There are libraries supporting Arrow in many different programming languages, including C, C++, C#, Go, Java, JavaScript, Julia, MATLAB, Python, R, Ruby, and Rust. Itâ€™s pretty cool.\n\nUsing Arrow in R\n\nA fundamental thing to understand about the arrow package in R is that it doesnâ€™t implement the Apache Arrow standard directly. In fact, it tries very hard not to do any of the heavy lifting itself. Thereâ€™s a C++ library that does that in a super efficient way, and the job of the R package is to supply bindings that allow the R user to interact with that library using a familiar interface. The C++ library is called libarrow. Although the long term goal is to make the integration so seamless that you can use the arrow R package without ever needing to understand the C++ library, my experience has been that most people want to know something about whatâ€™s happening under the hood. It can be unsettling to find yourself programming with tools you donâ€™t quite understand, so Iâ€™ll dig a little deeper in this post.\nLetâ€™s start with the C++ library. The role of libarrow is to do all the heavy computational work. It implements all the Arrow standards for representing tabular data in memory, provides support for the Apache â€œInter-Process Communicationâ€ (IPC) protocol that lets you efficiently transfer data from one application to another, and supplies various compute kernels that allow you to do some data wrangling when your data are represented as an Arrow table.1 It is, fundamentally, the engine that makes everything work.\nWhat about the R package? The role of arrow is to expose the functionality of libarrow to the R user, to make that functionality feel â€œnaturalâ€ in R, and to make it easier for R users to write Arrow code that is smoothly interoperable with Arrow code written in other languages (e.g., Python). In order to give you the flexibility you need, the arrow package allows you to interact with libarrow at three different levels of abstraction:\nThereâ€™s a heavily abstracted interface that uses the dplyr bindings supplied by arrow. This version strives to make libarrow almost completely invisible, hidden behind an interface that uses familiar R function names.\nThereâ€™s a lightly abstracted interface you can access using the arrow_*() functions. This version exposes the libarrow functions without attempting to exactly mirror any particular R functions, and provides a little syntactic sugar to make your life easier.\nFinally, thereâ€™s a minimally abstracted interface using call_function(). This version provides a bare bones interface, without any of the syntactic sugar.\nOver the next few sections section Iâ€™ll talk about these three levels of abstraction. So letâ€™s load the packages weâ€™re going to need for this post and dive right in!\n\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(arrow)\n\n\n\n\n\n\nFigure 4: Penny Adiyodi in the Neitherlands, diving head first into a fountain that transports him to a new and magical world. I cannot stress enough that Penny does not, by and large, make good choices. Impulse control is a virtue, but not one that he possesses in abundance. Image via giphy, copyright syfy\n\n\n\n\nUsing â€œarrowplyrâ€ bindings\n\n\nI think I have some special fish magicks. Â  Â  â€“ Josh Hoberman (The Magicians: Season 4, Episode 13)\n\nWhen I wrote my Getting started with Apache Arrow post, I concluded with an illustration of how you can write dplyr code that will work smoothly in R even when the data themselves are stored in Arrow. Hereâ€™s a little recap of how that works, using a tiny data set I pulled from The Magicians Wikipedia page. Hereâ€™s what that data set looks like:\n\n\nmagicians <- read_csv_arrow(\"magicians.csv\", as_data_frame = TRUE)\nmagicians\n\n\n# A tibble: 65 Ã— 6\n   season episode title                      air_date   rating viewers\n    <int>   <int> <chr>                      <date>      <dbl>   <dbl>\n 1      1       1 Unauthorized Magic         2015-12-16    0.2    0.92\n 2      1       2 The Source of Magic        2016-01-25    0.4    1.11\n 3      1       3 Consequences of Advanced â€¦ 2016-02-01    0.4    0.9 \n 4      1       4 The World in the Walls     2016-02-08    0.3    0.75\n 5      1       5 Mendings, Major and Minor  2016-02-15    0.3    0.75\n 6      1       6 Impractical Applications   2016-02-22    0.3    0.65\n 7      1       7 The Mayakovsky Circumstanâ€¦ 2016-02-29    0.3    0.7 \n 8      1       8 The Strangled Heart        2016-03-07    0.3    0.67\n 9      1       9 The Writing Room           2016-03-14    0.3    0.71\n10      1      10 Homecoming                 2016-03-21    0.3    0.78\n# â€¦ with 55 more rows\n\nIn the code above I used the read_csv_arrow() function from the arrow package. If youâ€™ve used the read_csv() function from readr this will seem very familiar: although Arrow C++ code is doing a lot of the work under the hood, the Arrow options have been chosen to mirror the familiar readr interface. The as_data_frame argument is specific to arrow though: when it is TRUE the data are imported into R as a data frame or tibble, and when it is FALSE the data are imported into Arrow. Strictly speaking I didnâ€™t need to specify it in this example because TRUE is the default value. I ony included here so that I could draw attention to it.\nOkay, now that we have the data letâ€™s start with a fairly typical data analysis process: computing summary variables. Perhaps I want to know the average popularity and ratings for each season of The Magicians, and extract the year in which the season aired. The dplyr package provides me with the tools I need to do this, using functions like mutate() to create new variables, group_by() to specify grouping variables, and summarise() to aggregate data within group:\n\n\nmagicians %>% \n  mutate(year = year(air_date)) %>% \n  group_by(season) %>% \n  summarise(\n    viewers = mean(viewers),\n    rating = mean(rating), \n    year = max(year)\n  )\n\n\n# A tibble: 5 Ã— 4\n  season viewers rating  year\n   <int>   <dbl>  <dbl> <dbl>\n1      1   0.776  0.308  2016\n2      2   0.788  0.323  2017\n3      3   0.696  0.269  2018\n4      4   0.541  0.2    2019\n5      5   0.353  0.111  2020\n\nAll of these computations take place within R. The magicians data set is stored in R, and all the calculations are done using this data structure.\nWhat can we do when the data are stored in Arrow? It turns out the code is almost identical, but the first thing Iâ€™ll need to do is load the data into Arrow. The simplest way to do this is to set as_data_frame = FALSE when calling arrow_read_csv()\n\n\narrowmagicks <- read_csv_arrow(\"magicians.csv\", as_data_frame = FALSE)\narrowmagicks\n\n\nTable\n65 rows x 6 columns\n$season <int64>\n$episode <int64>\n$title <string>\n$air_date <date32[day]>\n$rating <double>\n$viewers <double>\n\n\nThe â€œarrowmagicksâ€ variable name is a reference to the quote at the start of the section. For a while Josh was convinced he had been gifted with special magic because he had been a fish. It made sense at the time, I guess? Itâ€™s a weird show\nWhen I do this, two things happen. First, a data set is created outside of R in memory allocated to Arrow: all of the computations will be done on that data set. Second, the arrowmagicks variable is created inside R, which consists of a pointer to the actual data along with some handy metadata.\nThe most natural way to work with this data in R is to make sure that both the arrow and dplyr packages are loaded, and then write regular dplyr code. You can do this because the arrow package supplies methods for dplyr functions, and these methods will be called whenever the input data is an Arrow Table. Iâ€™ll refer to this data analyses that use this workflow as â€œarrowplyr pipelinesâ€. Hereâ€™s an example of an arrowplyr pipeline:\n\nIâ€™ve chosen not to boldface the â€œarrowplyrâ€ terminology. arrow is a package and dplyr is a package, but arrowplyr isnâ€™t. Itâ€™s simply a convenient fiction\n\n\narrowmagicks %>% \n  mutate(year = year(air_date)) %>% \n  group_by(season) %>% \n  summarise(\n    viewers = mean(viewers),\n    rating = mean(rating), \n    year = max(year)\n  )\n\n\nInMemoryDataset (query)\nseason: int64\nviewers: double\nrating: double\nyear: int64\n\nSee $.data for the source Arrow object\n\nIt looks like a regular dplyr pipeline, but because the input is arrowmagicks (an Arrow Table object), the effect of this is construct a query that can be passed to libarrow to be evaluated.\nItâ€™s important to realise that at this point, all we have done is define a query: no computations have been performed on the Arrow data. This is a deliberate choice for efficiency purposes: on the C++ side there are a lot of performance optimisations that are only possible because libarrow has access to the entire query before any computations are performed. As a consequence of this, you need to explicitly tell Arrow when you want to pull the trigger and execute the query.\n\nLater in the post Iâ€™ll talk about Arrow Expressions, the tool that powers this trickery\nThere are two ways to trigger query execution, one using the compute() function and the other using collect(). These two functions behave slightly differently and are useful for different purposes. The compute() function runs the query, but leaves the resulting data inside Arrow:\n\n\narrowmagicks %>% \n  mutate(year = year(air_date)) %>% \n  group_by(season) %>% \n  summarise(\n    viewers = mean(viewers),\n    rating = mean(rating), \n    year = max(year)\n  ) %>% \n  compute()\n\n\nTable\n5 rows x 4 columns\n$season <int64>\n$viewers <double>\n$rating <double>\n$year <int64>\n\nThis is useful whenever youâ€™re creating an intermediate data set that you want to reuse in Arrow later, but donâ€™t need to use this intermediate data structure inside R. If, however, you want the output to be pulled into R so that you can do R computation with it, use the collect() function:\n\n\narrowmagicks %>% \n  mutate(year = year(air_date)) %>% \n  group_by(season) %>% \n  summarise(\n    viewers = mean(viewers),\n    rating = mean(rating), \n    year = max(year)\n  ) %>% \n  collect()\n\n\n# A tibble: 5 Ã— 4\n  season viewers rating  year\n   <int>   <dbl>  <dbl> <int>\n1      1   0.776  0.308  2016\n2      2   0.788  0.323  2017\n3      3   0.696  0.269  2018\n4      4   0.541  0.2    2019\n5      5   0.353  0.111  2020\n\nThe nice thing for R users is that all of this feels like regular R code. Under the hood libarrow is doing all the serious computation, but at the R level the user really doesnâ€™t need to worry too much about that. The arrowplyr toolkit works seamlessly and invisibly.\nIn our ideal world, the arrowplyr interface is all you would ever need to use. Internally, the arrow package would intercept all the R function calls you make, and replace them with an equivalent function that performs exactly the same computation using libarrow. You the user would never need to think about whatâ€™s happening under the hood.\nReal life, however, is filled with leaky abstractions, and arrowplyr is no exception. Because itâ€™s a huge project that is under active development, thereâ€™s a lot of functionality being introduced. As an example, the current version of the package (v6.0.1) has limited support for tidyverse packages like lubridate and stringr. Itâ€™s awesome that this functionality is coming online, but because itâ€™s happening so quickly there are gaps. The small contribution that I made today was to fill one of those gaps: currently, you canâ€™t refer to the leap_year() function from lubridate in an arrowplyr pipeline. Well, technically you can, but whenever arrow encounters a function it doesnâ€™t know how to execute in Arrow it throws a warning, pulls the data into R, and completes the query using native R code. Hereâ€™s what that looks like:\n\n\narrowmagicks %>% \n  mutate(\n    year = year(air_date), \n    leap = leap_year(air_date)\n  ) %>% \n  collect()\n\n\nWarning: Expression leap_year(air_date) not supported in Arrow;\npulling data into R\n# A tibble: 65 Ã— 8\n   season episode title          air_date   rating viewers  year leap \n    <int>   <int> <chr>          <date>      <dbl>   <dbl> <dbl> <lgl>\n 1      1       1 Unauthorized â€¦ 2015-12-16    0.2    0.92  2015 FALSE\n 2      1       2 The Source ofâ€¦ 2016-01-25    0.4    1.11  2016 TRUE \n 3      1       3 Consequences â€¦ 2016-02-01    0.4    0.9   2016 TRUE \n 4      1       4 The World in â€¦ 2016-02-08    0.3    0.75  2016 TRUE \n 5      1       5 Mendings, Majâ€¦ 2016-02-15    0.3    0.75  2016 TRUE \n 6      1       6 Impractical Aâ€¦ 2016-02-22    0.3    0.65  2016 TRUE \n 7      1       7 The Mayakovskâ€¦ 2016-02-29    0.3    0.7   2016 TRUE \n 8      1       8 The Strangledâ€¦ 2016-03-07    0.3    0.67  2016 TRUE \n 9      1       9 The Writing Râ€¦ 2016-03-14    0.3    0.71  2016 TRUE \n10      1      10 Homecoming     2016-03-21    0.3    0.78  2016 TRUE \n# â€¦ with 55 more rows\n\n\nThis is a bit of an oversimplification. The â€œwarn and pull into Râ€ behaviour shown here is what happens when the data is a Table object. If it is a Dataset object, arrow throws an error\nAn answer has been calculated, but the warning is there to tell you that the computations werenâ€™t performed in Arrow. Realising that it doesnâ€™t know how to interpret leap_year(), the arrow package has tried to â€œfail gracefullyâ€ and pulled everything back into R. The end result of all this is that the code executes as a regular dplyr pipeline and not as an arrowplyr one. Itâ€™s not the worst possible outcome, but it still makes me sad. ğŸ˜­\n\n\n\nFigure 5: Quentin from timeline 40 talking to Alice from timeline 23. Communication across incommensurate universes is difficult. In the show it requires a Tesla Flexion. In Arrow, we use dplyr bindings. Image via giphy, copyright syfy\n\n\n\n\nCalling â€œarrow-prefixâ€ functions\n\nOkay, letâ€™s dig a little deeper.\nIn the last section I talked about arrowplyr, a collection of dplyr bindings provided by the arrow package. These are designed to mimic their native R equivalents as seamlessly as possible to enable you to write familiar code. Internally, thereâ€™s quite a lot going on to make this magic work. In most cases, the arrow developers â€“ which I guess includes me now! ğŸ‰ â€“ have rewritten the R functions that they mimic. Weâ€™ve done this in a way that the computations rely only the C++ compute functions provided by libarrow, thereby ensuring that the data never have to enter R. The arrowplyr interface is the way youâ€™d usually interact with Arrow in R, but there are ways in which you can access the C++ compute functions a little more more directly. There are two different ways you can call these compute functions yourself. If youâ€™re working within an arrowplyr pipeline it is (relatively!) straightforward, and thatâ€™s what Iâ€™ll talk about in this section. However, there is also a more direct method which Iâ€™ll discuss later in the post.\nTo see what compute functions are exposed by the C++ libarrow library, you can call list_compute_functions() from R:\n\n\nlist_compute_functions()\n\n\n  [1] \"abs\"                            \n  [2] \"abs_checked\"                    \n  [3] \"acos\"                           \n  [4] \"acos_checked\"                   \n  [5] \"add\"                            \n  [6] \"add_checked\"                    \n  [7] \"all\"                            \n  [8] \"and\"                            \n  [9] \"and_kleene\"                     \n [10] \"and_not\"                        \n....\n\nThe actual output continues for quite a while: there are currently 221 compute functions, most of which are low level functions needed to perform basic computational operations.\nLetâ€™s imagine youâ€™re writing dplyr code to work with datetime data in a Table object like arrowmagicks. If you were working with native R data like magicians, you can do something like this:\n\n\nstart_date <- as.Date(\"2015-12-16\")\n\nmagicians %>% \n  mutate(days = air_date - start_date)\n\n\n# A tibble: 65 Ã— 7\n   season episode title                air_date   rating viewers days \n    <int>   <int> <chr>                <date>      <dbl>   <dbl> <drt>\n 1      1       1 Unauthorized Magic   2015-12-16    0.2    0.92  0 dâ€¦\n 2      1       2 The Source of Magic  2016-01-25    0.4    1.11 40 dâ€¦\n 3      1       3 Consequences of Advâ€¦ 2016-02-01    0.4    0.9  47 dâ€¦\n 4      1       4 The World in the Waâ€¦ 2016-02-08    0.3    0.75 54 dâ€¦\n 5      1       5 Mendings, Major andâ€¦ 2016-02-15    0.3    0.75 61 dâ€¦\n 6      1       6 Impractical Applicaâ€¦ 2016-02-22    0.3    0.65 68 dâ€¦\n 7      1       7 The Mayakovsky Circâ€¦ 2016-02-29    0.3    0.7  75 dâ€¦\n 8      1       8 The Strangled Heart  2016-03-07    0.3    0.67 82 dâ€¦\n 9      1       9 The Writing Room     2016-03-14    0.3    0.71 89 dâ€¦\n10      1      10 Homecoming           2016-03-21    0.3    0.78 96 dâ€¦\n# â€¦ with 55 more rows\n\nHere Iâ€™ve created a new days column that counts the number of days that have elapsed between the air_date for an episode and the start_date (December 16th, 2015) when the first episode of Season 1 aired. There are a lot of data analysis situations in which you might want to do something like this, but right now you canâ€™t actually do this using the arrow dplyr bindings because temporal arithmetic is a work in progress. In the not-too-distant future users should be able to expect code like this to work seamlessly, but right now it doesnâ€™t. If you try it right now, you get this error:\n\nImproving support for date/time calculations is one of the things Iâ€™m working on\n\n\narrowmagicks %>% \n  mutate(days = air_date - start_date) %>% \n  collect()\n\n\nError: NotImplemented: Function subtract_checked has no kernel matching input types (array[date32[day]], scalar[date32[day]])\n\nRight now, there are no general purpose arithmetic operations in arrow that allow you to subtract one date from another. However, because I chose this example rather carefully to find an edge case where the R package is missing some libarrow functionality, it turns out there is actually a days_between() function in libarrow that we could use to solve this problem, and itâ€™s not too hard to use it. If you want to call one of the libarrow functions inside your dplyr pipeline, all you have to do is add an arrow_ prefix to the function name. For example, the C++ days_between() function becomes arrow_days_between() when called within the arrow dplyr pipeline:\n\n\narrowmagicks %>% \n  mutate(days = arrow_days_between(start_date, air_date)) %>% \n  collect()\n\n\n# A tibble: 65 Ã— 7\n   season episode title                air_date   rating viewers  days\n    <int>   <int> <chr>                <date>      <dbl>   <dbl> <int>\n 1      1       1 Unauthorized Magic   2015-12-16    0.2    0.92     0\n 2      1       2 The Source of Magic  2016-01-25    0.4    1.11    40\n 3      1       3 Consequences of Advâ€¦ 2016-02-01    0.4    0.9     47\n 4      1       4 The World in the Waâ€¦ 2016-02-08    0.3    0.75    54\n 5      1       5 Mendings, Major andâ€¦ 2016-02-15    0.3    0.75    61\n 6      1       6 Impractical Applicaâ€¦ 2016-02-22    0.3    0.65    68\n 7      1       7 The Mayakovsky Circâ€¦ 2016-02-29    0.3    0.7     75\n 8      1       8 The Strangled Heart  2016-03-07    0.3    0.67    82\n 9      1       9 The Writing Room     2016-03-14    0.3    0.71    89\n10      1      10 Homecoming           2016-03-21    0.3    0.78    96\n# â€¦ with 55 more rows\n\nNotice thereâ€™s no warning message here? Thatâ€™s because the computations were done in Arrow and the data have not been pulled into R.\n\nA slightly-evil digression\n\n\nMarina, blatantly lying: Â Â Â  â€œHi. Iâ€™m Marina. Iâ€™m here to help.â€  Josh, missing important memories: Â Â Â  â€œSo youâ€™re like some powerful, benevolent White Witch?â€  Marina, comically sincere: Â Â Â  â€œUh-huh.â€  Â  Â  â€“ The Magicians: Season 4, Episode 2\n\n\nAt this point in the show everybody except the currently-amnesic main characters knows that Marina has no interest in helping anyone except Marina. I love Marina so much\nOkay, hereâ€™s a puzzle. In the previous section I used the arrow_days_between() function in the middle of a dplyr pipe to work around a current limitation in arrow. What happens if I try to call this function in another context?\n\n\ntoday <- as.Date(\"2022-01-18\")\n\narrow_days_between(start_date, today)\n\n\nError in arrow_days_between(start_date, today): could not find function \"arrow_days_between\"\n\nIt turns out there is no R function called arrow_days_between(). This is â€¦ surprising, to say the least. I mean, it really does look like I used this function in the last section, doesnâ€™t it? How does this work? The answer to this requires a slightly deeper understanding of what the dplyr bindings in arrow do, and itâ€™s kind of a two-part answer.\nPart one: Object oriented programming\nLetâ€™s consider the mutate() function. dplyr defines mutate() as an S3 generic function, which allows it to display â€œpolymorphismâ€: it behaves differently depending on what kind of object is passed to to the generic. When you pass a data frame to mutate(), the call is â€œdispatchedâ€ to the mutate.arrow_dplyr_query() methods supplied by (but not exported by) dplyr. The arrow package builds on this by supplying methods that apply for Arrow objects. Specifically, there are internal functions mutate.ArrowTabular(), mutate.Dataset(), and mutate.arrow_dplyr_query() that are used to provide mutate() functionality for Arrow data sets. In other words, the â€œtop levelâ€ dplyr functions in arrow are S3 methods, and method dispatch is the mechanism that does the work.\nPart two: Metaprogramming\nNow letâ€™s consider the leap_year() function that my contribution focused on. Not only is this not a generic function, itâ€™s not even a dplyr function. Itâ€™s a regular function in the lubridate package. So how is it possible for arrow to mimic the behaviour of lubridate::leap_year() without messing up lubridate? This is where the dplyr binding part comes in. Letâ€™s imagine that Iâ€™d written an actual function called arrowish_leap_year() that performs leap year calculations for Arrow data. If Iâ€™d done this inside the arrow package2 then Iâ€™d include a line like this to register a binding:\n\nIâ€™ll show you how to write your own â€œarrowishâ€ functions later in the post\n\n\nregister_binding(\"leap_year\", arrowish_leap_year)\n\n\n\nOnce the binding has been registered, whenever leap_year() is encountered within one of the arrow-supplied dplyr functions, R will substitute my arrowish_leap_year() function in place of the lubridate::leap_year() function that would normally be called. This is only possible because R has extremely sophisticated metaprogramming tools: you (the developer) can write functions that â€œcaptureâ€ the code that the user input, and if necessary modify that code before R evaluates it. This is a very powerful tool for constructing domain-specific languages within R. The tidyverse uses it extensively, and the arrow package does too. The dplyr bindings inside arrow use metaprogramming tricks to modify the user input in such a way that â€“ in this example â€“ the user input is interpreted as if the user had called arrowish_leap_year() rather than leap_year().\nCooperative magic\nTaken together, these two pieces give us the answer to our puzzle. The call to arrow_days_between() works in my original example because that call was constructed within the context of an arrow-supplied mutate() function. The interpretation of this code isnâ€™t performed by dplyr it is handled by arrow. Internally, arrow uses metaprogramming magic to ensure that arrow_days_between() is reinterpreted as a call to the libarrow days_between() function. But that metaprogramming magic doesnâ€™t apply anywhere except the arrowplyr context. If you try to call arrow_days_between() from the R console or even in a regular dplyr pipeline, you get an error because technically speaking this function doesnâ€™t exist.\n\n\n\nFigure 6: I guess thereâ€™s a connection between slightly-evil-Julia burning down the talking trees and my slightly-evil digression? Sort of. I mean the truth is just that I just love this scene and secretly wish I was her. Of all the characters Julia has the most personally transformative arc (in my opinion), in both good ways and bad. Thereâ€™s a lot going on with her life, her person, and her body. I relate to that. Image via giphy, copyright syfy\n\n\n\n\nCalling libarrow directly\n\nThe weirdness of that digression leads naturally to a practical question. Given that the â€œarrow-prefixâ€ function donâ€™t actually exist in the usual sense of the term, and the corresponding bindings can only be called in an arrowplyr context, how the heck does an R developer call the libarrow functions directly? In everyday data analysis you wouldnâ€™t want to do this very often, but from a programming perspective it matters: if you want to write your own functions that play nicely with arrowplyr pipelines, itâ€™s very handy to know how to call libarrow directly.\nSo letâ€™s strip back another level of abstraction!\nShould you ever find yourself wanting to call libarrow compute functions directly from R, call_function() will become your new best friend. It provides a very minimal interface that exposes the libarrow functions to R. The â€œbare bonesâ€ nature of this interface has advantages and disadvantages. The advantage is simplicity: your code doesnâ€™t depend on any of the fancy bells and whistles. Those are fabulous from the user perspective, but from a developer point of view you usually want to keep it simple. The price you pay for this is that you must pass appropriate Arrow objects. You canâ€™t pass a regular R object to a libarrow function and expect it to work. For example:\n\n\ncall_function(\"days_between\", start_date, today)\n\n\nError: Argument 1 is of class Date but it must be one of \"Array\", \"ChunkedArray\", \"RecordBatch\", \"Table\", or \"Scalar\"\n\nThis doesnâ€™t work because start_date and today are R-native Date objects and do not refer to any data structures in Arrow. The libarrow functions expect to receive pointers to Arrow objects. To fix the previous example, all we need to do is create Arrow Scalars for each date. Hereâ€™s how we do that:\n\n\narrow_start_date <- Scalar$create(start_date)\narrow_today <- Scalar$create(today)\n\narrow_start_date\n\n\nScalar\n2015-12-16\n\nThe arrow_start_date and arrow_today variables are R data structures, but theyâ€™re only thin wrappers. The actual data are stored in Arrow, and the R objects are really just pointers to the Arrow data. These objects are suitable for passing to the libarrow days_between() function, and this works:\n\n\ncall_function(\"days_between\", arrow_start_date, arrow_today)\n\n\nScalar\n2225\n\nHuh. Apparently it took me over 2000 days to write a proper fangirl post about The Magicians. Iâ€™m really late to the pop culture party, arenâ€™t I? Oh dear. Iâ€™m getting old.\n\n\n\nFigure 7: Iâ€™m getting lazier with these connections. Using a Library gif because Iâ€™m talking about the C++ library? I mean really, youâ€™d think Iâ€™d be better than that wouldnâ€™t you? But no. I am not. Image via giphy, copyright syfy\n\n\n\n\nArrow expressions\n\nThereâ€™s one more foundational topic I should discuss before I can show you how to write arrowplyr-friendly functions, and thatâ€™s Arrow Expressions. When I introduced arrowplyr early in the post I noted that most of your code is used to specify a query, and that query doesnâ€™t get evaluated until compute() or collect() is called. If you want to write code that plays nicely with this workflow, you need to ensure that your custom functions return an Arrow Expression.\nThe basic idea behind expressions is probably familiar to R users, since they are what powers the metaprogramming capabilities of the language and are used extensively throughout tidyverse as well as base R. In base R, the quote() function is used to capture a user expression and eval() is used to force it to evaluate. Hereâ€™s a simple example where I use quote() to â€œcaptureâ€ some R code and prevent it from evaluating:\n\n\nhead_expr <- quote(head(magicians, n = 3))\nhead_expr\n\n\nhead(magicians, n = 3)\n\nIf I wanted to be clever I could modify the code in head_expr before allowing R to pull the trigger on evaluating it. I could combine a lot of expressions together, change parts of the code as needed, and evaluate them wherever I wanted. As you might imagine, this is super useful for creating domain specific languages within R. But this isnâ€™t a post about metaprogramming so letâ€™s evaluate it now:\n\n\neval(head_expr)\n\n\n# A tibble: 3 Ã— 6\n  season episode title                       air_date   rating viewers\n   <int>   <int> <chr>                       <date>      <dbl>   <dbl>\n1      1       1 Unauthorized Magic          2015-12-16    0.2    0.92\n2      1       2 The Source of Magic         2016-01-25    0.4    1.11\n3      1       3 Consequences of Advanced Sâ€¦ 2016-02-01    0.4    0.9 \n\nThe example above uses native R code. Itâ€™s not tied to Arrow in any sense. However, the arrow package provides a mechanism for doing something similar in an Arrow context. For example, hereâ€™s me creating a character string as an Arrow Scalar:\n\n\nfillory <- Scalar$create(\"A world as intricate as filigree\")\nfillory\n\n\nScalar\nA world as intricate as filigree\n\nHereâ€™s me creating the corresponding object within an Arrow Expression:\n\n\nfillory <- Expression$scalar(\n  Scalar$create(\"A world as intricate as filigree\")\n)\nfillory\n\n\nExpression\n\"A world as intricate as filigree\"\n\nI suspect this would not seem particularly impressive on its own, but you can use the same idea to create function calls that can be evaluated later within the Arrow context:\n\n\nember <- Expression$create(\"utf8_capitalize\", fillory)\nember\n\n\nExpression\nutf8_capitalize(\"A world as intricate as filigree\")\n\nSo close. We are so very close to the end now.\n\n\n\nFigure 8: Okay look, Iâ€™ll level with you. At this point there is absolutely no connection between the gifs and the content. This post is getting very long and my brain is fried. I need a short break to appreciate the beautiful people, and Kings Idri and Eliot are both very beautiful people. Image via giphy, copyright syfy\n\n\n\nWriting arrowplyr functions\nAt long last we have all the ingredients needed to write a function that can be used in an arrowplyr pipeline. Hereâ€™s a simple implementation of the base R toupper() function\n\n\narrowish_toupper <- function(x) {\n  Expression$create(\"utf8_upper\", x)\n}\n\n\n\nAs it happens arrowplyr pipelines already support the toupper() function, so there really wasnâ€™t a need for me to write this. However, at present they donâ€™t support the lubridate leap_year() function, which was the purpose of my very small contribution today. An Arrow friendly version of leap_year() looks like this:\n\n\narrowish_leap_year <- function(date) {\n   year <- Expression$create(\"year\", date)\n  (year %% 4 == 0) & ((year %% 100 != 0) | (year %% 400 == 0))\n}\n\n\n\nBefore putting our functions into action, letâ€™s see what happens when we try to write a simple data analysis pipeline without them:\n\n\narrowmagicks %>% \n  mutate(\n    title = toupper(title),\n    year = year(air_date), \n    leap = leap_year(air_date)\n  ) %>% \n  collect()\n\n\nWarning: Expression leap_year(air_date) not supported in Arrow;\npulling data into R\n# A tibble: 65 Ã— 8\n   season episode title          air_date   rating viewers  year leap \n    <int>   <int> <chr>          <date>      <dbl>   <dbl> <dbl> <lgl>\n 1      1       1 UNAUTHORIZED â€¦ 2015-12-16    0.2    0.92  2015 FALSE\n 2      1       2 THE SOURCE OFâ€¦ 2016-01-25    0.4    1.11  2016 TRUE \n 3      1       3 CONSEQUENCES â€¦ 2016-02-01    0.4    0.9   2016 TRUE \n 4      1       4 THE WORLD IN â€¦ 2016-02-08    0.3    0.75  2016 TRUE \n 5      1       5 MENDINGS, MAJâ€¦ 2016-02-15    0.3    0.75  2016 TRUE \n 6      1       6 IMPRACTICAL Aâ€¦ 2016-02-22    0.3    0.65  2016 TRUE \n 7      1       7 THE MAYAKOVSKâ€¦ 2016-02-29    0.3    0.7   2016 TRUE \n 8      1       8 THE STRANGLEDâ€¦ 2016-03-07    0.3    0.67  2016 TRUE \n 9      1       9 THE WRITING Râ€¦ 2016-03-14    0.3    0.71  2016 TRUE \n10      1      10 HOMECOMING     2016-03-21    0.3    0.78  2016 TRUE \n# â€¦ with 55 more rows\n\n\nThe internal arrow function that handles this is called â€œabandon_shipâ€. No, I donâ€™t know why I felt the need to mention this`\nYes, it returns the correct answer, but only because arrow detected a function it doesnâ€™t understand and has â€œabandoned shipâ€. It pulled the data into R and let dplyr do all the work. Now letâ€™s see what happens when we use our functions instead:\n\n\narrowmagicks %>% \n  mutate(\n    title = arrowish_toupper(title),\n    year = year(air_date),\n    leap = arrowish_leap_year(air_date)\n  ) %>% \n  collect()\n\n\n# A tibble: 65 Ã— 8\n   season episode title          air_date   rating viewers  year leap \n    <int>   <int> <chr>          <date>      <dbl>   <dbl> <int> <lgl>\n 1      1       1 UNAUTHORIZED â€¦ 2015-12-16    0.2    0.92  2015 FALSE\n 2      1       2 THE SOURCE OFâ€¦ 2016-01-25    0.4    1.11  2016 TRUE \n 3      1       3 CONSEQUENCES â€¦ 2016-02-01    0.4    0.9   2016 TRUE \n 4      1       4 THE WORLD IN â€¦ 2016-02-08    0.3    0.75  2016 TRUE \n 5      1       5 MENDINGS, MAJâ€¦ 2016-02-15    0.3    0.75  2016 TRUE \n 6      1       6 IMPRACTICAL Aâ€¦ 2016-02-22    0.3    0.65  2016 TRUE \n 7      1       7 THE MAYAKOVSKâ€¦ 2016-02-29    0.3    0.7   2016 TRUE \n 8      1       8 THE STRANGLEDâ€¦ 2016-03-07    0.3    0.67  2016 TRUE \n 9      1       9 THE WRITING Râ€¦ 2016-03-14    0.3    0.71  2016 TRUE \n10      1      10 HOMECOMING     2016-03-21    0.3    0.78  2016 TRUE \n# â€¦ with 55 more rows\n\nEverything works perfectly within Arrow. No ships are abandoned, the arrowplyr pipeline springs no leaks, and we all live happily ever after.\nSort of.\nI mean, weâ€™re all still alive.\nThat has to count as a win, right? ğŸ‰\n\n\n\nFigure 9: Eliot and Margo applaud your success. They are the best characters, and you are also the best because you have made it to the end of a long and strange blog post. Image via giphy, copyright syfy\n\n\n\n\nEpilogue: Whereâ€™s the rest of the owl?\n\n\nIn case you donâ€™t know the reference: how to draw an owl\nThe story Iâ€™ve told in this post is a little incomplete. Iâ€™ve shown you how to write a function like arrowish_leap_year() that can slot into a dplyr pipeline and operate on an Arrow data structure. But I havenâ€™t said anything about the precise workings of how register_binding() works, in part because the details of the metaprogramming magic is one of the mysteries Iâ€™m currently unpacking while I dig into the code base.\nBut thatâ€™s not the only thing Iâ€™ve left unsaid. I havenâ€™t talked about unit tests, for example. I havenâ€™t talked about the social/technical process of getting code merged into the Arrow repository. If youâ€™ve made it to the end of this post and are curious about joining the Arrow developer community, these are things you need to know about. Iâ€™ll probably write something about those topics later on, but in the meantime here are some fabulous resources that might be handy:\nApache Arrow New Contributors Guide (thank you to Alenka Frim!)\nDevelopers Guide to Writing Bindings (thank you to Nic Crane!)\nApache Arrow R Cookbook (thank you to Nic Crane again)\nEnjoy! ğŸ°\n\n\nLast updated\n2022-01-31 10:08:56 AEDT\nDetails\nsource code, R environment\n\n\n\n\n\n\nI didnâ€™t quite understand what â€œkernelsâ€ meant in this context until Nic Crane kindly explained it to me. The compute API contains a number of functions which are divided up into â€œkernelsâ€, specialised functions designed to work on a specific data type. The C++ Arrow compute documentation explains this better.â†©ï¸\nMy actual code didnâ€™t bother to name my function. Itâ€™s just an anonymous function passed to register_binding().â†©ï¸\n",
    "preview": "posts/2022-01-18_binding-arrow-to-r/img/magicians_hard_glossy_armour.jpg",
    "last_modified": "2022-02-05T07:54:51+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-10_setting-cran-repositories/",
    "title": "Setting CRAN repository options",
    "description": "A quick post on how to use RStudio public package manager instead of a standard CRAN mirror, and an example of why that can be useful sometimes.",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2022-01-10",
    "categories": [],
    "contents": "\n\nContents\nRStudio package manager\nMethod 1: RStudio settings\nMethod 2: Edit your R profile\nEpilogue\n\n\n\n\nThe shiny new work laptop arrives. Pristine and beautiful in its factory-fresh state. I am in love.\nI remain in love right up the moment I remember that with new work laptop comes the peculiar torture of setting up the machine. My gorgeous little Dell XPS 13 shipped with Windows 11, and while I do quite like Windows these days, Iâ€™ve become very accustomed to working in linux, so my first task was to install Ubuntu 20.04. These days thatâ€™s a pretty easy task, and the Ubuntu installer was even thoughtful enough to give me an option to enable full disk encryption. It all went smoothly. Yay me!\nEquipped with my fabulous new operating system, my next steps were to install R and RStudio, and for the first time in my life I was smart enough to remember to install the latest version of git along with the build-essential packages that Iâ€™m pretty much guaranteed to need the moment I need to build anything from source. Yay me again!\nThen comes the horror. Installing R packages. On linux. A small part of me dies.\nIâ€™m sure every linux-based R user shares my pain and needs no explanation, but some of the gentler souls who use Windows or Mac OS may not be aware of how tiresome package installation is on linux. The problem that linux users face is that CRAN does not maintain binaries for linux, so every time a linux user wants to install a package, it has to be built locally from the source code. This is both time consuming and frustrating, and very often you have to go hunting around to discover what other system dependencies need to be installed. So many tears have been shed over this.\nSo.\nMany.\nTears.\nRStudio package manager\nRecently, however, I have become aware that a better world is possible thanks to the magic of RStudio package manager. Iâ€™d sort of known that this existed as an option, but it wasnâ€™t until today that I realised that â€” in addition to the fancy commercial options â€” RStudio maintains a public package manager as a free service: the FAQ page is here. Anyone can configure R to install packages from the RStudio public package manager, if they want to.\n\n\n\n\n\n\nBut first a tiny bit of contextâ€¦ back in the distant part there was this strange, nightmarish time where I was teaching students R, but RStudio was not yet a thing. Many of the little niceties that RStudio users now take for granted didnâ€™t yet exist. In those dark years I had to spend a lot of time explaining to students that CRAN â€” the comprehensive R archive network â€” isnâ€™t actually a single website that contains lots of R packages. Itâ€™s more like a whole network of mirrors distributed all over the world, and youâ€™d have to manually choose which one you wanted to install packages from. It was mildly annoying. Itâ€™s considerably simpler now, because you can use the cloud.r-project.org service that automatically directs you to an appropriate server. In fact, if youâ€™re using RStudio youâ€™ve probably been using this service all along.\nRStudio package manager provides a modern alternative: it works like a CRAN mirror, but it has a lot of additional functionality. It has broader coverage, for instance: it includes R packages on Bioconductor as well as packages on CRAN. For my purposes, however, the attractive property is that it hosts binaries suitable for Ubuntu and other flavours of linux.\nâ€œBut how do I try it out, Danielle?â€ I hear you ask.\nIâ€™m so glad you asked, dear reader, because itâ€™s so much easier than it sounds.\nMethod 1: RStudio settings\nIf youâ€™re using RStudio, the easiest way to switch to RStudio PPM is to change your settings inside RStudio. Go to the RStudio Tools menu and select Global Options. When the popup window appears, click on Packages. Youâ€™ll see a screen that looks like this:\n\n\n\n\nIf it says â€œRStudio Global CDNâ€ next to â€œPrimary CRAN repoâ€, then youâ€™re using cloud.r-project.org as your CRAN repository. To switch to RStudio PPM, click on the â€œchangeâ€ button. It will bring up a list of CRAN mirrors, and if you want you can choose one of those. However the RStudio PPM isnâ€™t technically a CRAN mirror, so itâ€™s not listed there. If you want to switch to using the RStudio PPM, you have to enter the URL manually.\nSo what URL do you want? Well, it depends on whether you want to install packages from binaries or from source, and on what operating system youâ€™re using. Iâ€™m on Ubuntu 20.04, â€œFocal Fossaâ€, and the URL that serves binaries for my operating system is:\nhttps://packagemanager.rstudio.com/all/__linux__/focal/latest\nHereâ€™s me in the process of entering the URL:\n\n\n\n\nOkay, but what if youâ€™re not on Ubuntu 20.04? If youâ€™re on a different version of Ubuntu or some other operating system, you can find the link you need from the package manager setup page. The relevant part of the page should look something like this:\n\n\n\n\n\nTo get the URL youâ€™re looking for, click on the â€œchangeâ€ link to choose your operating system, or toggle between the binary and source options.\nMethod 2: Edit your R profile\nThere are a couple of limitations to this method. The most obvious one is that itâ€™s no help if you donâ€™t use RStudio, and even for RStudio users it can be awkward if you donâ€™t always use RStudio. If thatâ€™s your situation, you may want to manage your CRAN repository links by editing your R profile. To do this, open the .Rprofile file â€” using usethis::edit_r_profile(), for example â€” and add the following line:\n\n\noptions(repos = \"https://packagemanager.rstudio.com/all/__linux__/focal/latest\")\n\n\n\nYouâ€™ll need to restart your R session for this change to take effect.\nIf you want to be fancy, you can list multiple URLs. If the package you want to install is not found at the first link, R will try the second link, and so on. That can be useful. For instance, this is what I use in my R profile:\n\n\noptions(repos = c(\n  binary = \"https://packagemanager.rstudio.com/all/__linux__/focal/latest\",\n  source = \"https://packagemanager.rstudio.com/all/latest\",\n  CRAN = \"https://cloud.r-project.org\",\n  djnavarro = \"https://djnavarro.r-universe.dev\"\n))\n\n\n\nUsing this configuration, R will look for a suitable binary version of the package on RStudio PPM. If that fails it will try to install from RStudio PPM by building the package from the source code. If that fails, it checks CRAN in the usual way. Finally, if that fails, it looks to see if the package Iâ€™m requesting is one of the packages I listed at djnavarro.r-universe.dev, my very own tiny corner of the R-universe. Obviously, youâ€™re very unlikely to want to use my R-universe repository since it only consists of a handful of my own packages: but itâ€™s quite handy for me since they arenâ€™t all on CRAN!\nEpilogue\nIf youâ€™re a Windows or Mac user, you might not be aware of how much of a game changer this is for linux users. For example, in my previous blog post I wrote about my experiences getting started using Apache Arrow. Iâ€™m a big fan of Arrow â€” which should come as no surprise as Iâ€™ve recently started work at Voltron Data â€” but if youâ€™re installing the arrow R package on linux, itâ€™s extremely time consuming to build all the C++ libraries from source. It was a little cumbersome, but after switching to RStudio PPM, I can install arrow on my Ubuntu machine using the exact same command Iâ€™d use on Windowsâ€¦\n\n\ninstall.packages(\"arrow\")\n\n\n\nâ€¦and everything works. As easy on linux as it is on other operating systems! Yay! ğŸ‰\n\n\nLast updated\n2022-01-10 21:45:12 AEDT\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2022-01-10_setting-cran-repositories/img/rasa-kasparaviciene-0TFxOkhFt14-unsplash.jpg",
    "last_modified": "2022-02-05T07:54:51+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-19_starting-apache-arrow-in-r/",
    "title": "Getting started with Apache Arrow",
    "description": "I've been wanting to learn the basics of Apache Arrow a while: this is the story of how an R user learned to stop worrying and love a standardised in-memory columnar data format",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-11-19",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nWaitâ€¦ do I actually care?\nOkayâ€¦ so whatâ€™s the problem?\nFiiiiiine, Iâ€™ll keep readingâ€¦ tell me what Arrow is\nSigh. Apache Arrow please?\nI hate you\n\nOverview of Arrow\nInstalling Arrow\nDoes it work?\nExample 1: Arrow data sets arenâ€™t stored in R memory\nExample 2: Arrow plays nicely with dplyr\n\n\n\n\n\nIf youâ€™re like me and spend far too much time talking about R on Twitter you may have come across people talking about how to work with large data sets in R. Perhaps you hear people talking about Parquet files, Apache Arrow, and the arrow package for R, but youâ€™re not really sure what theyâ€™re about and are curious? If thatâ€™s you, thenâ€“\nSo weâ€™re just writing obvious â€œI want a job in tech, please hire me!â€ blog posts pitched at potential employers now?\nOh shush. Itâ€™s fun and useful too, you know.\nOkay fine, but could you at least be transparent about what youâ€™re doing? Because itâ€™s sort of obnoxious otherwise\nSheesh, what do you think this fake dialogue is for if not making the subtext blatant? Now could you please stop interrupting me and let me talk about Apache Arrow? It is in fact a more interesting subject than our pending unemployment.\nYeah, see how you feel about that in December babeâ€¦\nSigh.\n\n\n\nFigure 1: Arrow image by Tim Mossholder. It has nothing whatsoever to do with the Apache Software Foundation. Available by CC0 licence on unsplash.\n\n\n\nIntroduction\nOkay, where was I? Ah yesâ€¦\nIf youâ€™re like me and spend far too much time talking about R on Twitter you may have come across people talking about how to work with large data sets in R. Perhaps you hear people talking about Parquet files, Apache Arrow, and the arrow package for R, but youâ€™re not really sure what theyâ€™re about and are curious? If thatâ€™s you, then then this blog post is designed to help you get started.\nWaitâ€¦ do I actually care?\nLetâ€™s start at the beginning, with the most important question of all: do you actually need to care about this? This might be a long post (or possibly the first post in a long series), so letâ€™s make sure youâ€™re reading for the right reasons!\nFor a lot of people, the answer to the â€œdo I care?â€ question is going to be â€œprobably not â€“ or at least not right nowâ€. For example, if all your data sets are small and rectangular, then youâ€™re probably working with CSV files and not encountering a lot of problems. Your current workflow uses read.csv() or readr::read_csv() to import data, and everything is fine. Sure, the CSV format has some problems, but itâ€™s simple and it works. If that is you, then right now you donâ€™t need to worry about this.\nBut perhaps thatâ€™s not you, or maybe that wonâ€™t be you forever. You might be working with larger data sets, either now or in the future, and when that happens you might need to care.\nOkayâ€¦ so whatâ€™s the problem?\nThanks for a great question! Here are a few scenarios to think about.\nScenario 1: Letâ€™s suppose you have a big rectangular data set. An enormous table, basically, and currently itâ€™s stored as a file on your disk. The format of that file could be a plain CSV, a compressed CSV, or it could be something fancier like a Parquet file (Iâ€™ll come back to those in a later post, I suspect). It might be a couple of billion rows or so, the kind of thing that you can store on disk but is too big to fit into memory, so itâ€™s not going to be very easy to read this thing into R as a data frame! But your boss wants you to analyse it in R anyway. Thatâ€™s awkward. R likes to store things in memory. Eek.\nScenario 2: Okay, maybe your data isnâ€™t that big and it fits in memory, but itâ€™s still pretty big, and you need to do something complicated with it. Maybe your analysis needs to start in R but then continue in Python. Or something like that. In your head, youâ€™re thinking okay first I have to read the whole dataset into memory in R, and then it has to be transferred to Python which will have to read its own copy, andâ€¦ gosh that sounds slow and inefficient. Ugh.\nScenario 3: Honestly, youâ€™re just tired of having to deal with the fact that every language has its own idiosyncratic way of storing data sets in memory and itâ€™s exhausting to have to keep learning new things and you really wish there were some standardised way that programming languages represent data in memory and youâ€™d like a single toolkit that you can use regardless of what language youâ€™re in. Sighâ€¦\nIn any of these scenarios, Arrow might be useful to you.\nFiiiiiine, Iâ€™ll keep readingâ€¦ tell me what Arrow is\nYaaaaay! Green Arrow is a superhero in the DC Comics universe, whose real name is Oliver Queen. He was the subject of an unintentionally hilarious TV show, andâ€“\nSigh. Apache Arrow please?\nOh right. Apache Arrow is a standardised, language-independent format for storing table-like data in-memory, using a columnar format and supporting zero-copy reads without serialisation overhead.\nI hate you\nSorry. Letâ€™s unpack each of those terms:\nArrow is a standardised and language-independent format. Itâ€™s the same thing regardless of what programming language youâ€™re using: a data set accessed from R with Arrow has the same format as the a data set accessed in Python.\nArrow is used to store table-like data, very similar to a data frame or tibble.\nArrow refers to the in-memory format: itâ€™s not talking about how the data are stored on disk, and itâ€™s not talking about file formats. Itâ€™s all about how a loaded data set is represented in memory.1\nArrow uses columnar format. Unlike a CSV file, which stores the data row-wise, it represents the data column-wise: this turns out to be a much more efficient way to represent data when you need to subset the data (e.g., by using dplyr::filter() in R or the WHERE clause in SQL).\nArrow supports zero-copy reads without serialisation overhead, whichâ€¦ umâ€¦ yeah, what the heck does that mean?\nSo yeah. Serialisation is one of those terms that those fancy data people know all about, but a regular R user might not be quite as familiar with. Itâ€™s worth unpacking this a bit because itâ€™s helpful for understanding the problem that Arrow solvesâ€¦\nâ€¦Hey!\nWait a second, I already wrote a blog post about serialisation! I donâ€™t need to write another one.2 The TL;DR, for folks who quite reasonably donâ€™t want to do a deep dive into how R objects are written to RDS files, is that serialisation is the process of taking an in-memory data structure (like a data frame), and converting it into a sequence of bytes. Those bytes can either be written to disk (when youâ€™re saving a file) or they can be transmitted over some other channel. Regardless of what you want to do with the serialised data, this conversion takes time and resources, and at some point the data will need to be unserialised later. The resources expended in doing so are referred to as the â€œserialisation overheadâ€.\nFor small data sets, it doesnâ€™t take R very long to serialise or unserialise. The â€œserialisation overheadâ€ isnâ€™t a big deal. But when the data set is very large, this is not a trivial operation and you donâ€™t want to do this very often. Thatâ€™s a problem when a large data set needs to be passed around between multiple platforms. Loading the a CSV into R incurs a serialisation cost; transferring a copy of the data from R to Python incurs a serialisation cost. This happens because R and Python have different structured representations: a data frame in R is a different kind of thing to a panda in Python, so the data has to be serialised, transferred, and then unserialised at the other end in order to pass the data from one to another.\nWouldnâ€™t it be nice if we could avoid that? What if there was just one data structure representing the table in-memory, and R and Python could both agree to use it? That would remove the need to copy and transfer the data, right? And in doing so, it would eliminate those pesky serialisation costs incurred every time. It would be a â€œzero-copyâ€ mechanism.\nIf only there were a standardised, language-independent format for storing table-like data in-memory, using a columnar format and supporting zero-copy reads without serialisation overheadâ€¦\n\n\n\nFigure 2: Arrow image by Possessed Photography. It also has nothing whatsoever to do with the Apache Software Foundation. Available by CC0 licence on unsplash.\n\n\n\nOverview of Arrow\nHereâ€™s one of the two big ideas: standardisation prevents wasteful copying. The current situation that most of us are working in looks something like this. Every application and programming language defines its own format for storing data in memory (and often on disk too), and so any time multiple applications require access to the same data, thereâ€™s a serialisation cost. The bigger the data, the higher that cost will be. The more applications you connect to the same data, the more times you incur the cost:\n\n\n\nArrow solves this problem by allocating its own memory to store the data, and providing tools that allow you to access this from any language you like. The goal is to make those tools feel â€œnaturalâ€ in whatever language youâ€™re using. For example, if youâ€™re an R user, you may already be familiar with the dplyr grammar for data manipulation and youâ€™d like to be able to manipulate an Arrow Table using dplyr, in exactly the same way you would manipulate a data frame. The arrow R package allows you to do precisely this, and thereâ€™s a similar story that applies on the Python side. This allows you to write code that feels natural for the language youâ€™re working in.\nIn this approach, R and Python both have a toolkit that plays nicely with Arrow and feels native to that language. Applications written in R and applications written in Python can both work with the same underlying data (because itâ€™s in Arrow), so you donâ€™t have to serialise the data in order for them to talk to each other:\n\n\n\nSo thatâ€™s the first big idea.\nThe second big idea is that Arrow organises data column-wise in memory and as consequence it can support cool single instruction multiple data (or SIMD) operations that you can do with modern CPUs, which I totally understand 100% and am not just paraphrasing Wikipedia. Anyway, it doesnâ€™t really matter at the user level. All we care about there is that manipulating data with Arrow can be very fast. Thereâ€™s a very brief discussion of this on the Arrow overview page. (It also has prettier versions of my crappy handwritten diagrams)\n\n\n\nFigure 3: Arrow image by Denise Johnson. Yet again, it has nothing whatsoever to do with the Apache Software Foundation but it is very pretty. Available by CC0 licence on unsplash.\n\n\n\nInstalling Arrow\nInstalling Apache Arrow on your local machine as an R user is either extremely easy or mildly tiresome, depending almost entirely on whether youâ€™re on Linux. If youâ€™re using Windows or Mac OS, you shouldnâ€™t need to do anything except install the arrow package in the usual way. It just works:\n\n\ninstall.packages(\"arrow\")\n\n\n\nIf youâ€™re on Linux, there may not be any precompiled C++ binaries for your system, so youâ€™ll have to do it yourself. On my system this was quite time consuming, and the first couple of times I tried it I was convinced that nothing was actually happening because I wasnâ€™t seeing a progress bar or anything, and being impatient I killed the install process before it was finished. If youâ€™re like me and need visual confirmation that something is happening, thereâ€™s an ARROW_R_DEV environment variable you can set that will make the process more verbose:\n\n\nSys.setenv(ARROW_R_DEV = TRUE)\ninstall.packages(\"arrow\")\n\n\n\nThis way you get to see all the C++ build information scrolling by on the screen during the installation process. It doesnâ€™t make for very exciting viewing, but at least you have visual confirmation that everything is working!\nThere are quite a few ways you can customise the installation process, and theyâ€™re all documented on the installation page. One particularly useful thing to do is to set LIBARROW_MINIMAL to false, which ensures that arrow will install a bunch of optional features like compression libraries and AWS S3 support. It takes longer but you get more stuff! So the actual installation code I used was this:\n\n\nSys.setenv(\n  ARROW_R_DEV = TRUE,\n  LIBARROW_MINIMAL = FALSE\n)\ninstall.packages(\"arrow\")\n\n\n\nThis may take quite a long time if youâ€™re compiling from source so you may want to go make a cup of tea or something while it installs. At the end, hopefully, youâ€™ll have a working version of the package:\n\n\nlibrary(arrow)\n\n\n\nYou can use the arrow_info() function to obtain information about your installation:\n\n\narrow_info()\n\n\nArrow package version: 6.0.0.2\n\nCapabilities:\n               \ndataset    TRUE\nparquet    TRUE\njson       TRUE\ns3         TRUE\nutf8proc   TRUE\nre2        TRUE\nsnappy     TRUE\ngzip       TRUE\nbrotli     TRUE\nzstd       TRUE\nlz4        TRUE\nlz4_frame  TRUE\nlzo       FALSE\nbz2        TRUE\njemalloc   TRUE\nmimalloc   TRUE\n\nMemory:\n                  \nAllocator jemalloc\nCurrent    0 bytes\nMax        0 bytes\n\nRuntime:\n                        \nSIMD Level          avx2\nDetected SIMD Level avx2\n\nBuild:\n                          \nC++ Library Version  6.0.0\nC++ Compiler           GNU\nC++ Compiler Version 9.3.0\n\nYaaas queen! We are ready to go.\n\n\n\nFigure 4: Arrow image by Frank Busch. Now there are two! There are two arrows. Available by CC0 licence on unsplash.\n\n\n\nDoes it work?\nMy goal in this post is fairly modest. I wanted to understand why everyone I talk to seems so excited about Arrow, and try to get it configured to work on my machine. Assuming I can be bothered continuing this series, the next step would be to start playing with Arrow and do a proper exploration. For now though, Iâ€™ll try something simple, using the diamonds data from the ggplot2 package\n\n\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(ggplot2)\ndiamonds\n\n\n# A tibble: 53,940 Ã— 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# â€¦ with 53,930 more rows\n\nExample 1: Arrow data sets arenâ€™t stored in R memory\nOkay, so the first thing I want to investigate is this idea that Arrow holds the data in its own memory, not in the memory allocated to R. As things currently stand the diamonds tibble has 53940 rows stored in R memory, and that occupies about 3.3MB of memory:\n\n\nlobstr::obj_size(diamonds)\n\n\n3,456,344 B\n\nWhat happens when we move the data into Arrow? To do this we would construct a â€œTableâ€ object using the arrow_table() function, like this:\n\n\ndiamonds2 <- arrow_table(diamonds)\ndiamonds2\n\n\nTable\n53940 rows x 10 columns\n$carat <double>\n$cut <dictionary<values=string, indices=int8, ordered>>\n$color <dictionary<values=string, indices=int8, ordered>>\n$clarity <dictionary<values=string, indices=int8, ordered>>\n$depth <double>\n$table <double>\n$price <int32>\n$x <double>\n$y <double>\n$z <double>\n\nItâ€™s printed a little differently, but itâ€™s the same tabular data structure consisting of 53940 rows and 10 columns. So how much R memory does diamonds2 occupy?\n\n\nlobstr::obj_size(diamonds2)\n\n\n285,696 B\n\nOnly 279KB. The reason why it occupies so little memory is that diamonds2 doesnâ€™t contain all the data. The data are stored elsewhere, using memory allocated to Arrow. If a Python program wanted to access the diamonds2 data, it could do so without having to serialise the data again. It can link to the same data structure in Arrow memory that I just created. Neat!\nExample 2: Arrow plays nicely with dplyr\nOne neat thing about dplyr is that it cleanly separates the API from the backend. So you can use the dbplyr package to work with databases using dplyr code, or the dtplyr package to use a data.table backend, and so on. The arrow package does the same thing for Apache Arrow.\nHereâ€™s an example. If I were working with the original diamonds tibble, I might write a simple dplyr pipe to tabulate the clarity of premium-cut diamonds:\n\n\ndiamonds %>% \n  filter(cut == \"Premium\") %>% \n  count(clarity)\n\n\n# A tibble: 8 Ã— 2\n  clarity     n\n  <ord>   <int>\n1 I1        205\n2 SI2      2949\n3 SI1      3575\n4 VS2      3357\n5 VS1      1989\n6 VVS2      870\n7 VVS1      616\n8 IF        230\n\nCan I do the same thing using the diamonds2 Table? Letâ€™s try:\n\n\ndiamonds2 %>% \n  filter(cut == \"Premium\") %>% \n  count(clarity)\n\n\nInMemoryDataset (query)\nclarity: dictionary<values=string, indices=int8, ordered>\nn: int32\n\nSee $.data for the source Arrow object\n\nOkay, perhaps not what we were expecting. In order to optimise performance, the query doesnâ€™t get evaluated immediately (more on this in a later post perhaps) You have to tell it either to compute() the result, which will return another Table, or to collect() the result into a data frame\n\n\ndiamonds2 %>% \n  filter(cut == \"Premium\") %>% \n  count(clarity) %>% \n  collect()\n\n\n# A tibble: 8 Ã— 2\n  clarity     n\n  <ord>   <int>\n1 SI1      3575\n2 VS2      3357\n3 SI2      2949\n4 I1        205\n5 VS1      1989\n6 VVS1      616\n7 VVS2      870\n8 IF        230\n\nAt no point has the full data set been loaded into R memory. The diamonds2 object doesnâ€™t contain any new information. Itâ€™s still the same size:\n\n\nlobstr::obj_size(diamonds2)\n\n\n285,696 B\n\nMy example is trivial, of course, because the diamonds data set isnâ€™t very big. But if you start reading the Arrow documentation, they give an example using the NYC taxi data which is about 37GB in size. Thatâ€™sâ€¦ a teeensy bit bigger than Iâ€™d want to try loading into memory on my laptop, so I wouldnâ€™t be able to load it into R at all much less use dplyr. However, because Arrow supplies a dplyr back end, it is possible to write dplyr code for the NYC taxi data.\nOld and jaded though I may be, I have to admit thatâ€™s pretty cool.\n\n\n\nFigure 5: Okay yeah, this one actually does have something to do with the Apache Software Foundation. Itâ€™s, like, a registered trademark or something. Iâ€™m guessing this counts as fair use though.\n\n\n\n\n\n\n\n\n\n\n\n\nTechnically speaking, thereâ€™s a little ambiguity here. Usually when weâ€™re talking about Arrow weâ€™re talking about the in memory specification, but the term is also used to refer to the software implementing it, which includes a lot of compute functionality that goes beyond what the specification states. Similarly, the Arrow in-memory format doesnâ€™t have to imply any particular serialisation format, but in practice itâ€™s tightly connected to the IPC (â€œinterprocess communicationâ€) streaming and file format, and to the parquet file format. As a consequence, the term â€œArrowâ€ is sometimes used to refer to that broader suite of tools.â†©ï¸\nOkay, Iâ€™ll be honest, the RDS serialisation post came about because I was thinking about Arrow and serialisation costs, and got slightly distracted!â†©ï¸\n",
    "preview": "posts/2021-11-19_starting-apache-arrow-in-r/img/frank-busch-LpYcecGTifI-unsplash.jpg",
    "last_modified": "2021-11-29T09:14:59+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-15_serialisation-with-rds/",
    "title": "Data serialisation in R",
    "description": "A terrifying descent into madness, or, an explanation of how R serialises an in-memory data structure to summon a sequence of bytes that can be saved or transmitted. Eldritch horrors are unleashed by reading occult texts such as the R internals manual, SEXPTYPE codes are extracted from RDS with bitwise logic, and in the dark conclusion the R source code is consulted",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-11-15",
    "categories": [],
    "contents": "\n\nContents\nWhat is serialisation?\nHow does RDS serialisation work?\nThe serialize() function\nRDS uses gzip compression\nThe unserialize() function\n\nSerialising to plain text RDS\nInterpreting the RDS format\nThe RDS header\nLogical, integer, and numeric vectors\nCharacter vectors\nLists\nObject attributes\n\nType/flag packing\nDecoding the SEXPTYPE\nWhatâ€™s in the other bits?\nOkay but whatâ€™s up with 262153?\n\nAre we done yet?\n\n\n\n\n\nI still alive, and thatâ€™s what matters. The traumatic experience of the last week is fading, leaving a pale residue of fear and the few scraps of writing that are the sole surviving documentation of these days. It is a tale of fright, a desperate agony, and like any good tragedy it starts with the hope and naive optimism of youthâ€¦\n\n\nIâ€™ve decided the time has come for me to do a deep dive into data serialisation in R. Serialisation is one of those terms that comes up from time to time in data science, and itâ€™s popped up so many times on my twitter feed that I feel like I need a better grasp of how serialisation works in R. Itâ€™s a topic that folks who work with big data or have a computer science background likely understand quite well, but a lot of people who use R come from other backgrounds. If youâ€™re a social scientist who mostly works with small CSV files, for example, thereâ€™s no particular reason why youâ€™d have encountered this. In my case, Iâ€™ve worked as a mathematical psychologist and computational modeller for about 20 years, and until very recently Iâ€™ve never had never had to think about it in any detail. The issue only came up for me when I started reading about Apache Arrow (a topic for another post, perhaps) and realised that I needed to have a better understanding of what all this data serialisation business is about, and how R handles it.\nThis post is aimed at anyone who is in a similar situation to me!\n\nOh you sweet summer child. You really think you are prepared for the dark? Thatâ€™s adorable.\n\n\n\n\nFigure 1: Image by Andrey Zvyagintsev. Available by CC0 licence on unsplash.\n\n\n\nWhat is serialisation?\nIn general serialisation refers to any process that takes an object stored in memory and converts into a stream of bytes that can be written to a file or transmitted elsewhere. Any time we write data to a file, we are â€œserialisingâ€ it according to some encoding scheme. Suppose, for instance, I have a data frame called art:\n\n\n\n\n\nart\n\n\n   resolution      series sys_id img_id   short_name format\n1        1000 watercolour  sys02  img34 teacup-ocean    jpg\n2        1000 watercolour  sys02  img34 teacup-ocean    png\n3        2000 watercolour  sys02  img34 teacup-ocean    jpg\n4        2000 watercolour  sys02  img34 teacup-ocean    png\n5        4000 watercolour  sys02  img34 teacup-ocean    jpg\n6        4000 watercolour  sys02  img34 teacup-ocean    png\n7         500 watercolour  sys02  img34 teacup-ocean    jpg\n8         500 watercolour  sys02  img34 teacup-ocean    png\n9        8000 watercolour  sys02  img34 teacup-ocean    jpg\n10       8000 watercolour  sys02  img34 teacup-ocean    png\n\nThis data frame is currently stored in memory on my machine, and it has structure. R represents this data frame as a list of length 6. Each element of this list is a pointer to another data structure, namely an atomic vector (e.g., numeric vector). The list is accompanied by additional metadata that tells R that this particular list is a data frame. The details of how this is accomplished donâ€™t matter for this post. All that matters for now is that the in-memory representation of art is a structured object. Itâ€™s little more complicated than a stream of data, but if I want to save this data to a file it needs to be converted into one. The process of taking an in-memory structure and converting it to a sequence of bytes is called serialisation.\nSerialisation doesnâ€™t have to be fancy. The humble CSV file can be viewed as a form of serialisation for a data frame, albeit one that does not store all the metadata associated with the data frame. Viewed this way, write.csv() can be viewed as a serialisation function for tabular data:\n\n\nwrite.csv(art, file = \"art.csv\", row.names = FALSE)\n\n\n\nWhen I call this function R uses the art object to write text onto the disk, saved as the file â€œart.csvâ€. If I were to open this file in a text editor, Iâ€™d see this:\n\n\"resolution\",\"series\",\"sys_id\",\"img_id\",\"short_name\",\"format\"\n1000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n1000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n2000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n2000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n4000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n4000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n500,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n500,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n8000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n8000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n\nAlthough this view is human-readable, it is slightly misleading. The text in shown above isnâ€™t the literal sequence of bytes. Itâ€™s how those bytes are displayed when the have been unserialised and displayed on screen as UTF-8 plain text. To get a sense of what serialised text actually looks like we can use the charToRaw() function. The first few characters of the text file are \"resolu\" which looks like this when series of bytes:\n\n\ncharToRaw('\"resolu')\n\n\n[1] 22 72 65 73 6f 6c 75\n\nThe raw vector shown in the output above uses one byte to represent each character. For instance, the character \"l\" is represented with the byte 6c in the usual hexadecimal representation. We can unpack that byte into its consituent 8-bit representation using rawToBits()\n\n\n\"u\" |>\n  charToRaw() |>\n  rawToBits()\n\n\n[1] 01 00 01 00 01 01 01 00\n\n(Note that the base pipe |> is rendered as a triangle-shaped ligature in Fira Code)\nReturning to the â€œart.csvâ€ data file, I can use file() and readBin() to define a simple helper function that opens a binary connection to the file, reads in the first 100 bytes (or whatever), closes the file, and then returns those bytes as a raw vector:\n\n\nread_bytes <- function(path, max_bytes = 100) {\n  con <- file(path, open = \"rb\")\n  bytes <- readBin(con, what = raw(), n = max_bytes)\n  close(con)\n  return(bytes)\n}\n\n\n\nHere are the first 100 bytes of the â€œart.csvâ€ file:\n\n\nread_bytes(\"art.csv\")\n\n\n  [1] 22 72 65 73 6f 6c 75 74 69 6f 6e 22 2c 22 73 65 72 69 65 73 22\n [22] 2c 22 73 79 73 5f 69 64 22 2c 22 69 6d 67 5f 69 64 22 2c 22 73\n [43] 68 6f 72 74 5f 6e 61 6d 65 22 2c 22 66 6f 72 6d 61 74 22 0a 31\n [64] 30 30 30 2c 22 77 61 74 65 72 63 6f 6c 6f 75 72 22 2c 22 73 79\n [85] 73 30 32 22 2c 22 69 6d 67 33 34 22 2c 22 74 65\n\nThe read.csv() function is similar to read_bytes() in spirit: when I call read.csv(\"art.csv\"), R opens a connection to the â€œart.csvâ€ file. It then reads that sequence of bytes into memory, and then closes the file. However, unlike my simple read_bytes() function, it does something useful with that information. The sequence of bytes gets decoded (unserialised), and the result is that R reconstructs the original art data frame:\n\n\nart <- read.csv(\"art.csv\")\nart\n\n\n   resolution      series sys_id img_id   short_name format\n1        1000 watercolour  sys02  img34 teacup-ocean    jpg\n2        1000 watercolour  sys02  img34 teacup-ocean    png\n3        2000 watercolour  sys02  img34 teacup-ocean    jpg\n4        2000 watercolour  sys02  img34 teacup-ocean    png\n5        4000 watercolour  sys02  img34 teacup-ocean    jpg\n6        4000 watercolour  sys02  img34 teacup-ocean    png\n7         500 watercolour  sys02  img34 teacup-ocean    jpg\n8         500 watercolour  sys02  img34 teacup-ocean    png\n9        8000 watercolour  sys02  img34 teacup-ocean    jpg\n10       8000 watercolour  sys02  img34 teacup-ocean    png\n\nThrilling stuff.\n\nDo you feel that slow dread yet, my dear? Do you feel yourself slipping? You are on the edge of the cliff. You can still climb back to safety if you want. You donâ€™t have to fall. The choice is still yours.\n\n\n\n\nFigure 2: Image by Daniel Jensen. Available by CC0 licence on unsplash.\n\n\n\nHow does RDS serialisation work?\nData can be serialised in different ways. The CSV format works reasonably well for rectangular data structures like data frames, but doesnâ€™t work well if you need to serialise something complicated like a nested list. The JSON format is a better choice for those cases, but it too has some limitations when it comes to storing R objects. To serialise an R object we need to store the metadata (classes, names, and other attributes) associated with the object, and if the object is a function there is a lot of other information relevant to its execution besides the source code (e.g., enclosing environment). Because R needs this information, it relies on the native RDS format to do the work. As it happens I have an â€œart.rdsâ€ file on disk that stores the same data frame in the RDS format. When I use readRDS() to unserialise the file, it recreates the same data frame:\n\n\nreadRDS(\"art.rds\")\n\n\n   resolution      series sys_id img_id   short_name format\n1        1000 watercolour  sys02  img34 teacup-ocean    jpg\n2        1000 watercolour  sys02  img34 teacup-ocean    png\n3        2000 watercolour  sys02  img34 teacup-ocean    jpg\n4        2000 watercolour  sys02  img34 teacup-ocean    png\n5        4000 watercolour  sys02  img34 teacup-ocean    jpg\n6        4000 watercolour  sys02  img34 teacup-ocean    png\n7         500 watercolour  sys02  img34 teacup-ocean    jpg\n8         500 watercolour  sys02  img34 teacup-ocean    png\n9        8000 watercolour  sys02  img34 teacup-ocean    jpg\n10       8000 watercolour  sys02  img34 teacup-ocean    png\n\nHowever, when I read this file using read_bytes() itâ€™s also clear that â€œart.rdsâ€ contains a very different sequence of bytes to â€œart.csvâ€:\n\n\nread_bytes(\"art.rds\")\n\n\n  [1] 1f 8b 08 00 00 00 00 00 00 03 8b e0 62 60 60 60 66 60 61 64 62\n [22] 60 66 05 32 19 58 43 43 dc 74 2d 80 62 c2 40 0e 1b 10 f3 02 31\n [43] 50 11 f3 0b 08 66 bf 00 c1 fc 0b 20 98 f1 0b 04 cb 3b 40 30 83\n [64] 00 58 3d 0b 03 27 90 e6 2e 4f 2c 49 2d 4a ce cf c9 2f 2d 1a 4a\n [85] 42 a8 be 60 2d ae 2c 36 30 1a 18 0e 9a 4b 32 73\n\nThis is hardly surprising since RDS and CSV are different file formats. But while I have a pretty good mental model of what the contents of a CSV file look like, I donâ€™t have a very solid grasp of what the format of an RDS file is. Iâ€™m curious.\n\nOh sweetie, I tried to warn youâ€¦\n\nThe serialize() function\nTo get a sense of how the RDS format works, itâ€™s helpful to note that R has a serialize() function and an unserialize() function that provide low-level access to the same mechanisms that underpin saveRDS() and readRDS().\n\n\nbytes <- serialize(art, connection = NULL)\n\n\n\nAs you can see, this is the same sequence of bytes returned by read_bytes()â€¦\n\n\nbytes[1:100]\n\n\n  [1] 58 0a 00 00 00 03 00 04 01 02 00 03 05 00 00 00 00 05 55 54 46\n [22] 2d 38 00 00 03 13 00 00 00 06 00 00 00 0d 00 00 00 0a 00 00 03\n [43] e8 00 00 03 e8 00 00 07 d0 00 00 07 d0 00 00 0f a0 00 00 0f a0\n [64] 00 00 01 f4 00 00 01 f4 00 00 1f 40 00 00 1f 40 00 00 00 10 00\n [85] 00 00 0a 00 04 00 09 00 00 00 0b 77 61 74 65 72\n\nâ€¦oh wait, no itâ€™s not. What gives???? The â€œart.rdsâ€ file begins with 1f 8b 08 00, whereas serialize() returns a sequence of bytes that begins with 58 0a 00 00. These are not the same at all! Why is this happening???\nRDS uses gzip compression\nAfter digging a little into the help documentation, I realised that this happens because the default behaviour of saveRDS() is to write a compressed RDS file using gzip compression. In contrast, serialize() does not employ any form of compression. The art.rds file that I have stored on disk is that gzipped version, but itâ€™s easy enough to save an uncompressed RDS file, simply by setting compress = FALSE:\n\n\nsaveRDS(art, file = \"art_nozip.rds\", compress = FALSE)\n\n\n\nSo now when I inspect the uncompressed file using read_bytes(), the output is the same one I obtained when I called serialize(art) earlier:\n\n\nread_bytes(\"art_nozip.rds\")\n\n\n  [1] 58 0a 00 00 00 03 00 04 01 02 00 03 05 00 00 00 00 05 55 54 46\n [22] 2d 38 00 00 03 13 00 00 00 06 00 00 00 0d 00 00 00 0a 00 00 03\n [43] e8 00 00 03 e8 00 00 07 d0 00 00 07 d0 00 00 0f a0 00 00 0f a0\n [64] 00 00 01 f4 00 00 01 f4 00 00 1f 40 00 00 1f 40 00 00 00 10 00\n [85] 00 00 0a 00 04 00 09 00 00 00 0b 77 61 74 65 72\n\nThatâ€™s a relief. I was getting very anxious there, but I feel a little better now. My sanity is restored.\n\nâ€¦for now.\n\nThe unserialize() function\nThat was frustrating. Anyway getting back to the main thread, the inverse of the serialize() function is unserialize(). Itâ€™s very similar to the readRDS() function that youâ€™d normally use to read an RDS file, but you can apply it to a raw vector like bytes. Once again we reconstruct the original data frame:\n\n\nunserialize(bytes)\n\n\n   resolution      series sys_id img_id   short_name format\n1        1000 watercolour  sys02  img34 teacup-ocean    jpg\n2        1000 watercolour  sys02  img34 teacup-ocean    png\n3        2000 watercolour  sys02  img34 teacup-ocean    jpg\n4        2000 watercolour  sys02  img34 teacup-ocean    png\n5        4000 watercolour  sys02  img34 teacup-ocean    jpg\n6        4000 watercolour  sys02  img34 teacup-ocean    png\n7         500 watercolour  sys02  img34 teacup-ocean    jpg\n8         500 watercolour  sys02  img34 teacup-ocean    png\n9        8000 watercolour  sys02  img34 teacup-ocean    jpg\n10       8000 watercolour  sys02  img34 teacup-ocean    png\n\nYay.\n\nYou can sense it canâ€™t you? It will only get worse for you, my sweet. Look upon the grim visage of those that have passed this way before. Their lifeless bones are a warning.\n\n\n\n\nFigure 3: Image by Chelms Varthoumlien. Available by CC0 licence on unsplash.\n\n\n\nSerialising to plain text RDS\nOkay, so what Iâ€™ve learned so far is that in most cases, an RDS file is just a gzipped version of â€¦ something. Itâ€™s the gzipped version of whatever the hell it is that serialize() creates. What I donâ€™t yet know is how the serialize() function operates. What secret magic does it use? How does it construct this sequence of bytes? What do the contents of this file actually include?\nIâ€™ll start simple. Trying to understand how a complicated object is serialised might be painful, so Iâ€™ll set the art data frame to one side. Instead, Iâ€™ll serialise a numeric vector containing three elements, and â€¦ I guess Iâ€™ll set ascii = TRUE so that R uses UTF-8 to serialise the object to plain text format rather than â€¦ writing a binary file?\n\nClever girl. Yes, the default behaviour is binary serialization. Unless otherwise specified using the xdr argument, serialize() enforces a big-endian representation on the binary encoding. But you didnâ€™t want to go there did you? It frightened you, didnâ€™t it? The abyss stares back at you, sweetness, and you are beginning to attract its attention\n\n\n\nbytes <- serialize(\n  object = c(10.1, 2.2, 94.3), \n  connection = NULL,\n  ascii = TRUE\n)\n\n\n\nWhen I print out the bytes vector I still donâ€™t get text though?\n\n\nbytes\n\n\n [1] 41 0a 33 0a 32 36 32 34 30 32 0a 31 39 37 38 38 38 0a 35 0a 55 54\n[23] 46 2d 38 0a 31 34 0a 33 0a 31 30 2e 31 0a 32 2e 32 0a 39 34 2e 33\n[45] 0a\n\nI was expecting text. Where is my text??? I dig a little deeper and realise my mistake. What Iâ€™m looking at here is the sequence of bytes that correspond to the UTF-8 encoded text. If I want to see that text using actual letters, I need to use rawToChar(). When I do that I see something that looks vaguely like data:\n\n\nrawToChar(bytes)\n\n\n[1] \"A\\n3\\n262402\\n197888\\n5\\nUTF-8\\n14\\n3\\n10.1\\n2.2\\n94.3\\n\"\n\nIt is a little easier to read if I use cat() to print the output:\n\n\nbytes |>\n  rawToChar() |>\n  cat()\n\n\nA\n3\n262402\n197888\n5\nUTF-8\n14\n3\n10.1\n2.2\n94.3\n\nItâ€™sâ€¦ not immediately obvious how this output should be interpreted? I donâ€™t know what all these lines mean, but I recognise the last three lines: those are the three values stored in the vector I serialised. Now I just need to work out what the rest of it is all about.\nBut before I do, Iâ€™ll check that this is exactly the same text that I see if I create an RDS file using the following command and then open that file in a text editor:\n\n\nsaveRDS(\n  object = c(10.1, 2.2, 94.3), \n  file = \"numbers.rds\", \n  ascii = TRUE, \n  compress = FALSE\n)\n\n\n\nOkay, it checks out. My excitement can barely be contained.\n\nWilting already, arenâ€™t you? Poor little flower, youâ€™ve been cut from the stem. Youâ€™re dead already but you donâ€™t even know it. All that is left is to wither away under the blistering glare of knowledge.\n\n\n\n\nFigure 4: Image by Daria Shevtsova. Available by CC0 licence on unsplash.\n\n\n\nInterpreting the RDS format\nAll right, lets see if I can interpret the contents of an RDS file. Rather than tediously writing the file to disk using saveRDS() and then loading it again, Iâ€™ll cheat slightly and write a show_rds() function that serialises an object and prints the results directly to the R console:\n\n\nshow_rds <- function(object, header = TRUE) {\n  rds <- object |>\n    serialize(connection = NULL, ascii = TRUE) |>\n    rawToChar() |>\n    strsplit(split = \"\\n\") |>\n    unlist()\n  if(header == FALSE) rds <- rds[-(1:6)]\n  cat(rds, sep = \"\\n\")\n}\n\n\n\nJust to make sure itâ€™s doing what itâ€™s supposed to Iâ€™ll make sure it gives the output Iâ€™m expecting. Probably a good idea given how many times Iâ€™ve been surprised so farâ€¦\n\n\nshow_rds(object = c(10.1, 2.2, 94.3))\n\n\nA\n3\n262402\n197888\n5\nUTF-8\n14\n3\n10.1\n2.2\n94.3\n\nOkay, phew. That looks good.\nI guess my next task is to work out what all this output means. The last three lines are obvious: thatâ€™s the data! What about the line above the data? That line reads 3 and is followed by three data values. I wonder if thatâ€™s a coincidence? Iâ€™ll see what happens if I try to serialise just 2 numbers. Does that line change to 2?\n\n\nshow_rds(object = c(10.1, 2.2))\n\n\nA\n3\n262402\n197888\n5\nUTF-8\n14\n2\n10.1\n2.2\n\nYes. Yes it does. I am learning things.\nHereâ€™s what I know so far:\nA\n3\n262402\n197888\n5\nUTF-8\n14\n3      # the object has length 3\n10.1   # first value is 10.1\n2.2    # second value is 2.2\n94.3   # third value is 94.3\nOkay, so whatâ€™s next? The 14 in the preceding line. What does that mean?\nI puzzled over this for a while, and ended up needing to consult an occult tome of dangerous lore â€“ the R Internals Manual â€“ to find a partial answer. On the very first page of the Infernals Manual there is a table listing the SEXPTYPE codes that R uses internally to specify what kind of entity is encoded by an R object. Here are a few of these SEXPTYPE codes:\nValue\nSEXPTYPE\nVariable type\n10\nLGLSXP\nlogical\n13\nINTSXP\ninteger\n14\nREALSXP\nnumeric\n16\nSTRSXP\ncharacter\n19\nVECSXP\nlist\nSoâ€¦ when I serialise a plain numeric vector, the RDS file writes the number 14 to the file. In that case I will tentatively update my beliefs about the RDS file\nA\n3\n262402\n197888\n5\nUTF-8\n14     # the object is numeric\n3      # the object has length 3\n10.1   # first value is 10.1\n2.2    # second value is 2.2\n94.3   # third value is 94.3\n\nOh no dear. You have strayed so far from the light already. That 14 carries much more meaning than your fragile mind is prepared to handle. Soon you will know better. Soon you will unravel entirely. You can feel it coming, canâ€™t you?\n\n\n\n\nFigure 5: Image by Roxy Aln Available by CC0 licence on unsplash.\n\n\n\nThe RDS header\nAt this point, I have annotated every part of the RDS file that corresponds to the actual object. Consulting the section of the Infernal Manual devoted to serialisation, I learn that the six lines at the beginning of the file are known as the RDS header. Reading further I learn that the first line specifies the encoding scheme (A for ASCII, X for binary big-endian). The second line specifies which version of the RDS file format is used. The third line indicates the version of R that wrote the file. Finally, the fourth line is the minimum version of R required to read the file.\nIf I annotate my RDS header to reflect this knowledge, I get this:\nA       # use ASCII encoding\n3       # use version 3 of the RDS format\n262402  # written with R version 4.1.2\n197888  # minimum R version that can read it is 3.5\n5\nUTF-8 \nI am confused. Where did those numbers come from? Why does version 4.1.2 correspond to the number 262402, and why does 3.5 get encoded as 197888? The Manual is silent, and my thoughts become bleak. Am I losing my mind? Is the answer obvious??? What mess have I gotten myself into?\nIn desperation, I look at the R source code which reveals unto me the magic formula:\n\n\nencode_r_version <- function(major, minor, patch) {\n  (major * 65536) + (minor * 256) + patch\n}\n\n\n\nYessss. This all makes sense nowâ€¦\n\n\nencode_r_version(4, 1, 2)\nencode_r_version(3, 5, 0)\n\n\n[1] 262402\n[1] 197888\n\nâ€¦so much sense.\nWhat about the other two lines in the header? Prior to RDS version 3 â€“ which was released in R version 3.5 â€“ those two lines didnâ€™t exist in the header. Those are now used to specify the â€œnative encodingâ€ of the file, according to the Manual.\nâ€œBut isnâ€™t that ASCII????â€, whispers a voice in my head. â€œIs that not what the A is for?â€\nNot quite. The RDS file format isnâ€™t restricted to ASCII characters. In the usual case, the RDS file can encode any UTF-8 character and the native encoding line reads UTF-8. There is another possibility though: the file may use the Latin-1 alphabet. Because of this, there is some ambiguity that needs to be resolved. The RDS file needs to indicate which character set is used for the encoding.\nMy annotated header now looks like this:\nA      # the file uses ASCII encoding\n3      # the file uses version 3 of the RDS format\n262402 # the file was written in R version 4.1.2\n197888 # the minimum R version that can read it is 3.5\n5\nUTF-8  # the file encodes UTF-8 characters not Latin-1\nOkay, that makes a certain kind of sense, but whatâ€™s the story behind that 5? What does that mean? What dark secret does it hide?\nIt took me so very long to figure this one out. As far as I can tell this line isnâ€™t discussed in the R Internals Manual, but I worked it out by looking at the source code for serialize. That line reads 5 because itâ€™s telling the parser that the string that follows on the next line (i.e., UTF-8) contains five characters. Presumably if Iâ€™d used Latin-1 encoding, the corresponding line would have been 7.\nThis is doing my head in, but I think Iâ€™m okay?\n\nAre you sure? Really? You donâ€™t sound too certain\n\n\n\n\nFigure 6: Image by Liza Polyanskaya. Available by CC0 licence on unsplash.\n\n\n\nLogical, integer, and numeric vectors\nNow that I have a sense of how the RDS header works, Iâ€™ll set header = FALSE whenever I call show_rds() from now on. That way I wonâ€™t have to look at that same six lines of output over and over and they will no longer haunt my dreams.\n\nOh no my dear. Hiding wonâ€™t save you.\n\nI think the time has come to look at how RDS encodes other kinds of data. For three of the four commonly used atomic vector types (logical, integer, and numeric), the RDS format looks exactly as I expected given what I learned earlier. As shown in the table above, the SEXPTYPE code for a logical vector is 10, so a logical vector with four elements looks like this:\n\n\nshow_rds(\n  object = c(TRUE, TRUE, FALSE, NA), \n  header = FALSE\n)\n\n\n10\n4\n1\n1\n0\nNA\n\nTRUE values are represented by 1 in the RDS file, and FALSE values are represented by 0. Missing values are represented as NA.\nFor an integer vector, the output is again familiar. The SEXPTYPE here is 13, so a vector of four integer looks like this:\n\n\nshow_rds(\n  object = c(-10L, 20L, 30L, NA),\n  header = FALSE\n)\n\n\n13\n4\n-10\n20\n30\nNA\n\nNumeric vectors Iâ€™ve already seen. They have SEXPTYPE of 14, so a numeric vector of length 3 starts with 14 on the first line, 3 on the second line, and then the numbers themselves appear over the remaining three lines. However, there is a catch. There always is when dealing with real numbers. Numeric values are subject to the vagaries of floating point arithmetic when represented in memory, and the encoding is not exact. As a consequence, it is entirely possible that something like this happens:\n\n\nshow_rds(\n  object = c(10.3, 99.9, 100),\n  header = FALSE\n)\n\n\n14\n3\n10.3\n99.90000000000001\n100\n\nFloating point numbers always make my head hurt. It is best not to dwell too long upon them lest my grip on sanity loosen.\n\nToo late. Far, far too late.\n\n\n\n\nFigure 7: Image by Hoshino Ai. Available by CC0 licence on unsplash.\n\n\n\nCharacter vectors\nWhat about character vectors?\n\nAdorable that you think these will be safer waters in which to swim my dear. A wiser woman would turn back now and return to the shallows. Yet there you go, drifting out to sea. Fool.\n\n\nLetâ€™s create a simple character vector. According to the table above, character vectors have SEXPTYPE 16, so Iâ€™d expect that a character vector with three elements would start with 16 on the first line and 3 on the second line, which would then be followed by the contents of each cell.\nAnd thatâ€™sâ€¦ sort of true?\n\n\nshow_rds(\n  object = c(\"text\", \"is\", \"strange\"),\n  header = FALSE\n)\n\n\n16\n3\n262153\n4\ntext\n262153\n2\nis\n262153\n7\nstrange\n\nThe format of this output is roughly what I was expecting, except for the fact that each string occupies three lines. For instance, these three lines correspond to the word \"strange\":\n262153\n7\nstrange\nThis puzzled me at first. Eventually, I remembered that the source code for R is written in C, and C represents strings as an array. So where R treats the word \"strange\" a single object with length 1, C treats it as a string array containing 7 characters. In the R source code, the object encoding a string is called a CHARSXP. So lines two and three begin to make sense:\n262153\n7        # the string has \"length\" 7\nstrange  # the 7 characters in the string\nWhat about the first line? Given everything Iâ€™ve seen previously itâ€™s pretty tempting to guess that it means something similar to the SEXPTYPE codes that weâ€™ve seen earlier. Perhaps in the same way that numeric is SEXPTYPE 14 and logical is SEXPTYPE 10, maybe thereâ€™s some sense in which a single string has a â€œSEXPTYPEâ€ of 262153? That canâ€™t be right though. According to the R Internals Manual, a CHARSXP object has a SEXPTYPE code of 9, not 262153. I must be misunderstanding something? Why is it 262153?\n\nFrightened by the first wave, are you? All in good time my love. The secrets of 262153 will reveal themselves soon.\n\n\n\n\nFigure 8: Image by Tim Marshall Available by CC0 licence on unsplash.\n\n\n\nLists\nWhat about lists? Lists are more complicated than atomic vectors, because theyâ€™re just containers for other data structures that can have different lengths and types. As mentioned earlier, they have SEXPTYPE 19, so a list with three elements will of course start with 19 on the first line and 3 on the second line. Hereâ€™s an example:\n\n\nshow_rds(\n  object = list(\n    c(TRUE, FALSE), \n    10.2, \n    c(\"strange\", \"thing\")\n  ),\n  header = FALSE\n)\n\n\n19\n3\n10\n2\n1\n0\n14\n1\n10.2\n16\n2\n262153\n7\nstrange\n262153\n5\nthing\n\nThis output makes my brain hurt, but it does make sense if I stare at it long enough. It begins with the two lines specifying that itâ€™s a list of length three. This is then followed by the RDS representation for the logical vector c(TRUE, FALSE), the RDS representation for the numeric vector 10.2, and finally the RDS representation for the character vector c(\"strange\", \"thing\").\nI have started using annotations and whitespace to make it clearer:\n19 # it's a list\n3  # of length 3\n\n  10  # list entry 1 is logical\n   2  # of length 2\n   \n    1       # value is TRUE\n    0       # value is FALSE\n      \n  14  # list entry 2 is numeric \n   1  # of length 1\n   \n    10.2    # value is 10.2\n    \n  16  # list entry 3 is character\n   2  # of length 2\n   \n    262153  # every string starts with this\n         7  # this string has 7 characters\n   strange  # values are: s, t, r, a, n, g, e\n   \n    262153  # every string starts with this\n         5  # this string has 5 characters\n     thing  # values are: t, h, i, n, g\nI feel so powerful! My mind is now afire with knowledge! All the secrets of RDS will be mineâ€¦\n\nâ€¦and the madness strikes at last. Pride comes before the fall, always.\n\n\n\n\nFigure 9: Image by Moreno MatkoviÄ‡. Available by CC0 licence on unsplash.\n\n\n\nObject attributes\nOne of the key features of R is that vectors are permitted to have arbitrary metadata: names, classes, attributes. If an R object contains metadata, that metadata must be serialised too. That has some slightly surprising effects. Letâ€™s start with this very simple numeric object with two elements:\n\n\nshow_rds(object = c(100, 200), header = FALSE)\n\n\n14\n2\n100\n200\n\nAs expected it has SEXPTYPE 14 (numeric), length 2, and the values it stores are 100 and 200. Nothing out of the ordinary here. But when I add a name to the object, the output is â€¦ complicated.\n\n\nshow_rds(object = c(a = 100, b = 200), header = FALSE)\n\n\n526\n2\n100\n200\n1026\n1\n262153\n5\nnames\n16\n2\n262153\n1\na\n262153\n1\nb\n254\n\nI â€¦ donâ€™t know what I am looking at here. First off, I seem to be having the same problem I had with character strings. If I take the first line of this output at face value I would think that a named numeric vector has SEXPTYPE 526. That canâ€™t be right, can it?\n\nIt isnâ€™t. In the same way that strings donâ€™t have a SEXPTYPE of 262153 (the actual number is 9), the 526 here is a little misleading. This is a numeric vector and like all numeric vectors it is SEXPTYPE 14. You will learn the error of your ways very soon.\n\n\nSetting that mystery aside, I notice that the RDS output is similar to the output we saw when converting a list to RDS. The output contains the numeric vector first (the data), which is then followed by a list that specifies the attributes linked to that object?\n\nNot quite. Youâ€™re so close, but itâ€™s a pairlist, not a list. The underlying data structure is different. Donâ€™t let it worry your mind, sweet thing. Preserve your mind for the trials still to come.\n\n\nFor this object, thereâ€™s only one attribute that needs to be stored, corresponding to the names associated with each element of the vector. If I annotate the output again, I get this:\n526     # Numeric vector \n2       # with two values\n\n   100     # value 1 \n   200     # value 2\n   \n1026    # Pairlist for attributes\n1       # with one pair of entries\n\n   262153  # The attribute is called \"names\"\n   5       # \n   names   # \n   \n   16      # The attribute has two values\n   2       # \n   \n      262153   # First value is \"a\"\n           1   #\n           a   # \n\n      262153   # Second value is \"b\"\n           1   #\n           b   #\n\n254   # end of pairlist\nThe 254 marking the end of the pairlist confused me for a little while, but it isnâ€™t arbitrary. It represents a NULL value in the RDS format:\n\n\nshow_rds(NULL, header=FALSE)\n\n\n254\n\n\nYes, my dear. If you look at the relevant part of the R source code, you see that there are a collection of â€œadministrative codesâ€ that are used to denote special values in a SEXPTYPE-like fashion. NULL is the one youâ€™d be most likely to encounter though. Perhaps best not to travel down that road tonight though? Wait until day. Youâ€™re getting tired.\n\n\n\n\nFigure 10: Image by Kelly Sikkema. Available by CC0 licence on unsplash.\n\n\n\nType/flag packing\nThroughout this post, Iâ€™ve given the impression that when R serialises an object to RDS format, the first thing it writes is the SEXPTYPE of that object. Technically I wasnâ€™t lying, but this is an oversimplificiation that hides something important. Itâ€™s time to unpack this, and to do that Iâ€™ll have to dive into the R source codeâ€¦\nDecoding the SEXPTYPE\nAfter digging around in the source code I found the answer. What R actually does in that first entry is write a single integer, and packs multiple pieces of information into the bits that comprise that integer. Only the first eight bits are used to define the SEXPTYPE. Other bits are used as flags indicating other things. Earlier on, I said that a value of 526 actually corresponds to a SEXPTYPE of 14. That becomes clearer when we take a look at the binary representation of 14 and 526. The first eight bits are identical:\n\n\nintToBits(14)\n\n\n [1] 00 01 01 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n[23] 00 00 00 00 00 00 00 00 00 00\n\nintToBits(526)\n\n\n [1] 00 01 01 01 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00\n[23] 00 00 00 00 00 00 00 00 00 00\n\nTo extract the SEXPTYPE, what we want to do is ignore all the later bits. I could write a function that uses intToBits() to unpack an integer into its binary representation, then sets all the bits except the first eight to 0, and then converts back to an integer â€¦but thereâ€™s no need. The thing I just described is a â€œbitwise ANDâ€ operation:\n\n\ndecode_sexptype <- function(x) bitwAnd(x, 255)\n\ndecode_sexptype(14)\n\n\n[1] 14\n\ndecode_sexptype(526)\n\n\n[1] 14\n\nWhen I said that those 262153 values we encounter every time a string is serialised actually correspond to a SEXPTYPE of 9, this is exactly what I was talking about:\n\n\ndecode_sexptype(262153)\n\n\n[1] 9\n\nThe attributes pairlist, which gave us a value of 1026 when the RDS is printed out as text?\n\n\ndecode_sexptype(1026)\n\n\n[1] 2\n\nThose are SEXPTYPE 2, and if we check the R internals manual again, we see that this is indeed the code for a pairlist.\nI feel triumphant, but broken.\n\nGirl, same.\n\n\n\n\nFigure 11: Image by Aimee Vogelsang. Available by CC0 licence on unsplash.\n\n\n\nWhatâ€™s in the other bits?\nI fear that my mind is lost, but in case anyone uncover these notes and read this far, I should document what I have learned about the contents of the other bits. There are a few different things in there. The two youâ€™d most likely encounter are the object flag (bit 9) and the attributes flag (bit 10). For example, consider the data frame below:\n\n\ndata.frame(\n  a = 1, \n  b = 2\n)\n\n\n  a b\n1 1 2\n\nhas an integer code of 787. Data frames are just lists with additional metadata, so itâ€™s not surprising that when we extract the SEXPTYPE we get a value of 19:\n\n\ndecode_sexptype(787)\n\n\n[1] 19\n\nBut data frames are also more than lists. They have an explicit S3 class (\"data.frame\") and they have other attributes too: \"names\" and \"row.names\". If we unpack the integer code 787 into its constituent bits we see that bit 9 and bit 10 are both set to 1:\n\n\nintToBits(787)\n\n\n [1] 01 01 00 00 01 00 00 00 01 01 00 00 00 00 00 00 00 00 00 00 00 00\n[23] 00 00 00 00 00 00 00 00 00 00\n\nBit 9 is the â€œobject flagâ€: it specifies whether or not the R data structure has a class attribute. Bit 10 is the more general one, and is called the â€œattribute flagâ€: it specifies whether or not the object has any attributes.\nOkay but whatâ€™s up with 262153?\nWho is asking me all these questions anyway?\nIt worries me that Iâ€™m now listening to the voices in my head, but okay fine. If we unpack the integer code 262153, we see that thereâ€™s something encoded in bit 19:\n\n\nintToBits(262153)\n\n\n [1] 01 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00\n[23] 00 00 00 00 00 00 00 00 00 00\n\nI havenâ€™t found the part of the source code that sets this bit yet, but Iâ€™m pretty sure that the role of this bit is to flag whether or not the string should be added to the global string pool. In recent versions of R thatâ€™s true for all strings, so in practice every string has an integer code of 262153 rather than 9.\n\n\n\nFigure 12: Image by Pelly Benassi. Available by CC0 licence on unsplash.\n\n\n\nAre we done yet?\nWell that depends on what you mean by asking the question. If you mean â€œhave we described everything there is to know about the RDS format and how data serialisation works in base R?â€ then no, weâ€™re absolutely not done. I havenâ€™t said anything about how R serialises functions or expressions:\n\n\nexpr <- quote(sum(a, b, c))\nfn <- function(x) x + 1 \n\n\n\nThese are both R objects and you can save them to RDS files. So of course thereâ€™s a serialisation format for those but itâ€™s not a lot of fun. I mean, if you squint at it you can kiiiiiinnnnda see whatâ€™s going on with the expressionâ€¦\n\n\nshow_rds(expr, header = FALSE)\n\n\n6\n1\n262153\n3\nsum\n2\n1\n262153\n1\na\n2\n1\n262153\n1\nb\n2\n1\n262153\n1\nc\n254\n\n\n\n\nâ€¦but if I do the same thing to serialise the function it gets unpleasant. This has been quite an ordeal just getting this far, and I see no need to write about the serialisation of closures. Let someone else suffer through that, because my brain is a wreck.\nSo no, we are not â€œdoneâ€. The RDS format keeps some secrets still.\nBut if you mean â€œhave we reached the point where the author is losing her mind and needs to rest?â€ thenâ€¦ oh my god yes I am utterly and completely done with this subject, and wish to spend the rest of my night sobbing quietly in darkness.\n\nLet us never speak of this again.\n\n\n\n\nFigure 13: Image by Andrey Zvyagintsev. Available by CC0 licence on unsplash.\n\n\n\n\n\nLast updated\n2021-11-19 13:58:59 AEDT\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-11-15_serialisation-with-rds/preview-image.jpg",
    "last_modified": "2021-11-19T19:07:25+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-01_unpredictable-paintings/",
    "title": "Unpredictable paintings",
    "description": "An example showing how to build a generative art system in R. The post walks through some of the creative and design choices that are involved, and highlights how much of a trial-and-error process generative art can be",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-11-01",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nDo something, anything\nMix it up a bit\nCRAN is a girlâ€™s best friend\nFormalise a systemâ€¦\nVary parametersâ€¦\nHave fun exploiting loopholes\nTweet it!\n\n\n\n\nAlmost two years (2020-01-15) ago I wrote this blog post as an introduction to generative art in R. The idea behind the post was to start making a new generative art system from scratch, and write the blog post at the same time. By doing it that way the reader can see how the process unfolds and how many false starts and discarded ideas a generative artist tends to go through, even for a simple system like this one. The post disappeared when I moved my blog to its own subdomain and its own repository, but Iâ€™ve now managed to rescue it! Hope itâ€™s helpfulâ€¦\nIntroduction\nOver the last year or so Iâ€™ve taken up generative artwork as a hobby, and I am occasionally asked to write an introduction to the subjectâ€¦ how does one get started in generative art? When I started posting the code for my generative art to my â€œrosemaryâ€ repository I has this to say about my subjective experience when making artwork,\n\nMaking generative artwork reminds me a lot of gardening. Both are aesthetic exercise, creating a pleasant and relaxing evironment that the artist/gardener can enjoy no less than anyone visiting the space. Both are hard work, too. Learning how to prune, learning which plants will thrive in the land that you have, knowing what nutrients differnt plants need, taking care of the garden in hard times, et cetera, none of these are easy. At the end you might have a sustainable native garden that blends in seamlessly with the environment and brings you joy, but growing the garden is itself a technical and sometimes physically demanding exercise. The analogy between gardening and generative artwork feels solid to me, but itâ€™s not amazingly helpful if you want to start making this kind of art. If you want to start gardening, you probably donâ€™t really want a fancy gardener to talk about their overall philosophy of gardens, youâ€™d like a few tips on what to plant, how often to water and so on. This post is an attempt to do that, and like so many things in life, it is entirely Mathew Lingâ€™s fault.\n\nThe first thing to say about generative artwork is that itâ€™s really up to you how you go about it. I do most of my programming using R, so thatâ€™s the language I use for my artwork. Most of the artwork Iâ€™ve been making lately has relied on the ambient package for the â€œgenerativeâ€ component, but to be honest you donâ€™t have to rely on fancy multidimensional noise generators or anything like that. You can use the standard pseudorandom number generators built into R to do the work. Since the point of this post is to talk about â€œhow to get startedâ€, this is exactly what Iâ€™ll do!\nIn fact, what Iâ€™m going to do in this post is build a new system for generative artâ€¦ Iâ€™m not sure what Iâ€™m going to end up with or if it will be any good, but letâ€™s see where it goes! For the purposes of this post Iâ€™m assuming that youâ€™re somewhat familiar with the tidyverse generally and ggplot2 specifically, and are comfortable writing functions in R. Thereâ€™s a couple of spots where I do something slightly more complex, but Iâ€™ll explain those when they pop up. So, here goesâ€¦\nDo something, anything\nTruth be told, I almost never have a plan when I start building a new system. What I do is start playing with pictures that visualise random data in some fashion, and see where that takes me. So, okayâ€¦ Iâ€™ll start out creating a data frame that contains random numbers: each row in the data frame is a single â€œpointâ€, and each column specifies an attribute: an x variable specifying a horizontal co-ordinate, a y variable specifying the vertical location, and a g variable that randomly assigns each of point to a â€œgroupâ€ of some kind. At this point in time I have no idea how Iâ€™m going to use this information:\n\n\nlibrary(tidyverse)\nset.seed(1)\nobj <- tibble(\n  x = rnorm(100), \n  y = rnorm(100), \n  g = sample(10, 100, TRUE)\n)\nobj\n\n\n# A tibble: 100 Ã— 3\n        x       y     g\n    <dbl>   <dbl> <int>\n 1 -0.626 -0.620      1\n 2  0.184  0.0421     3\n 3 -0.836 -0.911     10\n 4  1.60   0.158      7\n 5  0.330 -0.655      4\n 6 -0.820  1.77       1\n 7  0.487  0.717      9\n 8  0.738  0.910      7\n 9  0.576  0.384      6\n10 -0.305  1.68       4\n# â€¦ with 90 more rows\n\nSomething to note about this code is that I used set.seed(1) to set the state of the random number generator in R. This will ensure that every time I call the same â€œrandomâ€ code I will always get the same output. To get a different output, I change the seed to something different.\nSo I guess the first thing Iâ€™ll do is try a scatterplot:\n\n\nggplot(obj, aes(x, y, colour = g)) +\n  geom_point(show.legend = FALSE) + \n  coord_equal() + \n  theme_void()\n\n\n\n\nOkay, yeah thatâ€™s scatterplot. Iâ€™m not feeling inspired here, but it does occur to me that Iâ€™ve seen some very pretty hexbin plots in the past and maybe thereâ€™s some fun I could have playing with those?\n\n\nggplot(obj, aes(x, y)) +\n  geom_hex(show.legend = FALSE) + \n  coord_equal() + \n  theme_void()\n\n\n\n\nHm. Interesting? Maybe I could split this by group and try overlaying different hexagonal shapes? That sometimes makes for a neat three-dimensional feel when two hexagonal grids are offset from one anotherâ€¦ okay letâ€™s pursue that for a bitâ€¦\n[an hour passes in which I draw many boring plots]\nâ€¦yeah, okay Iâ€™ve got nothing. It seemed like a good idea but I couldnâ€™t make anything I really liked. This is, in my experience, really common. I go down quite a few blind alleys when making a generative system, discard a lot of things that donâ€™t seem to do what I want. Itâ€™s an exploration process and sometimes when you explore you get lost. Oh well, letâ€™s try something else. Instead of drawing a scatterplot, letâ€™s connect the dots and draw some lines:\n\n\nggplot(obj, aes(x, y, colour = g)) +\n  geom_path(show.legend = FALSE) + \n  coord_equal() + \n  theme_void()\n\n\n\n\nHm. A bit scribbly, but thereâ€™s something aesthetically pleasing there. Okay, what if I decided to turn the paths into polygons?\n\n\nggplot(obj, aes(x, y, fill = g, group = g)) +\n  geom_polygon(show.legend = FALSE) + \n  coord_equal() + \n  theme_void()\n\n\n\n\nOkay, this feels promising. It reminds me a bit of the time I accidentally drew some really pretty pictures by setting axis limits inappropriately when drawing kernel density estimates with ggplot2, and ended up using it as a way to explore the scico package. Letâ€™s run with thisâ€¦\nMix it up a bit\nTo try to get a sense of what you can do with a particular approach, itâ€™s usually helpful to try out some variations. For example, the previous plot uses the ggplot2 default palette, which isnâ€™t the most appealing colour scheme. So letâ€™s modify the code to use palettes from the scico package. One of my favourites is the lajolla palette:\n\n\nlibrary(scico)\nggplot(obj, aes(x,y, fill = g, group = g)) +\n  geom_polygon(show.legend = FALSE) + \n  coord_equal() + \n  theme_void() + \n  scale_fill_scico(palette = \"lajolla\")\n\n\n\n\nThis is definitely neat. I do like the â€œjagged little polygonsâ€ feel to this, but to be honest Iâ€™m getting a bit bored. Iâ€™ve done a few different art pieces that exploit this effect before, and this isnâ€™t the most exciting thing for me, so I want to push things in a different direction. Speaking of which, Iâ€™m not sure I want all the polygons to lie on top of each other so much, so what Iâ€™ll do is create a small tibble called grp that specifies a random â€œoffsetâ€ or â€œshiftâ€ for each group, and then using full_join() from dplyr to merge it into the data object:\n\n\ngrp <- tibble(\n  g = 1:10,\n  x_shift = rnorm(10),\n  y_shift = rnorm(10)\n)\nobj <- full_join(obj, grp)\nobj\n\n\n# A tibble: 100 Ã— 5\n        x       y     g x_shift y_shift\n    <dbl>   <dbl> <int>   <dbl>   <dbl>\n 1 -0.626 -0.620      1   1.13   -1.00 \n 2  0.184  0.0421     3   0.741   0.945\n 3 -0.836 -0.911     10  -0.581   1.78 \n 4  1.60   0.158      7  -0.408   0.376\n 5  0.330 -0.655      4  -1.32    0.434\n 6 -0.820  1.77       1   1.13   -1.00 \n 7  0.487  0.717      9  -0.701  -1.43 \n 8  0.738  0.910      7  -0.408   0.376\n 9  0.576  0.384      6   0.398  -0.390\n10 -0.305  1.68       4  -1.32    0.434\n# â€¦ with 90 more rows\n\nSo now I can adjust my ggplot2 code like this. Instead of defining each polygon in terms of the x and y columns, Iâ€™ll add the x_shift and y_shift values so that each polygon gets moved some distance away from the origin. This is kind of helpful, because now I can see more clearly what my objects actually look like!\n\n\nggplot(\n  data = obj, \n  mapping = aes(\n    x = x + x_shift, \n    y = y + y_shift, \n    fill = g, \n    group = g\n  )\n) +\n  geom_polygon(show.legend = FALSE) + \n  coord_equal() + \n  theme_void() + \n  scale_fill_scico(palette = \"lajolla\")\n\n\n\n\nVery pretty! But as I said, Iâ€™m bored with the â€œjagged little polygonâ€ look, so what I want to do is find some way of changing the appearance of the shapes.\nCRAN is a girlâ€™s best friend\nAt this point in my process I was a bit lost for ideas. I want to do something different, and I think what I want to do is turn each point set into more of a regular shape, something without holes in it. It then occurred to me that way back in 1998 I did my honours thesis on combinatorial optimisation problems in neuropsychological testing and had played around with things like the Travelling Salesperson Problem (TSP) and remembered that the solutions to two-dimensional planar TSPs can sometimes be quite pretty. A few minutes on google uncovers the TSP package, and a few more minutes playing around with the API gives me a sense of what I need to do in order to work out what order to connect the points in order to generate a TSP solution:\n\n\nlibrary(TSP)\ntour <- function(obj) {\n  obj$tour <- unname(c(solve_TSP(ETSP(obj[, c(\"x\", \"y\")]))))\n  arrange(obj, order(tour))\n}\n\n\n\nThe code here is very ugly because I wrote it in a rush. The gist of it is that what you want to do normally is feed a data frame to the ETSP() function, which creates the data structure needed to solve the corresponding optimisation problem. The output is then passed to solve_TSP() which can produce an approximate solution via one of many different algorithms, and that then returns a data structure (as an S3 object) that specifies the order in which the points need to be connected, along with some handy metadata (e.g., the length of the tour). But I donâ€™t want any of that information, so I use c() and unname() to strip all that information out, append the resulting information to the data object, and then use the arrange() function from the dplyr package to order the data in the desired fashion.\nNext, because I want to apply the tour() function separately to each group rather than to compute a TSP solution for the overall data structure, I use group_split() to split the data set into a list of data frames, one for each group, and then map_dfr() to apply the tour() function to each element of that list and bind the results together into a data frame:\n\n\nobj <- obj %>%\n  group_split(g) %>%\n  map_dfr(~tour(.x))\nobj\n\n\n# A tibble: 100 Ã— 6\n         x      y     g x_shift y_shift  tour\n     <dbl>  <dbl> <int>   <dbl>   <dbl> <int>\n 1 -0.626  -0.620     1    1.13   -1.00     1\n 2 -0.165  -1.91      1    1.13   -1.00     9\n 3  0.267  -0.926     1    1.13   -1.00     3\n 4 -0.103  -0.589     1    1.13   -1.00     8\n 5  0.370  -0.430     1    1.13   -1.00     4\n 6  0.557  -0.464     1    1.13   -1.00     5\n 7  2.17    0.208     1    1.13   -1.00     2\n 8  0.821   0.494     1    1.13   -1.00    10\n 9 -0.820   1.77      1    1.13   -1.00     7\n10 -0.0162 -0.320     1    1.13   -1.00     6\n# â€¦ with 90 more rows\n\nNow when I apply the same plotting code to the new data object, hereâ€™s what I get:\n\n\nggplot(\n  data = obj, \n  mapping = aes(\n    x = x + x_shift, \n    y = y + y_shift, \n    fill = g, \n    group = g\n  )\n) +\n  geom_polygon(show.legend = FALSE) + \n  coord_equal() + \n  theme_void() + \n  scale_fill_scico(palette = \"lajolla\")\n\n\n\n\nOoh, I like this.\nFormalise a systemâ€¦\nThe next step in the process is to take all the moving parts and write a system. The exact details of what consitutes a generative art system is a little vague, but I usually think of it as a collection of functions that capture the essence of the process. If Iâ€™m being fancy Iâ€™ll convert this set of functions to a full-fledged R package, but letâ€™s not bother with that for this simple system. So what do we need? First, Iâ€™ll state the dependencies:\n\n\nlibrary(tidyverse)\nlibrary(scico)\nlibrary(TSP)\n\n\n\nNext, letâ€™s keep the tour() function as a separate thing. Itâ€™s one way of organising the points that belong to the same group, but there might be others:\n\n\ntour <- function(obj) {\n  tsp <- ETSP(obj[,c(\"x\",\"y\")])\n  obj$tour <- unname(c(solve_TSP(tsp)))\n  arrange(obj, order(tour))\n}\n\n\n\nMy personal style is to separate the â€œbuilderâ€ functions that generate the underlying data structure from the â€œstylingâ€ functions that render that data structure as an image. For the current project, our builder function is build_art() and defined as follows:\n\n\nbuild_art <- function(\n  points = 100,   # total number of points\n  groups = 10,    # number of groups\n  polygon = tour, # function used to organise points\n  gap = 1,        # standard deviation of the \"shift\" separating groups\n  seed = 1        # numeric seed to use\n) {\n  \n  # set the seed\n  set.seed(seed)\n  \n  # create the initial data frame\n  obj <- tibble(\n    x = rnorm(points), \n    y = rnorm(points), \n    g = sample(groups, points, TRUE)\n  )\n  \n  # create the offset for each group\n  grp <- tibble(\n    g = 1:groups,\n    x_shift = rnorm(groups) * gap,\n    y_shift = rnorm(groups) * gap\n  )\n  \n  # merge obj with grp\n  obj <- full_join(obj, grp, by = \"g\") \n  \n  # split obj by group and apply the \"polygon\" mapping\n  # function separately to each group\n  obj <- obj %>%\n    group_split(g) %>%\n    map_dfr(~polygon(.x))\n  \n  return(obj) # output\n}\n\n\n\nAs you can see, itâ€™s more or less the same as the code I developed for my original example, just written with a little more abstraction so that I can feed in different parameter values later. The draw_art() function takes this object as input, and creates a plot using the same ggplot2 code. The only free â€œparameterâ€ here is the ... that I can use to pass arguments to the palette function:\n\n\ndraw_art <- function(obj, ...) {\n  ggplot(\n    data = obj, \n    mapping = aes(\n      x = x + x_shift, \n      y = y + y_shift, \n      fill = g, \n      group = g\n    )\n  ) +\n    geom_polygon(show.legend = FALSE) + \n    coord_equal() + \n    theme_void() + \n    scale_fill_scico(...)\n}\n\n\n\nNow weâ€™re ready to go! Because I set it up so that every parameter has a default value that corresponds to the same parameters I used to draw the original picture, this code reproduces the original image:\n\n\nbuild_art() %>% \n  draw_art()\n\n\n\n\nâ€¦ well, almost!\nVary parametersâ€¦\nOkay, the one thing that I didnâ€™t do is specify the default palette. In the scico package the default palette is â€œbilbaoâ€, and the original artwork I produced used the â€œlajollaâ€ palette. So the default output of the system is identical to this:\n\n\nbuild_art(seed = 1) %>% \n  draw_art(palette = \"bilbao\")\n\n\n\n\nIf Iâ€™d set palette = \"lajolla\" Iâ€™d have obtained exactly the same result as before. But letâ€™s play around a little bit. If I switch to the â€œvikâ€ palette I get output with the same shapes, just with a different colours scheme:\n\n\nbuild_art(seed = 1) %>% \n  draw_art(palette = \"vik\")\n\n\n\n\nHowever, if I modify the seed argument as well I get different random points, and so the resulting shapes are different.\n\n\nbuild_art(seed = 2) %>% \n  draw_art(palette = \"vik\")\n\n\n\n\nMore generally, I can play around with my new system and find out what it is capable of. Hereâ€™s a version with 1000 points divided into 5 groups with a fairly modest offset:\n\n\nbuild_art(\n  points = 1000, \n  groups = 5,\n  gap = 2\n) %>% \n  draw_art(\n    palette = \"vik\", \n    alpha = .8\n  )\n\n\n\n\nThe shapes arenâ€™t quite what I was expecting: Iâ€™m not used to seeing TSP solutions rendered as polygons, because theyâ€™re usually drawn as paths, and they make me think of crazy shuriken or maybe really screwed up snowflakes. Not as organic as I thought it might look, but still neat. Notice that Iâ€™ve also made the shapes slightly transparent by setting the alpha argument that gets passed to scale_fill_scico(). Okay, letâ€™s play around a bit more:\n\n\nbuild_art(\n  points = 5000, \n  groups = 20,\n  gap = 7\n) %>% \n  draw_art(\n    palette = \"bamako\", \n    alpha = .8\n  )\n\n\n\n\nThis is kind of neat too, but I want to try something different. The general pattern for a TSP solution is that they take on this snowflake/shuriken look when there are many points, but not when there are fewer data points. So this time Iâ€™ll have 10000 points in total, but divide them among 1000 groups so that on average each polygon is defined by 10 vertices. Iâ€™ll space them out a little bit more too, andâ€¦\n\n\nbuild_art(\n  points = 10000, \n  groups = 1000,\n  gap = 15, \n  seed = 10\n) %>% \n  draw_art(palette = \"tokyo\")\n\n\n\n\nI kind of love it!\nHave fun exploiting loopholes\nAt the beginning, when I created the system, I set tour() to be the default polygon function used to modify each polygon. My original plan was that this function was really just supposed to be used to order the points, but thereâ€™s actually nothing in the system that prevents me from doing something fancier. For example, hereâ€™s a sneaky trick where the function calls dplyr::mutate() before passing the data for that group to the tour() function. In this case, what Iâ€™ve done is a dilation transformation: the overall size of each group is multiplied by the group number g, so now the shapes will lie on top of each other with different scales. It also, in another slightly sneaky trick, flips the sign of the group number which will ensure that when the data gets passed to draw_art() the order of the colours will be reversed. The resultâ€¦\n\n\nshift_tour <- function(obj) {\n  obj %>% \n    mutate(\n      x = x * g, \n      y = y * g, \n      g = -g\n    ) %>%\n    tour()\n}\n\nbuild_art(\n  points = 5000,\n  groups = 200,\n  gap = 0,\n  polygon = shift_tour\n) %>% draw_art(palette = \"oslo\")\n\n\n\n\nâ€¦ is really quite lovely. Later on, I might decide that this little trick is worth bundling into another function, the system gains new flexibility, and the range of things you can do by playing around with it expands. But I think this is quite enough for now, so itâ€™s time to move on to the most important step of all â€¦\nTweet it!\nBecause whatâ€™s the point of making art if you canâ€™t share it with people?\n\n\nLast updated\n2021-11-02 06:45:43 AEDT\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-11-01_unpredictable-paintings/featured.png",
    "last_modified": "2021-11-02T06:45:44+11:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-10-19_rtistry-posts/",
    "title": "Generative art resources in R",
    "description": "An extremely incomplete (and probably biased) list of resources to\nhelp an aspiring generative artist get started making pretty pictures\nin R",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nPeople often ask me if I have any words of advice for young people. No wait, thatâ€™s not right. Nobody wants to hear my words of advice for young people, largely because I have none. What they often do ask me is if I have any words of advice for aspiring generative artists who want to use R to make pretty pictures. To be honest, I donâ€™t have a lot of advice there either, but Iâ€™ll try my best.\nLetâ€™s start with the big picture: there are no books or general purpose introductions out there. There are no books, no CRAN task views, no courses you can take. In fact, until quite recently generative art in R was an extremely niche topic. To my knowledge, the #rtistry hashtag on twitter is where youâ€™ll find the most art and the most posts about the topic, but that hashtag is pretty new.1 There were resources that existed prior to that, of course: how could there not be? After all, Thomas Lin Pedersen has been building a toolkit for generative art in R for quite some time now. In his keynote talk at celebRation2020, he refers to an â€œart driven developmentâ€ process that has led him to create several packages that are valuable to the would-be generative artist. For example:\nambient is amazingly useful when you want to play around with flow fields or generate interesting multidimensional noise patterns\nparticles provides a toolkit for particle simulation in R: itâ€™s inspired by the d3-force javascript module, and is useful when you want to simulate forces acting on particles\nggforce provides a collection of tools that add some missing functionality to ggplot2, much of which is very handy when making art\nThese tools are great, but if youâ€™re just getting started it can be helpful to play around in a more constrained environment. If you want something extremely simple, you could play around with the flametree package I wrote. Itâ€™s not very flexible (it just draws branching things!) but it does have the advantage that you can get started with something as simple as this:\n\n\nlibrary(flametree)\n\n# pick some colours\nshades <- c(\"#1b2e3c\", \"#0c0c1e\", \"#74112f\", \"#f3e3e2\")\n\n# data structure defining the trees\ndat <- flametree_grow(time = 10, trees = 10)\n\n# draw the plot\ndat %>% \n  flametree_plot(\n    background = \"antiquewhite\",\n    palette = shades, \n    style = \"nativeflora\"\n  )\n\n\n\n\nPlaying around with a package like flametree â€“ or jasmines if you want something a little more flexible â€“ is a nice way to start drawing things, but at some point you might want to understand the process involved in creating a system like this. Iâ€™ve occasionally used art as a way to help teach people how to program in R, so you might find these programming of aRt slides helpful, and the precursor to flametree is discussed in my slides on functional programming.\nResources like mine can help get you started, but there are many other great artists out there who often post tutorials and walkthroughs. For instance, Antonio SÃ¡nchez ChinchÃ³n has a lot of really useful tutorials on his blog fronkonstin.com. Ijeamaka Anyene has written a lovely and gentle introduction to her system for rectangle subdivision. Will Chase writes about his process on his blog sometimes: hereâ€™s an example on a grid system. Jiwan Heo has a wonderful post on how to get started with flow fields in R among many other lovely posts! You can look outside of the R community too: Tyler Hobbs writes a lot of essays about generative art that describe algorithms in fairly general terms. For instance, one of my systems is built from his essay on simulating watercolour paints. And of course thereâ€™s also the walkthrough I wrote for one of my systems here and the piece I wrote that talks a little bit about the psychological process of making art in R.\nMy hope is that these resources will point you in the right direction to get started, but more than anything else I would emphasise that it takes time and effort. Art is a skill like any other. Iâ€™ve been practicing for about three years now, and while I am happy with the pieces I make, I still have a lot to learn. And thatâ€™s okay â€“ one of the big things I always want to stress is that play is a huge part of the process. Making polished systems comes later!\nIn any case, Iâ€™ll leave this post as it is for now but if folks would like to suggest additional resources, I can always update it if need be!\nPostscript\nOkay, Iâ€™m going to start adding things. This is just a completely unstructured list for now, but I know how my brain works: if I donâ€™t bookmark the cool posts and resources I see pop up on my timeline Iâ€™ll never find them againâ€¦\nR specific\nThinking outside the grid by Meghan Harris\nGradients repository by Sharla Gelfand\nGenerative art package by Nicola Rennie\nVarious art posts by Claus Wilke\nggbenjamini package by Urs Wilke\nGenerative art examples by Pierre Casadebaig\nThe art in randomness by Dorit Geifman\nMore general\nthatcreativecode.page is a great general resource\nthe description of asemi.ca shows a design process in detail\nTyler Hobbs generative art essays\n\nLast updated\n2021-12-20 10:00:48 AEDT\nDetails\nsource code, R environment\n\n\n\n\n\n\nA little bit of searching on twitter suggests that I proposed the hashtag on January 20th 2021 in a now-deleted tweet.â†©ï¸\n",
    "preview": "posts/2021-10-19_rtistry-posts/preview-image.png",
    "last_modified": "2021-12-20T10:00:48+11:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-09-30_on-blogging-reproducibly/",
    "title": "On blogging reproducibly with renv",
    "description": "Some initial thoughts on how to deploy a distill blog in a reproducible fashion. It's a little harder than it looks and I am still working out all the details. To make my life a little easier, I started writing a small package called \"refinery\", which uses the renv package to manage a separate R environment for every post, and aims to prevent conflicts between renv and distill. I'm not sure it's useful to anyone except me, but it makes me happy.",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-09-30",
    "categories": [],
    "contents": "\n\nContents\nWhy is this so hard?\nSome useful tools\nProject environments with renv\nEscaping the Catch 22\nStep 1: Create the post\nStep 2: Start using renv\nStep 3: Loading the environment\nStep 4: Updating the R environment\nStep 5: Let your readers know!\n\nSoâ€¦ what next?\n\n\n\n\nI started my very first blog in the dark ages, when dialup internet was a thing and the 21st century was still shiny and new. There are very few hints that this blog ever existed, which is perhaps fortunate for me since it wasnâ€™t very good. For all its flaws though, it was a useful thing to try: rather than use one of the big blogging platforms, I hosted my own static site using the university website, and it got me started thinking about other forms of professional communication besides the tiresome process of writing academic papers. Besides, writing blog posts isnâ€™t just useful, itâ€™s fun.\nIt is surprising, then, that I havenâ€™t managed to keep any of my many blogs running consistently. I used to think this was a personal failing on my part, but Iâ€™ve come to realise that technical blogging is an extremely difficult thing to do cleanly. In my first post on this blog I outlined four principles that Iâ€™ve tried to adhere to over the last year or two, and I think theyâ€™ve served me well:\nSimplicity. Try to use the simplest tools you can: hidden dependencies will hurt you later\nEncapsulation. Isolate the blog: donâ€™t incorporate it into your home page\nFocus. A blog should do one thing well: if you do many things, you can have many blogs\nReproducibility. An R blog needs to manage the R environment cleanly\nThe first three are (I think) somewhat self explanatory. Itâ€™s the fourth one that I want to talk about here, because itâ€™s a lot harder than it looks, and my initial post on this blog underestimated how tricky it can be to get this one right. I wonâ€™t be so arrogant as to claim that Iâ€™ve gotten it right now, but with the help of Kevin Usheyâ€™s very excellent renv package, Iâ€™m slowly making progress!\nWhy is this so hard?\nRunning a programming blog based in R markdown is fundamentally hard, because of the very thing that makes R markdown attractive: the blog post is also the source code. This is a both a blessing and a curse. Itâ€™s a blessing because it forces you, the blogger, to write code that is readable to your audience. It forces you to write code that actually works: if the code doesnâ€™t work, the post doesnâ€™t knit. This is extremely valuable to you and to your audience. Having become addicted to literate programming tools such as R markdown, I would never want to go back to the bad old days where you wrote your code in scripts and pasted chunks of non-executable code into a document. Over and over again I found that this introduced horrible problems: Iâ€™d fix a bug in the source code, and then forget to update it in the document. With the advent of R markdown and the many tools that rely on it (distill, blogdown, bookdown, etc), I hope never to be forced to return to that nightmare.\nHowever, there is a catch. There is always a catch. The catch in this cases is that managing your R environment is hard. Every time you write a new post, your R environment is likely to change. Packages will have been updated, and there is a chance that code you wrote in an old post will no longer run the same way now as it did back then. The passage of time means that eventually all your old posts break: they were written using a particular R environment that no longer exists on your computer. Whatâ€™s worse is that every post has a unique environment. If you want to ensure that old posts still knit, then every post needs to be associated with its own reproducible R environment. In effect, youâ€™re in a situation where you need to maintain many R projects (one per blog post), that are themselves contained within an encompassing R project (the blog itself). Thatâ€™s not easy to do.\n\n\n\nFigure 1: Image by Patrick Tomasso. Available by CC0 licence on unsplash.\n\n\n\nSome useful tools\nThe difficulty in managing the R environments in a blogging context is something that comes up a lot, and there are a few workarounds that make your life a little easier. For example, in a Distill blog like this one, you maintain manual control over when a post is rendered. Building the whole site with rmarkdown::render_site() wonâ€™t trigger a rebuild of posts, so itâ€™s possible to rebuild the rest of the site without attempting to re-knit old posts. This is a very good thing, and in the early days of blogdown the fact that you didnâ€™t have that protection was the source of a lot of problems (happily, they fixed that now!)\nAnother thing you can do to make things a little easier is to use utils::sessionInfo() or devtools::session_info(). Appending a call to one of these functions to your post will at least ensure that the reader of your post knows something about what the R environment was at the time you last knit the post:\n\n\nsessionInfo()\n\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nloaded via a namespace (and not attached):\n [1] fansi_0.5.0         rprojroot_2.0.2     digest_0.6.28      \n [4] R6_2.5.1            jsonlite_1.7.2      magrittr_2.0.1     \n [7] evaluate_0.14       highr_0.9           stringi_1.7.4      \n[10] rlang_0.4.11        renv_0.14.0         fs_1.5.0           \n[13] refinery_0.0.0.9057 jquerylib_0.1.4     bslib_0.3.0        \n[16] vctrs_0.3.8         rmarkdown_2.10      distill_1.2        \n[19] tools_4.1.1         stringr_1.4.0       xfun_0.26          \n[22] yaml_2.2.1          fastmap_1.1.0       compiler_4.1.1     \n[25] htmltools_0.5.2     knitr_1.36          downlit_0.2.1      \n[28] sass_0.4.0         \n\nThese are useful, and taken together itâ€™s possible to run a blog that wonâ€™t break on you, but itâ€™s still less than ideal. For example, one problem I used to encounter often is the â€œminor editâ€ headache. I would often want to revisit an old blog post â€“ one that no longer knits because the R environment has changed â€“ and add a brief note mentioning that the code doesnâ€™t work with more recent versions of certain packages! This is something I think is important to do, so that anyone reading my old posts wonâ€™t try using the same code in an R environment that wonâ€™t run it. At a bare minimum that seems polite, butâ€¦ in order to make the update, I would need to modify the post, which means Iâ€™d have to re-knit the post, butâ€¦ as aforementioned, the post wonâ€™t knit. Itâ€™s a Catch 22: you canâ€™t inform people that the post wonâ€™t knit unless you are able to knit the post.\n\n\n\nFigure 2: Image by Andrew Neel. Available by CC0 licence on unsplash.\n\n\n\nProject environments with renv\nI imagine there are many different ways to solve this problem, but the approach Iâ€™ve taken in this blog is to rely on the renv package by Kevin Ushey. The goal of renv is to allow you to create and manage reproducible R environments that you can associate with a project. This post isnâ€™t the place to write a full tutorial on how to use renv, but to oversimplify somewhat, the renv package manages an R environment using two things the lockfile and the a local package library. For any given project, you can start using renv using renv::init().\nThe lockfile associated with an R project has the file name renv.lock and it consists of a collection of records that precisely specify the version of renv, the version of R, and detailed information about the packages used in the project. One nice property of renv is that the lockfile is capable of tracking packages installed from GitHub as well as CRAN. For example, hereâ€™s what an entry looks like for a package installed from CRAN:\n\"distill\": {\n  \"Package\": \"distill\",\n  \"Version\": \"1.2\",\n  \"Source\": \"Repository\",\n  \"Repository\": \"CRAN\",\n  \"Hash\": \"5edf0b55f685c668d5e800051bc31f3d\"\n}\nThis entry tells you that this post (because Iâ€™m copying from the lockfile for this post) was generated using version 1.2 of the distill package, downloaded from CRAN. On the other hand, the version of cli that Iâ€™m currently using came from GitHub:\n\"cli\": {\n  \"Package\": \"cli\",\n  \"Version\": \"3.0.1.9000\",\n  \"Source\": \"GitHub\",\n  \"RemoteType\": \"github\",\n  \"RemoteHost\": \"api.github.com\",\n  \"RemoteRepo\": \"cli\",\n  \"RemoteUsername\": \"r-lib\",\n  \"RemoteRef\": \"HEAD\",\n  \"RemoteSha\": \"154f3215e458728a2155217a7f4897da5b8edea0\",\n  \"Hash\": \"3347d46b7c20b31f8d40491f57e65c38\"\n}\nThe complete lockfile is rather long, and it contains all the information that you need to recreate the R environment.1 For any given project, you can create a lockfile using renv::snapshot().\nHowever, although the lockfile contains the description of the R environment, it doesnâ€™t actually contain the packages. Without the actual packages, you canâ€™t do very much, so the renv package creates a local package library for each project, which contains the actual package installations.2. Given a lockfile, you can update the corresponding library using renv::restore().\nTo learn more about renv, I strongly recommend reading the package documentation. Itâ€™s very good.\n\n\n\nFigure 3: Image by Nadine Marfurt. Available by CC0 licence on unsplash.\n\n\n\nEscaping the Catch 22\nThe usual intent when using renv is to maintain one R environment per project, which is not quite perfectly aligned with the needs of a blog. For the blogging situation, we want one R environment per post, and â€“ importantly â€“ we donâ€™t want the renv infrastructure and the blog infrastructure to interfere with each other. Itâ€™s not too difficult to do this, but I found it a little finicky to get started. So, to make my life a little easier, I started writing refinery, a small package whose sole purpose is to make distill and renv play nicely together!\nThe package is very much a work in progress. Itâ€™s reached the point where I can start using it on a regular basis in my own blogging, but thatâ€™s as far as Iâ€™ve gotten. But, to give you a sense of some of the design choices Iâ€™ve made, hereâ€™s a quick run through. The intended blogging workflow is as follows:\nStep 1: Create the post\nAs a general rule, I find it extremely helpful to create posts from a template file. In my blog thereâ€™s a _templates folder containing R markdown files that are pre-populated with information that rarely changes (e.g., my name doesnâ€™t change very often). Actually, I only have one template for this blog, but in principle there can be as many as you like: my post template has author information pre-populated, contains instructions on which fields need to be updated, and so on. Using templates is a low-tech but effective way of improving reproducibility, because it will help to ensure that all posts adhere to a common structure.\nSo the first step is to create a new post from a template, and to that end the refinery package has a use_article_template() function:\n\nrefinery::use_article_template(\n  template = \"_templates/standard_post.Rmd\",\n  slug = \"fabulous-blog-post\", \n  date = \"1999-12-31\"\n  renv_new = FALSE\n)\n\nAt a minimum, you need to specify the template argument and the slug argument. If you donâ€™t specify a date, todayâ€™s date will be used. The concept behind this function is not at all novel: it was inspired by and is deeply similar to the create_post_from_template() function from Ella Kayeâ€™s distilltools package. The arguments are a little different, but itâ€™s the same idea.\nWhere use_article_template() differs from other â€œnew postâ€ functions is that it contains a renv_new argument. If renv_new = TRUE (the default), then creating the post will also set up the infrastructure necessary to manage the R environment with renv. My usual approach is to stick with the default, and allow use_article_template() to take care of that step for me, but for expository purposes the code snippet above prevents that from happening. So weâ€™ll have to do that manually in the next section.\nIn the meantime, however, the effect of calling use_article_template() is to create a post inside the _posts folder of your blog. In the example above, a new folder will be created here:\n_posts/1999-12-31_fabulous-blog-post\nInside this folder will be an index.Rmd file that has been constructed from the post template.\n\n\n\nFigure 4: Image by Nick Fewings. Available by CC0 licence on unsplash.\n\n\n\nStep 2: Start using renv\nBecause I set renv_new = FALSE in the code snippet above, we currently donâ€™t have any renv infrastructure associated with this post. To do that, weâ€™d use the following command:\n\n\nrefinery::renv_new(\"1999-12-31_fabulous-blog-post\")\n\n\n\nLike everything else in the refinery package, this is just a convenience function. All of the heavily lifting is being performed by renv::init(). What the refinery::renv_new() does is make sure that the renv infrastructure doesnâ€™t get lumped in with the distill infrastructure, and a few other little niceties.\nWhy separate renv from distill? Iâ€™m so glad you asked! The default behaviour of renv::init() is to create a renv folder inside your project directory. This makes perfect sense in the â€œone environment per projectâ€ scenario, but itâ€™s awkward for a blog. If you define â€œthe blogâ€ as the project, then youâ€™re right back where you started: thereâ€™s no way to have separate environments for each post. But if you define â€œthe postâ€ as the project, you run into a different problem: distill doesnâ€™t know about renv, and if a post folder contains a renv folder, distill will search inside it looking for things that might be blog posts (and it sometimes finds them, which leads to chaos!) We donâ€™t want that.\nThe solution adopted by the refinery package is to create a new top level folder called _renv,3 and then place all the renv infrastructure in there. For our hypothetical post above, the renv infrastructure would be stored in\n_renv/_posts/1999-12-31_fabulous-blog-post\nThe lockfile and library files associated with our new blog post are stored in there, cleanly separated from anything that distill would be interested in peeking at!\nStep 3: Loading the environment\nThe next step is to make sure that your blog post makes proper use of the renv infrastructure weâ€™ve just created. To do that for the hypothetical post above, all youâ€™d need to do is ensure that the R markdown file contains a line like this:\n\n\nrefinery::renv_load(\"1999-12-31_fabulous-blog-post\")\n\n\n\nWhat that will do is ensure that when the post is knit, all the R code is executed using the R environment associated with the post. Yet again, if you take a look at the source code youâ€™ll see that the refinery package really isnâ€™t doing very much work. This is a very thin wrapper around renv::load().\nStep 4: Updating the R environment\nWhen writing a new blog post, there are two main functions in the refinery package that I use to manage the R environment (and a third one I use to rage quit!)\nThe refinery::renv_snapshot() function is a wrapper around renv::snapshot(): it uses renv to scan the post folder looking for package dependencies, and then writes the lockfile to the appropriate location in the renv infrastructure.\nThe refinery::renv_restore() function is a wrapper around renv::restore(). It updates the state of the local package library so that it mirrors the state of the lockfile\nThe refinery::renv_delete() function deletes all the renv infrastructure associated with a particular post. It exists so that you can wipe the local library, lockfile, etc, and start over.\nThe process works like this. When the renv infrastructure gets created using refinery::renv_new(), it includes a bare minimum of packages in the local package library: only renv, distill, refinery, and their dependencies are added. It doesnâ€™t, for example, include dplyr.\nAs youâ€™re writing your blog post, you might find yourself using dplyr functions, and when you go to knit that postâ€¦ it wonâ€™t work, even if you have dplyr on your machine. Thatâ€™s because dplyr is not yet listed in the lockfile and itâ€™s not stored in the local package library. We can fix this with two lines of code. First, we can use refinery::renv_snapshot() to scan the current post: because Kevin Ushey is very smart and renv is a very good package, the renv::snapshot() function that does all the real work will automatically discover that dplyr is being used, and it will update the lockfile:\n\n\nrefinery::renv_snapshot(\"1999-12-31_fabulous-blog-post\")\n\n\n\nThis updates the lockfile, but only the lockfile. What you can then do is use the updated lockfile to update the library. The command for that is refinery::renv_restore() which â€“ shockingly â€“ is in fact just a thin wrapper around renv::restore():\n\n\nrefinery::renv_restore(\"1999-12-31_fabulous-blog-post\")\n\n\n\nOnce youâ€™ve done that, your post will knit, your lockfile will record all the reproducibility information associated with your post, and you will be happy! (Maybe)\nStep 5: Let your readers know!\nOne thing Iâ€™ve been doing on my blog is including a couple of additional appendices besides the usual ones that distill provides: a â€œlast updatedâ€ appendix that contains the timestamp indicating when post was most recently re-knit, and a â€œdetailsâ€ appendix that contains two links: one that goes to the R markdown source for the blog post, and another one that goes to the renv lockfile for the post. For that, thereâ€™s a convenience function called insert_appendix(). There are two arguments you need to include: repo_spec is the usual â€œuser/repoâ€ specification for the GitHub repository, and name is the name of the folder containing the blog post. Something like this:\n\nrefinery::insert_appendix(\n  repo_spec = \"djnavarro/distill-blog\"\n  name = \"1999-12-31_fabulous-blog-post\"\n)\n\n\n\n\nFigure 5: Image by Brett Jordan Available by CC0 licence on unsplash.\n\n\n\nSoâ€¦ what next?\nOne of the open questions I have is whether itâ€™s worthwhile putting much more effort into the refinery package. As it stands Iâ€™m planning to improve the documentation a little (so that â€œme six months in the futureâ€ doesnâ€™t hate â€œme todayâ€), but in truth this is something I wrote for myself: I like having the refinery package around because it supports this blog, but that goal is now (I hope!) mostly accomplished. It may be that other folks running distill blogs would like to use these tools, in which case it might be valuable to do something more rigorous like, oh, write some unit tests and send it to CRAN.\nFor now though Iâ€™m happy where things stand. If things work as planned, this should give me the infrastructure I need to maintain this blog properly for as long as I want to, and when the world moves on and an old post is no longer accurate, it should be easy to edit the post noting that the code in the post wonâ€™t work any more, re-knit it using the original R environment, and continue blogging with fewer tears. At least, thatâ€™s the hope!\n\n\nLast updated\n2021-10-01 20:41:34 AEST\nDetails\nsource code, R environment\n\n\n\n\n\n\nIt doesnâ€™t give you complete information about the machine itâ€™s running on though, and Iâ€™m not quite at the point that Iâ€™m willing to resort to docker yet!â†©ï¸\nThis is an oversimplification: renv tries to be efficient and maintains a cache that helps you avoid duplication. But as I said, Iâ€™m not going to dive into details hereâ†©ï¸\nI decided to call the top-level folder _renv rather than renv to ensure that distill will ignore the folder unless you explicitly tell it otherwise. The _renv files wonâ€™t end up being copied to your website.â†©ï¸\n",
    "preview": "posts/2021-09-30_on-blogging-reproducibly/preview-image.jpg",
    "last_modified": "2021-10-01T20:41:35+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-26_setting-up-on-hic-et-nunc/",
    "title": "How to mint digital art on HEN",
    "description": "Not every artist wants to make cryptoart, and that's okay. Others do, and that's okay too. But if you want to try it out in a socially responsible way, it takes a bit of effort to get started. This post attempts to make the process a little easier by walking you through the whole thing, on the assumption that you are an artist who (like me) has very little interest in blockchain or cryptocurrency but would like to get paid and pay the rent",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-09-26",
    "categories": [],
    "contents": "\n\nContents\nPrologue: Barriers to entry\nEnvironmental costs matter\nThere are alternatives\nIt can be confusing\nLetâ€™s demystify it\n\n1: Get an overview\n2: Create a wallet\n3: Tell HEN who you are\nSynchronise with your wallet\nThe HEN menu is weird\nName, avatar and bio\nCheck out your profile\n\nIntermission: Follow people\n4: Get a little money\n5: Release your art!\nMinting the art\nPutting OBJKTs up for sale\nCreating auctions\nSome artâ€¦\n\n6: Share on social media\n7: Manage your identity\nEstablishing a Tezos Profile\nCreating an alias on Tezos Blockchain Explorer\nPurchasing a Tezos Domain\n\n8: Convert tez to dollars\nEpilogue: Is all it worth it?\n\n\n\nCryptoart can be a touchy subject for generative artists, and itâ€™s something a lot of us have messy feelings about. In my case it is no secret that I feel conflicted, and I completely understand why a lot of us are uncomfortable with it. I genuinely believe there are many perfectly good reasons why a generative artist would choose not to participate. On the other hand, I also recognise that there are some very sensible reasons why a generative artist would want (or need) to sell NFTs: artists have to pay rent, for example. So this post isnâ€™t about passing judgment one way or the other. Itâ€™s intended to be a guide to help other artists get started in this area, particularly artists in the R community, if they should decide to try it out. Thatâ€™s all.\nThis post is also not supposed to be an introduction to blockchains or cryptocurrencies. It doesnâ€™t dive into the details on what these things are or even what an NFT is. I make art: I donâ€™t care about any of these subjects. What Iâ€™m assuming is that youâ€™re coming to this world from a similar position to me: you have a vague understanding of what blockchain is, what cryptocurrencies are about, and have a similarly vague notion that an NFT is kind of like a â€œdigitally signed copyâ€ of your art that you can sell to other people. Thatâ€™s all you need.\nPrologue: Barriers to entry\nOne thing I have noticed about the world of cryptoart is that there are many barriers to entry. Some barriers are obvious: if you want to sell art on Foundation, for example, you need to be invited. To be invited, you need to know someone who can and will invite you. As anyone who has ever been excluded from a fancy venue by virtue of their race, gender, sexual orientation, transgender status etc can attest, an invitation requirement is a non-trivial and frequently discriminatory barrier. â€œBy invitationâ€ systems create entry barriers by design: for good or ill, they are inherently exclusionary. But there are other ways in which cryptoart creates barriers to entry.\nEnvironmental costs matter\nAnother kind of barrier comes from the nature of cryptoart. Blockchains were not designed to be energy efficient, and they can be extraordinarily wasteful (much more than youâ€™d think). Environmental considerations also create barriers to entry, albeit indirect barriers. For example, the biggest cryptocurrencies like Bitcoin and Ethereum operate on a â€œproof of workâ€ principle (often abbreviated to â€œPoWâ€) and as the name suggests, operations on those chains require a lot of computational work. A lot. They are staggeringly wasteful, and as a consequence the total energy consumption of these chains is so high that an NFT minted on one of these chains has a very high carbon footprint. Proof of work chains are an environmental disaster, and so (in my mind) they are socially irresponsible. Donâ€™t use them if you can avoid it.\nThis poses a problem for artists, unfortunately. The biggest cryptoart markets are based on the Ethereum chain, and Ethereum is a proof of work chain. True, there are plans to change this and make Ethereum more ethical, but it hasnâ€™t happened yet and I personally am unwilling to participate until that switch actually occurs. This is deeply unfortunate from artistic point of view, because it rules out OpenSea. It sucks because OpenSea is the largest marketplace and itâ€™s very easy to get started there. For instance, I have an unused account that I set up in a few minutes before I realised the problem. But for me the end-user convenience wasnâ€™t worth the environmental costs, so I abandoned this idea at the outset. On the plus side, OpenSea have announced that they are planning to support the Tezos blockchain (see below), and when that day comes I will probably make use of my OpenSea account: the thing I take moral issue with is not OpenSea, it is with Ethereum (or more precisely, with proof-of-work chains). Personally, I donâ€™t want to touch the stuff.\nSo what are the alternatives?\nThere are alternatives\nThe main alternative to the â€œproof of workâ€ blockchains are the â€œproof of stakeâ€ (PoS) blockchains. These donâ€™t require anywhere near as much computation, and as a consequence are much more energy efficient. For that reason, NFTs on those chains are often called â€œclean NFTsâ€. There are a multiple proof of stake chains (Tezos, Solana, etc), but the one Iâ€™m most familiar with is Tezos. To give you a sense of just how extreme the difference is, this is a screenshot that popped up on one of the sites while I was doing my initial exploration:\n\n\n\nEven if this claim is somewhat exaggerated for marketing purposes, the sheer scale of it is remarkable. A multiplicative factor of 1.5 million isâ€¦ enormous. I could literally mint NFTs on Tezos for every single image that I have ever created for the rest of my life, and it would still be several orders of magnitude more energy efficient than minting one piece on Ethereum. To my way of thinking, that makes a massive difference to the moral calculus associated with minting NFTs. In fact, the difference between Tezos and Ethereum is so extreme that there is actually one art marketplace there â€“ Bazaar â€“ that is not just carbon neutral but is actually carbon negative. Thatâ€™s only possible because Tezos is so much more efficient than Ethereum, and it becomes practical for the developers to impose a carbon tax on minting: the transaction costs are used to purchase sufficient carbon offsets to ensure the system as a whole remains carbon negative. Right now I wouldnâ€™t recommend setting up on Bazaar because itâ€™s so early in development that itâ€™s hard to use, but Iâ€™m absolutely keeping an eye on it for the future!\nSetting up on the Tezos blockchain is particularly appealing because it has an established digital art marketplace called â€œhic et nuncâ€. The name is Latin in origin and translates to â€œhere and nowâ€. Youâ€™ll usually see it abbreviated to â€œHENâ€, which is what Iâ€™ll call it in this post, but some people use â€œH=Nâ€, I guess because it looks visually similar to the HEN logo. The HEN marketplace is completely open: you donâ€™t need an invitation. Thereâ€™s no super-secret club to be invited into (as far as I know!), and to my mind thatâ€™s a huge positive. Better yet, a few folks from the R art community are already there. Iâ€™m entirely certain that there are others I donâ€™t know about yet, but so far on HEN Iâ€™ve already found Thomas Lin Pedersen, Will Chase, Antonio S. ChinchÃ³n, and George Savva. As of a few days ago, Iâ€™m there too.\nOpenness! Community! Yay!\nIf thereâ€™s one thing I have learned from the lovely R folks on twitter, everything is better when you are part of a supportive team of people who actually care about each other and work to build each other up. From my perspective, this makes HEN a very attractive option.\nThere is, unfortunately, a catch. There is always a catch.\nIt can be confusing\nOne big limitation to HEN is that it isnâ€™t easy to get started there unless you are already somewhat enmeshed in the crypto world generally, or the cryptoart scene specifically. The ecosystem is distributed over several sites that have weird names without enough vowels, the user interfaces on the sites tend to be unconventional (often pointlessly so in my opinion), and the â€œhow toâ€ guides arenâ€™t very easy to read. The overall aesthetic and typology screams out WE ARE THE COOL KIDS in capital letters. It doesnâ€™t even have the good grace to be subtle about it. Taken together, all these little things add up, and it annoys me. I have been a professional educator for 15 years now, and I can absolutely guarantee that the overall effect of this is to create a de facto entry barrier. All these things act as signals to exclude people who arenâ€™t already part of the clique. It feels disproportionately uncomfortable if youâ€™re an outsider. It tells you that youâ€™re not welcome if youâ€™re not one of the cool kids. Are you one of the cool kids? No? Then sorry. No HEN for you babe.\nWell, fuck.\nYet again, there are barriers to entry to HEN, and that makes me uncomfortable. However, unlike the other cryptoart options I looked at, thereâ€™s something I can do to improve the situation: I can write a blog post explaining the process. This blog post.\nLetâ€™s demystify it\nLetâ€™s assume youâ€™re not one of the cool kids. Letâ€™s assume youâ€™re just a regular human being who likes to make generative art in R, and are a little curious. You have a vague idea of what cryptocurrencies are (yeah, yeah, digital currency blah blah blah). You have a vague idea of what an NFT is (digitally signed copy of the art, whatever dude). Maaaaaybe youâ€™ve sort of heard of HEN â€¦ but thatâ€™s only because youâ€™ve seen some R people posting about it on twitter. And thatâ€™s it. Thatâ€™s all you know. But maybe you want to try it out, just to see if itâ€™s for you? Just to try. But you really, really, reaaaaaalllllllly donâ€™t want to wade into all the details and youâ€™re secretly worried that itâ€™s all too complicated and you wonâ€™t be able to do it. Your impostor sydrome is going wild. Is that you? Do you feel the same way I felt?\nIf so, this post is written for you.\n1: Get an overview\nWhen I started setting up on, I wandered around the Tezos cryptoart landscape in confusion, wandering aimlessly over the terrain. It was all deeply unsettling. Eventually I pieced together some overall view of things, but I wouldnâ€™t recommend doing things the same way I did. I think the best thing to do first is to â€œzoom outâ€ and look at the landscape as a whole. The best site Iâ€™ve found for doing that is tezos.art. If you click on the link it will take you to a page with the following three sections:\nMarketplaces: Sites where you can mint, buy, and sell art\nWallets: Tools that handle your identity and store your funds\nCommunity: Places where you can go for help\nItâ€™s worth taking a quick look at this page because it gives you a feel for what all the moving parts are, but doesnâ€™t dive into details. Youâ€™ve taken a quick peek, yes? Cool. Letâ€™s get startedâ€¦\n2: Create a wallet\nItâ€™s a little counterintuitive, but the natural place to start is not the art marketplaces: the first thing you need is a wallet. The reason for this is that your wallet serves two distinct purposes. As the name suggests, the wallet provides a method for storing funds: the currency itself is referred to as â€œtezosâ€, which youâ€™ll see abbreviated to â€œtezâ€ or denoted â€œêœ©â€. However, it also serves as your unique identifier on the Tezos blockchain. On blockchains as in life it is rather hard to do anything interesting without a public identity, so you need to create one first.\nOkaaaayâ€¦ at this point youâ€™d probably be wondering â€œwhere do I sign up for one of these wallets?â€ Excellent question. As you will have noticed by peeking at the tezos.art website, you have a few different options. Being offered choices is nice, of course, but it can also be anxiety-provoking when you donâ€™t even know what the differences between the options are. So, for whatever itâ€™s worth, Iâ€™ll mention that I chose Temple Wallet. I made that choice for two reasons and only two reasons. First, it was one of the options listed on the HEN wiki. Second, I was complaining privately to Will Chase about how confused I was and he told me uses Temple and I copied what he did. That being said, I suspect the choice is arbitrary.\nFor the sake of argument, Iâ€™ll assume you decided to use Temple too. So now youâ€™re clicking through the link above in order to open an account with Temple Wallet andâ€¦ wait, itâ€™s just a browser extension? Yup. This seems to be very common in blockchain land, and initially it struck me as bizarre. The longer I hang around there, however, the more I realise it does make a kind of sense. Once you start doing things on Tezos, youâ€™ll find that you have to validate everything you do. Any time you ask a website to undertake some action on your behalf, the first thing that will happen is that youâ€™ll be asked to authorise the action using your public identity. What that means is that you have to use your wallet all the time, even for things that donâ€™t cost money. A browser extension makes this a little easier. When the website asks you to authenticate, the wallet browser extension will create a little popup window that asks you to confirm the transaction. Thereâ€™s a bit of friction to the process sometimes, and it feels a little alien, but it does start to feel normal after a while.\nMoving onâ€¦ the next little strangeness is that when you set up the wallet you donâ€™t create a username, only the password, and youâ€™ll be given a â€œrecovery phraseâ€, which is a sequence of 12 random words. Donâ€™t lose either of these things. Here, as always, I strongly recommend that you use a password manager to store your password, because there arenâ€™t that many options for recovery if you start losing passwords. Personally, Iâ€™ve been using 1password for a few years and I really like it. So yes. Use a password manager, store your wallet password there and store your recovery phrase there too.\nAt the end of this process you are assigned a public identity, which is a long string of complete gibberish. For example, this is me:\ntz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7\nNaturally, the first thing I did when seeing this is groan. The second thing I did is notice the Srmojf substring and it made me think of Smurfs. So I secretly think of this gibberish identifier as the Smurf, and thatâ€™s how Iâ€™ll refer to it for the rest of this post. Of course, in the long run you probably donâ€™t want to be a random string of digits, you want to have a name! This is possible to do, and Iâ€™ll walk you through that later. But right now thatâ€™s not a complication you need to care about.\nWeâ€™ll get to that a little bit later but the key thing for now is that your equivalent of the Smurf is both a public identifier and a bank account number. If someone wants to send you some tez, all they need to know is that string.\n3: Tell HEN who you are\nSynchronise with your wallet\nWhen you go to the HEN website youâ€™ll see a little bit of text on the top right hand side that has a link that says â€œsyncâ€. Click on that:\n\n\n\nThis will bring up an overlay that looks like this:\n\n\n\nIf you chose a Temple wallet choose the â€œTemple - Tezos Wallet (ex. Thanos)â€ option. It might ask for your password at this point but it probably wonâ€™t if youâ€™re already logged in. What youâ€™re more likely to see is a screen like this:\n\n\n\nThis is a message from your wallet asking you to confirm that yes, you do want to synchronise with HEN (it also shows you that I currently have a balance of 11 tez, which I guess is something like US$60). Click on connect, and HEN will now be synchronised with your identity. You can see that because the menu at the top now looks something like this:\n\n\n\nYouâ€™re now synced: in effect, you are now logged in to HEN. You still donâ€™t have a username, but you have authenticated yourself and you can now change some settings.\nThe HEN menu is weird\nOkay, letâ€™s move to the next step. To the right of your Smurf, youâ€™ll see the â€œhamburgerâ€ menu. It behaves pretty much the same as any menu youâ€™d encounter on the internet, but some of the options have very non-intuitive names. Hereâ€™s what the menu looks like, with my annotations added:\n\n\n\nAs with everything about HEN, itâ€™s very minimalist. Some of the options are easy to understand, but others are not. The options Iâ€™ve been using most are these:\nsearch takes you to the HEN search page\nedit profile allows you add some information about yourself (see next section)\nmanage assets will take you to your profile page (it took me a long time to realise this)\nOBJKT (mint) is the option you select when you want to create art. Iâ€™ll talk bout that later\nName, avatar and bio\nThe time has come to give yourself a name. If you do things in the right order and with the right mental model of whatâ€™s going on, this is pretty easy to do, but itâ€™s easy to get a little confused because there are actually multiple things going on here, and you always have to keep in mind that your equivalent of my Smurf string is your actual identity.\nSoâ€¦ your first step is to tell HEN to link your Smurf string to a name, bio and avatar. Click on â€œedit profileâ€. This brings up another slightly unconventional looking screen that has several options you can set. Hereâ€™s what mine currently looks like:\n\n\n\nThere are three things you can do immediately without any major hassle:\nFirst, if you click on â€œchoose fileâ€ you can upload an image to give yourself a profile image.\nSecond, you can give yourself a username. The advice I read on the relevant HEN wiki page suggested that you should avoid spaces and special characters, and should stick to lower case letters because usernames are case sensitive.\nThird, you can write a brief description of yourself. It doesnâ€™t have to be very thorough. Most people say something about who they are and what they do, but you donâ€™t have to. For example, Iâ€™ve had a habit of identifying myself as â€œan object of type closureâ€ on all my social media websites. Itâ€™s intended as a silly reference to the classic R error message:\n\n\nidentity[]\n  \n\nError in identity[]: object of type 'closure' is not subsettable\n\nAs it happens, this allowed me to make an even sillier double-layered joke in my HEN bio. When you create art on HEN the tokens that you generate are referred to as OBJKTs, so now I refer to myself as â€œan OBJKT of type closureâ€. Iâ€™m so funny.\nAaaaanywayâ€¦ once youâ€™ve done those three things, click on â€œsave profileâ€, and youâ€™re done for now. Ignore everything below the â€œsave profileâ€ button. All that stuff is useful, and it will let you do things like link to your twitter profile and your github profile, but itâ€™s surprisingly finicky to set up and it costs money, so weâ€™ll leave that until later.\nCheck out your profile\nBefore moving on, take a quick look at your profile. As I mentioned earlier, you can do this through the menu system, by selecting the â€œmanage assetsâ€ option. Personally I wish theyâ€™d chosen a better name: Iâ€™m not an investor and I donâ€™t think of my art as â€œassetsâ€. The page that displays my art is my homepage on HEN, and it bothers me a little that the site frames it in such mercenary terms. Itâ€™s irritating. But whatever, itâ€™s not a dealbreaker.\nItâ€™s worth quickly commenting on the URL for your profile. When you click on the â€œmanage assetsâ€ link, it will take you to a URL that identifies you using the Smurf. For me, that URL is:\nhttps://www.hicetnunc.xyz/tz/tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7/\nAs long as you have your very own Smurf in your wallet, youâ€™ll have this URL. However, if you followed the instructions in the last section, HEN is kind enough to arrange it so that the ugly Smurf based URL will automatically redirect to one based on your username. For me, that URL is:\nhttps://www.hicetnunc.xyz/djnavarro/\nAt this point, you exist on HEN! Yaaaay!\nIntermission: Follow people\nThereâ€™s more stuff you can do to get your account set up, but you might want to take a little breather and look for some art. Maybe you want to search for someone you know in the R community who might be on HEN, and youâ€™d like to find them. As I mentioned earlier, the HEN site does have a search page, but there are some limitations. Itâ€™s okay if you want to search by keywords to find art or artists, but what it wonâ€™t let you do is follow them. Personally, I quite like being able to follow artists whose work I love, and it would be pretty cool to have a feed where I can see what theyâ€™ve posted, arranged in chronological order. Thatâ€™s where the the â€œHEN explorerâ€ website is handy:\nhttps://www.henext.xyz/\nLike HEN itself, the HEN explorer site has browsing and search capability. Itâ€™s a little clunky in places (on my browser, there seems to be a bug where the search box only works when youâ€™re on the home page), but it does the job.\nTo use HEN explorer, youâ€™ll need to synchronise with your wallet (i.e., log in). To do that you can click on the â€œprofileâ€ icon in the nav bar (the one that looks like a little person), or just visit\nhttps://henext.xyz/profile\nThat will bring up a screen that looks like this\n\n\n\nClick on the â€œconnect walletâ€ button, and it will take you through the same steps that were involved when you connected your wallet to the HEN site.\nOnce youâ€™ve done that, youâ€™re logged in to HEN explorer, and youâ€™re able to find artists you like and follow them! If you would like to follow me, you can search for â€œdjnavarroâ€ on the HEN explorer search box, or you can visit my HEN explorer profile page directly:\nhttps://www.henext.xyz/tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7\nAdd a few artists you like, and youâ€™ll get a sense of what the feed looks like. The location of the feed is\nhttps://www.henext.xyz/following\nHappy browsing!\n4: Get a little money\nOne slightly frustrating thing about this process is that itâ€™s hard accomplish very much in this arena without spending money, and weâ€™re rapidly reaching the point where youâ€™ll need a little bit. Thankfully, if youâ€™re an artist wanting to create your own art, and arenâ€™t looking to collect anyone elseâ€™s, you donâ€™t need very much to get started. If youâ€™re in the R community thereâ€™s a good chance you can ask one of the other R folks on HEN to help out. Thatâ€™s what I did, and Iâ€™m grateful to the people who sent me a few tez, and the others who spontaneously offered. R people are lovely.\nIf the â€œask a friendâ€ approach is an option for you, Iâ€™d recommend it for artists. The reason I say this is that you have a bigger set up cost (in terms of your time and effort) than someone who is joining in order to purchase art, so from the perspective of the artist all you need â€“ right now â€“ is a little start up fund. To use myself as the example, I made a lot of weird mistakes setting up and wasted quite a lot of transactions, but even with all that I think I only spent about 1 tez in total (at the exchange rate at the time that was about US$5).\nAssuming that you can solve the problem that way, you can take care of the other financials later (and thereâ€™s a guide on how to do that coming later in the post). Thereâ€™s a part of me that hopes that if the R art community does end up with a larger presence on HEN, weâ€™ll look after our own. Weâ€™re R folks, and we pay it forward because we care for each other.\nThat being said, Iâ€™m also not naive, and I know perfectly well that it doesnâ€™t always work that way, so Iâ€™ll briefly mention other options. For example, the HEN website has some suggestions for other places you can ask for help. Alternatively if you have a Visa card, one possibility is to buy through https://tzkt.io/buy-tezos (the tzkt.io site will come up later in the post!), though youâ€™ll need identification documents for this (or any other option) because itâ€™s a financial institution. Finally, you can sign up at a currency exchange, which youâ€™ll probably want to do later anyway because thatâ€™s going to be how you convert the funds from your HEN sales to regular currency. Iâ€™ll talk about that later on.\nRegardless of how you solve this part of the problem, Iâ€™m hoping that at this point you have a few tez to start out!\n5: Release your art!\nMinting the art\nSurprisingly, the process of releasing your art on HEN is quite easy, at least when compared to how complicated everything else is. If you open the menu and click on the â€œOBJKT (mint)â€ option, it will take you to the minting page, which looks like this:\n\n\n\nAt this stage in the process you upload the file, give it a name and a description, and make some decisions about (a) how many tokens you want to create, and (b) your royalties, the percentage of future sales that are returned to you. Hereâ€™s me filling one out:\n\n\n\nClick on the preview button, and it will show you a preview of what the page will look like when it goes live. If youâ€™re happy with it you can proceed and click the â€œmint OBJKTâ€ button. Youâ€™ll be asked by your wallet to confirm the minting operation (this costs a small amount of tez), and then after a short time the OBJKT (i.e., the token) exists. In this case, hereâ€™s the page displaying the OBJKT that Iâ€™ve just created:\nhttps://www.hicetnunc.xyz/objkt/359761\nPutting OBJKTs up for sale\nThe tokens now exist, but as yet they have not been placed on the market. People canâ€™t buy them from you. To place the token for sale, go to the page showing the token (i.e., the link above). It will look something like this:\n\n\n\nIf you want to put the art on sale, click on the â€œswapâ€ link that Iâ€™ve highlighted here (and if you change your mind and want to destroy it, click on the â€œburnâ€ link next to it). The interface will look like this:\n\n\n\nIt will then let you decide you many of your tokens you want to put up for sale, and set the price for each one. For this particular piece Iâ€™d decided to create a lot of tokens (there are 50 of them), and Iâ€™m going to put them all on sale at the very low price of 2 tez. I honestly know nothing about pricing, but Iâ€™m playing around with it at the moment: some pieces I mint only a single token and set the price high, others I mint a large number of tokens and set the price low. In any case, when youâ€™re happy press the â€œswapâ€ button, confirm with your wallet, and the pieces will now be on sale!\nCreating auctions\nThe mechanism Iâ€™ve shown above is the simplest way to put art on sale: you list a price and wait for someone to purchase it. However, if you want to try more exotic options like auctions, you can check out objkt.com.\nSome artâ€¦\nHere are the Native Flora pieces I posted while writing this post. Theyâ€™re all available for purchase!\n\n\n\n\n\n\n\n\nOBJKT 359814\n\n\n\n\n\n\n\n\nOBJKT 359795\n\n\n\n\n\n\n\n\nOBJKT 359761\n\n\n\n\n\n\n\n\nOBJKT 359745\n\n\n\n\n\n\n6: Share on social media\nAt some point youâ€™ll probably want to advertise the fact that the artwork is available for purchase. You donâ€™t have to, of course, and Iâ€™m honestly not sure how much of my online life I want to spend advertising art for sale, but itâ€™s handy to have the option, and that probably means sharing on social media.\nMost of us in the R community who make art are primarily sharing on twitter. Yes, I have seen some people post on reddit, others on instagram, and no doubt many other places besides, but my social media world is dominated by twitter, and Iâ€™d like to be able to post to twitter. To my mild irritation, the HEN website doesnâ€™t seem to do twitter cards properly, so if you share the link on its own, people wonâ€™t see a nice preview image.\nThere are a couple of ways to get around this. The first is to post the link on twitter and attach your art as an image: that way folks on twitter will get the link and and the image. But they wonâ€™t get an actual twitter card displaying the title of the piece.\nThe second solution is to use the hic.art website. At the moment, if you visit the website it will tell you that signups are closed, but that actually doesnâ€™t matter. You donâ€™t need to sign up to use the service. All you have to do is provide the OBJKT identifier. For instance, hereâ€™s one of my pieces on HEN:\nhttps://www.hicetnunc.xyz/objkt/354474\nThe identifier here is 354474. If I share the link above on twitter, it wonâ€™t display a very nice twitter preview. However, if I tweet this link\nhttps://hic.art/354474\nIt will display a very lovely looking twitter preview, and when the user clicks on the link or the preview it will automatically redirect to the official HEN page. Itâ€™s a nice service!\nHereâ€™s an example from Antonio SÃ¡nchez ChinchÃ³n:\n\n\nMondrianomie 28Basic cellular multiorganism grown according to neoplasticism assumptions (2033 x 2400 PNG)3 ed - 28 tez at @hicetnunc2000https://t.co/TyNvt1zMBu#HEN #hicetnunc #hicetnunc2000 #nft #NFTs #nftcommunity #nftcollectors #cleannft #nftart #tezos #tezosart\n\nâ€” Antonio SÃ¡nchez ChinchÃ³n (@aschinchon) September 25, 2021\n\n7: Manage your identity\nThere are at least three additional tools that may be useful to you in managing your identity in the peculiar world of cryptoart on the Tezos blockchain: (1) you can set up a Tezos Profile, (2) you can establish an alias on the Tezos Blockchain Explorer, and/or (3) you can purchase a Tezos Domain. None of these are strictly necessary, but all of them offer some value to you as an artist on HEN so Iâ€™ll discuss each one.\nEstablishing a Tezos Profile\nEarlier in this post I mentioned that itâ€™s possible to connect your twitter profile, github account, website, etc with your HEN profile? You can do this with the assistance of Tezos Profiles. So lets go back HEN, open the menu, click on the option that says â€œedit profileâ€ and then take a closer look at the window that pops up. Itâ€™s almost impossible to notice, but the text that reads â€œTezos Profilesâ€ is in fact a link:\n\n\n\nClicking on that link will take you to https://tzprofiles.com/, where you will see a very prominent â€œconnect walletâ€ button. Click on that button, confirm with your wallet that you want to allow tzprofiles to connect (the little popup window will appear, like it always does), and then youâ€™ll see a screen that looks like this:\n\n\n\nThere are several different things you can do here, and any of them that you verify on tzprofiles will eventually end up on HEN. For example, if you want to verify your twitter account, youâ€™ll go through a series of elaborate steps (which, yes, will have to be confirmed with your wallet) and in the end youâ€™ll be forced to send a tweet like this one:\n\n\nI am attesting that this twitter handle @djnavarro is linked to the Tezos account tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7 for @tzprofilessig:edsigtaH3nvbQjpiAfMCnT4zcQESZefXoVLPf2NEYaZeUfhwHjzRYp4oeBiiyDFLdrUAUvjBhvepyDFoxuyE2ynVYxd7TvV9fj6\n\nâ€” Danielle Navarro (@djnavarro) September 21, 2021\n\nTo verify your GitHub account itâ€™s pretty similar, except that it forces you to create a gist, using your GitHub account, that includes a signature block similar to the one in the tweet. For a website, itâ€™s the same idea except you have to insert it as a DNS record (which I found extremely irritating to do). You can verify as many or as few of these as you like, but there is some value to doing them. Because Tezos Profiles forces you to go through the clunky verification process, other people can check your HEN profile and verify for themselves that it really is you posting your artwork onto the site, and not someone else who has stolen your piece (apparently, that happens way too often)\nOnce youâ€™re done verifying your accounts, you may need to use your wallet to confirm again so that the updated Tezos Profile information can be accessed by the HEN website. After thatâ€™s been done, youâ€™ll see icons appear on your HEN page, linking to your twitter account, github account, etc:\n\n\n\nAt this point your HEN profile is meaningfully linked to your other public identities, and any artwork you mint on HEN can be traced back to you, the original artist.\nCreating an alias on Tezos Blockchain Explorer\nAll right. If youâ€™re like me youâ€™ve probably been exploring as you go and youâ€™ve been encountering other sites that seem connected to this ecosystem. In particular, you may have clicked on links associated with transactions and it has taken you to the Tezos Blockchain Explorer website. As the name suggests, the role of this website is to publicly display transactions that take place on the Tezos blockchain. For example, hereâ€™s the page showing all the transactions that have involved me in some fashion:\nhttps://tzkt.io/tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7/operations/\nWhen I first started (oh so many days agoâ€¦) it looked something like this:\n\n\n\nA lot of it is gibberish, but you can kind of see whatâ€™s going on here. Yet again you can see my Smurf, thereâ€™s a bunch of transactions that show me minting NFTs, etc. It makes a kind of sense.\nWhat might be surprising, particularly if youâ€™ve just gone to the effort of setting up a Tezos Profile, is that the account information doesnâ€™t show my avatar. It doesnâ€™t include my name, or a bio, and it doesnâ€™t include my social media links. Instead, all I have is a cartoon image of a very suspicious looking cartoon cat. Unlike HEN, the tzkt.io site doesnâ€™t pull information from your Tezos Profile.\nThe mystery deepens a little when you start noticing that the exact same cartoon cat appears on various other sites. For example, this was how my profile looked on objkt.com at the time:\n\n\n\nThe weird cryptocat was following me around across all these websites. Hm. The suspicious kitty is cute and everything, but honestly Iâ€™d prefer my usual name and profile image to follow me around instead.\nAs it turns out, the source for all these skeptical cats is the blockchain explorer site, tzkt.io, and you can submit an application to the people who run that site to create an alias for you. The process is described in this post on the â€œBaking Badâ€ blog (donâ€™t let the name and silly images fool you, the blog is associated with the people who run the site). The post will take you to a Google Form that you can fill out, in order to have your alias created. When you do this, it wonâ€™t update immediately: there is a manual verification process that takes about three days, so youâ€™ll need to be patient.\nOnce that happens youâ€™ll discover that your links have appeared on your tzkt.io page, and more importantly perhaps, you have an avatar and description on other sites that make use of this alias. This is what my profile page on objkt.com looks like now:\n\n\n\nMine is a deliberately vague because Iâ€™m a peculiar person, but you can see a slightly more informative version if you look at Thomas Lin Pedersenâ€™s profile:\n\n\n\nPurchasing a Tezos Domain\nWhen you look at the two profiles above, thereâ€™s something slightly peculiar. Notice how Thomasâ€™ profile now links to thomasp85.tez and mine links to djnavarro.tez? Thatâ€™s something slightly different again. Those addresses arenâ€™t created by the Tezos Profile, nor are they created when you set your alias on the Tezos Blockchain Explorer. Those are Tezos Domains. The idea is very sensible: human beings donâ€™t really enjoy memorising long strings of random digits. It would be much more convenient if I could message someone and say â€œhey send tez to me at djnavarro.tez, because thatâ€™s me!â€. Itâ€™s certainly nicer than trying to say â€œsend tez to me at tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7, because thatâ€™s me!â€\nIf youâ€™d like to do this, visit tezos.domains and follow the instructions there: it costs 1 tez per year to set one up.\n8: Convert tez to dollars\nAt some point, hopefully very soon, youâ€™ll sell some artwork and youâ€™ll want to get paid. To do that, youâ€™ll probably need to sign up with one of the currency exchanges. Although you likely have no desire to be a currency trader, itâ€™s a necessity if you want to get paid in real money. Yes, cryptocurrencies sound cool, but coolness does not pay the rent. My landlord expects to be paid in Australian dollars, and â€“ by extension â€“ so do I. That means exchanging your tez for regular money. The HEN wiki lists a couple of options along with the standard warning that you should definitely do your own research, because this is a thing that will depend a bit on where you live. I looked into one of their suggested options (Kraken) and it seemed fairly standard, but in the end used an Australian provider, CoinSpot. The sign up process was fairly standard, requiring identification documents for verification. Once that was completed, I was able to send money to my bank account. It ended up being a three-step process:\nSend tezos from the Temple wallet associated with my public identity (i.e., the one Iâ€™ve been using on HEN etc), to a tezos wallet that is provided for me through my CoinSpot account\nOn CoinSpot, sell the tezos in exchange for Australian dollars\nWithdraw money from my CoinSpot account and deposit it in my Australian bank account\nOnce I figured it all out it was surprisingly smooth. I imagine the process varies a lot from country to country and from exchange to exchange, but hopefully the description of my process is at least somewhat helpful.\nEpilogue: Is all it worth it?\nI havenâ€™t been doing this for very long, but Iâ€™m a little surprised to find that Iâ€™m enjoying the process of minting art on HEN. Iâ€™ve sold three pieces to people who know me, and it is a nice feeling. Iâ€™m not making mountains of money, and I donâ€™t expect that I will any time soon, but it is still quite satisfying. The fact that Iâ€™m doing it on HEN makes a big difference to how I feel about it too: the environmental costs worry me a lot and donâ€™t think I could make myself use a site that relied on Ethereum. And to be honest, it really is nice to get paid for my art. Praise is nice, sure, but you canâ€™t live off that.\nI suppose the other thing Iâ€™m noticing already is that I feel a little less constrained on HEN. When I post art to twitter or instagram, itâ€™s always with the knowledge that the big social media websites are also places where my professional networks exist, and Iâ€™m obliged to behave, to be nice, to be the â€œgood girlâ€. I might swear and be grumpy on twitter sometimes, but for the most part I try not to let other parts of my personality and my life show on those sites. Thatâ€™s okay, and itâ€™s probably how it should be. Twitter is a place where itâ€™s okay to mix some parts of your personal life with some parts of your work life, but thereâ€™s a tacit understanding that you probably ought to keep some things carefully sequestered from the bird site. There are a lot of things about a personâ€™s life that their employer and professional colleagues may not want to know.\nWhere that runs into some difficulty, for me at least, is that a lot of my generative art is deeply entwined with my personal life, with my moods, and my experiences. When done well, art is always intimate, and the intimacy of creating and sharing the art often entails personal disclosures that might not be welcome on twitter. Consider these pieces, for example:\n\n\n\n\n\n\n\n\nOBJKT 341833\n\n\n\n\n\n\n\n\nOBJKT 341852\n\n\n\n\n\n\n\n\nOBJKT 341868\n\n\n\n\n\n\n\n\nOBJKT 341880\n\n\n\n\n\n\nI am very fond of these pieces, but they arenâ€™t the easiest ones to share on twitter. The title of the series is Bruises are how sadists kiss, and the pieces are tagged with â€œsadomasochismâ€ on my HEN profile. The title isnâ€™t deliberately intended to be provocative or anything of the sort. Thatâ€™s not really my preferred style. Itâ€™s much more prosaic: those things are part of my world and part of my life, and sometimes they show up in my art. The emotional experience expressed through the art (via the code) was one in which a very polite sadist had turned up in my life after a long absence. I was reminiscing, trying to work out what he meant to me, and I wrote the code while I was thinking about it. This was the system that emerged.\nOn twitter I would not dream of referring to those parts of my world so overtly (nor would I typically do so on this blog, focused as it is on technical topics). On HEN though, it feels a little more natural: art is often raw, it is often personal, and those subjects do come up if you spend a little time exploring the cryptoart space. It feels like a place where that version of me is permitted to have an online existence. As it turns out, thatâ€™s a thing that has some value to me.\n\n\nLast updated\n2021-10-17 13:58:48 AEDT\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-09-26_setting-up-on-hic-et-nunc/preview-image.jpg",
    "last_modified": "2021-10-17T13:58:49+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-14_tidy-tuesday-billboard/",
    "title": "Visualising the hits of Queen Britney",
    "description": "A gentle walkthrough of a few data wrangling and visualisation tools using the Billboard 100 data for this weeks Tidy Tuesday. Pitched at beginners looking to refresh their knowledge and maybe get some pointers at intermediate level tools",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-09-14",
    "categories": [],
    "contents": "\n\nContents\nFinding the data on GitHub\nGitHub repositories\nRepositories have branches\nRepositories are usually organised\nThe anatomy of the data link\n\nAttaching packages\nImporting the data\nFinding Britney\nPattern matching for text data\nCreating the Britney data\nFixing the dates\n\nVisualising a queen\n\n\n\n\nIâ€™ve never participated in Tidy Tuesday before, but because Iâ€™ve now joined a slack that does, it is high time I did something about that poor track record. I wasnâ€™t sure what I wanted to do with this weekâ€™s â€œBillboardâ€ data, other than I wanted it to have something to do with Britney Spears (because sheâ€™s awesome). After going back and forward for a while, I decided what Iâ€™d do is put together a couple of plots showing the chart performance of all her songs and â€“ more importantly â€“ write it up as a blog post in which I try to â€œover-explainâ€ all my choices. There are a lot of people in our slack who havenâ€™t used R very much, and I want to â€œunpackâ€ some of the bits and pieces that are involved. This post is pitched at beginners who are hoping for a little bit of extra scaffolding to explain some of the processesâ€¦\n\nThereâ€™s an R script containing the source code for this analysis here, and an R markdown version that mirrors this post almost exactly here\nFinding the data on GitHub\nEvery week the Tidy Tuesday data are posted online, and the first step in participating is generally to import the data. After a little bit of hunting online, you might discover that the link to the billboard data looks like this:\nhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/billboard.csv\nLetâ€™s start by unpacking this link. There is a lot of assumed knowledge buried here, and while it is entirely possible for you to get started without understanding it all, for most of us in the slack group the goal is to learn new data science skills. At some point you are probably going to want to learn the â€œversion controlâ€ magic. This post is not the place to learn this sorcery, but I am going to start foreshadowing some important concepts because they will be useful later.\nGitHub repositories\nThe place to start in understanding this link is the peculiar bit at the beginning: what is this â€œgithubâ€ nonsense? The long answer is very long, but the short answer is that https://github.com is a website that programmers use to store their code. GitHub is one of several sites (e.g., https://gitlab.org, https://bitbucket.com) that are all built on top of a version control system called â€œgitâ€. Git is a powerful tool that lets you collaborate with other people when writing code, allows you to keep track of the history of your code, and to backup your code online in case your laptop mysteriously catches on fire.\n\nIn the R community, â€œlaptop firesâ€ are universally understood to be a reference to what happens to you when you foolishly ignore the wise advice of Jenny Bryan\nGit is a complicated tool and it takes quite some time to get the hang of (Iâ€™m still learning, quite frankly), but it is worth your effort. When you have time, I recommend starting a free GitHub account. You can sign up using an email address, and if you have a university email address you get the educational discount (basically you get the â€œproâ€ version for free). My username on GitHub is djnavarro, and you can find my profile page here:\nhttps://github.com/djnavarro\nThe Tidy Tuesday project originated in the â€œR for data scienceâ€ learning community, and there is a profile page for that community too:\nhttps://github.com/rfordatascience\nR for data science is a wonderful book by Hadley Wickham and Garrett Grolemund\nOkay, so thatâ€™s part of the link explained. The next thing to understand is that when you create projects using git and post them to GitHub, they are organised in a â€œrepositoryâ€ (â€œrepoâ€ for short). Each repo has its own page. The Tidy Tuesday repo is here:\nhttps://github.com/rfordatascience/tidytuesday\nIf you click on this link, youâ€™ll find that thereâ€™s a nice description of the whole project, links to data sets, and a whole lot of other things besides.\nMost of the work organising this is done by Thomas Mock, and itâ€™s very very cool.\nRepositories have branches\nWhenever someone creates a git repository, it will automatically have at least one â€œbranchâ€ (usually called â€œmasterâ€ or â€œmainâ€). The idea behind it is really sensible: suppose youâ€™re working on a project and you think â€œooooh, I have a cool idea I want to try but maybe it wonâ€™t workâ€. What you can do is create a new â€œbranchâ€ and try out all your new ideas in the new branch all without ever affecting the master branch. Itâ€™s a safe way to explore: if your new idea works you can â€œmergeâ€ the changes into the master branch, but if it fails you can switch back to the master branch and pick up where you left off. No harm done. If you have lots of branches, you effectively have a â€œtreeâ€, and itâ€™s a suuuuuuper handy feature. Later on as you develop your data science skills youâ€™ll learn how to do this yourself, but for now this is enough information. The key thing is that what youâ€™re looking at when you visit the Tidy Tuesday page on GitHub is actually the master branch on the tree:\nhttps://github.com/rfordatascience/tidytuesday/tree/master\nRepositories are usually organised\nThe Tidy Tuesday repository has a lot of different content, and itâ€™s all nicely organised into folders (no different to the folders youâ€™d have on your own computer). One of the folders is called â€œdataâ€, and inside the â€œdataâ€ folder there is a â€œ2021â€ folder:\nhttps://github.com/rfordatascience/tidytuesday/tree/master/data/2021\nInside that folder you find lots more folders, one for every week this year. If you scroll down to the current week and click on the link, it will take you here:\nhttps://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-09-14\nBeing the kind soul that he is, Thomas has included a â€œreadmeâ€ file in this folder: itâ€™s a plain markdown file that gets displayed in a nice human readable format on the github page. Whenever youâ€™re doing a Tidy Tuesday analysis, itâ€™s super helpful to look at the readme file, because it will provide you a lot of the context you need to understand the data. Whenever doing your own projects, Iâ€™d strongly recommend creating readme files yourself: theyâ€™re reeeeaaaaaally helpful to anyone using your work, even if thatâ€™s just you several months later after youâ€™ve forgotten what you were doing. Over and over again when I pick up an old project I curse the me-from-six-months ago when she was lazy and didnâ€™t write one, or feel deeply grateful to her for taking the time to write one.\n\nReadme files are your best friend. Seriously\nIn any case, one of the things youâ€™ll see on that page is a link to the â€œbillboard.csvâ€ data. If you click on that link it will take you here:\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-09-14/billboard.csv\nNotice that this doesnâ€™t take you to the data file itself: it goes to a webpage! Specifically, it takes you to the â€œblobâ€ link that displays some information about the file (notice the â€œblobâ€ that has sneakily inserted itself into the link above?). In this case, the page wonâ€™t show you very much information at all because the csv file is 43.7MB in size and GitHub doesnâ€™t try to display files that big! However, what it does give you is a link that tells you where theyâ€™ve hidden the raw file! If you click on it (which I donâ€™t recommend), it will take you to the â€œrawâ€ file located atâ€¦\nhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/billboard.csv\nThis is the link that you might have discovered if youâ€™d been googling to find the Billboard data. Itâ€™s a GitHub link, but GitHub uses the â€œraw.githubusercontent.comâ€ site as the mechanism for making raw files accessible, which is why that part of the link has changed.\n\nI didnâ€™t intend for this section to be this long, honest\nThe anatomy of the data link\nAll of this tedious exposition should (I hope) help you make sense of what youâ€™re actually looking at when you see this link. In real life I would never bother to do this, but if you wanted to you could decompose the link into its parts. In the snippet below Iâ€™ll create separate variables in R, one for each component of the link:\n\n\nsite <- \"https://raw.githubusercontent.com\"\nuser <- \"rfordatascience\"\nrepo <- \"tidytuesday\"\nbranch <- \"master\"\nfolder1 <- \"data\"\nfolder2 <- \"2021\" \nfolder3 <- \"2021-09-14\"\nfile <- \"billboard.csv\"\n\n\n\nArgh. Wait. Thereâ€™s something slightly off-topic that I should point outâ€¦ one thing you might be wondering when you look at this snippet, is where that pretty â€œarrowâ€ character comes from. Donâ€™t be fooled. Itâ€™s not a special arrow character, itâ€™s two ordinary characters. What Iâ€™ve actually typed is <-, but this blog uses a fancypants font that contains a special ligature that makes <- appear to be a single smooth arrow. The font is called â€œFira Codeâ€, and a lot of programmers use it on their blogs. Once you know the trick, itâ€™s really nice because it does make the code a little easier to read, but it can be confusing if youâ€™re completely new to programming! Itâ€™s one of those little things that people forget to tell you about :-)\nAnyway, getting back on topic. The URL (a.k.a. â€œlinkâ€) for the Billboard data is what you get when you paste() all these components together, separated by the â€œ/â€ character:\n\n\ndata_url <- paste(\n  site, \n  user, \n  repo, \n  branch,\n  folder1, \n  folder2, \n  folder3, \n  file, \n  sep = \"/\"\n)\n\ndata_url\n\n\n[1] \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/billboard.csv\"\n\nExciting stuff.\nAttaching packages\nIâ€™m relatively certain that everyone in the slack has been exposed to the idea of an â€œR packageâ€. A package is a collection of R functions and data sets that donâ€™t automatically come bundled with R, but are freely available online. The tidyverse, for example, is a collection of R packages that a lot people find helpful for data analysis, and you can install all of them onto your machine (or your RStudio Cloud project) by using this command:\n\n\ninstall.packages(\"tidyverse\")\n\n\n\nThis can take quite a while to complete because there are a lot of packages that make up the tidyverse! Once the process is completed, you will now be able to use the tidyverse tools. However, itâ€™s important to recognise that just because youâ€™ve â€œinstalledâ€ the packages, it doesnâ€™t mean R will automatically use them. You have to be explicit. There are three tidyverse packages that Iâ€™m going to use a lot in this post (dplyr, stringr, and ggplot2), so Iâ€™ll use the library() function to â€œattachâ€ the packages (i.e.Â tell R to make them available):\n\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\n\n\n\nImporting the data\nAt this point we know where the data set is located, and we have some R tools that we can use to play around with it. The next step is reading the data into R. The readr package is part of the tidyverse, and it contains a useful function called read_csv() that can go online for you, retrive the billboard data, and load it into R. Thatâ€™s cool and all but if you look at the library() commands above, I didnâ€™t actually attach them. I didnâ€™t want to do this because honestly Iâ€™m only going to use the readr package once, and it feels a bit silly to attach the whole package. Instead, what Iâ€™ll do is use the â€œdouble colonâ€ notation :: to refer to the function more directly. When I write readr::read_csv() in R, what Iâ€™m doing is telling R to use the read_csv() function inside the readr package. As long as I have readr on my computer, this will work even if I havenâ€™t attached it using library(). The technical name for this is â€œnamespacingâ€, and if you hang around enough R programmers long enough thatâ€™s a word that will pop up from time to time. The way to think about it is that every package (e.g., readr) contains a collection of things, each of which has a name (e.g., â€œread_csvâ€ is the name of the read_csv() function). So you can think of a â€œspaceâ€ of these namesâ€¦ and hence the boring term â€œnamespaceâ€.\nOkay, letâ€™s use a â€œnamespacedâ€ command to import the data, and assign it to a variable (i.e., give the data a name). Iâ€™ll call the data billboard:\n\n\nbillboard <- readr::read_csv(data_url)\n\n\n\nThe billboard data is a nice, rectangular data set. Every row refers to a specific song on a specific date, and tells you its position in the charts on that date. We can type print(billboard) to take a look at the first few rows and columns. In most situations (not all), you can print something out just by typing its name:\n\n\nbillboard\n\n\n# A tibble: 327,895 Ã— 10\n   url      week_id  week_position song   performer song_id   instance\n   <chr>    <chr>            <dbl> <chr>  <chr>     <chr>        <dbl>\n 1 http://â€¦ 7/17/19â€¦            34 Don'tâ€¦ Patty Duâ€¦ Don't Juâ€¦        1\n 2 http://â€¦ 7/24/19â€¦            22 Don'tâ€¦ Patty Duâ€¦ Don't Juâ€¦        1\n 3 http://â€¦ 7/31/19â€¦            14 Don'tâ€¦ Patty Duâ€¦ Don't Juâ€¦        1\n 4 http://â€¦ 8/7/1965            10 Don'tâ€¦ Patty Duâ€¦ Don't Juâ€¦        1\n 5 http://â€¦ 8/14/19â€¦             8 Don'tâ€¦ Patty Duâ€¦ Don't Juâ€¦        1\n 6 http://â€¦ 8/21/19â€¦             8 Don'tâ€¦ Patty Duâ€¦ Don't Juâ€¦        1\n 7 http://â€¦ 8/28/19â€¦            14 Don'tâ€¦ Patty Duâ€¦ Don't Juâ€¦        1\n 8 http://â€¦ 9/4/1965            36 Don'tâ€¦ Patty Duâ€¦ Don't Juâ€¦        1\n 9 http://â€¦ 4/19/19â€¦            97 Don'tâ€¦ Teddy Peâ€¦ Don't Keâ€¦        1\n10 http://â€¦ 4/26/19â€¦            90 Don'tâ€¦ Teddy Peâ€¦ Don't Keâ€¦        1\n# â€¦ with 327,885 more rows, and 3 more variables:\n#   previous_week_position <dbl>, peak_position <dbl>,\n#   weeks_on_chart <dbl>\n\n\nFinally, some data!\nThis view helps you see the data in its â€œnativeâ€ orientation: each column is a variable, each row is an observation. Itâ€™s a bit frustrating though because a lot of the columns get chopped off in the printout. Itâ€™s often more useful to use dplyr::glimpse() to take a peek. When â€œglimpsingâ€ the data, R rotates the data on its side and shows you a list of all the variables, along with the first few entries for that variable:\n\n\nglimpse(billboard)\n\n\nRows: 327,895\nColumns: 10\n$ url                    <chr> \"http://www.billboard.com/charts/hot-â€¦\n$ week_id                <chr> \"7/17/1965\", \"7/24/1965\", \"7/31/1965\"â€¦\n$ week_position          <dbl> 34, 22, 14, 10, 8, 8, 14, 36, 97, 90,â€¦\n$ song                   <chr> \"Don't Just Stand There\", \"Don't Justâ€¦\n$ performer              <chr> \"Patty Duke\", \"Patty Duke\", \"Patty Duâ€¦\n$ song_id                <chr> \"Don't Just Stand TherePatty Duke\", \"â€¦\n$ instance               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦\n$ previous_week_position <dbl> 45, 34, 22, 14, 10, 8, 8, 14, NA, 97,â€¦\n$ peak_position          <dbl> 34, 22, 14, 10, 8, 8, 8, 8, 97, 90, 9â€¦\n$ weeks_on_chart         <dbl> 4, 5, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4,â€¦\n\nNotice that this time I just typed glimpse rather than dplyr::glimpse. I didnâ€™t need to tell R to look in the dplyr namespace because Iâ€™d already attached it when I typed library(dplyr) earlier.\nFinding Britney\nTime to start analysing the data. I have made a decision that today I have love in my heart only for Britney. So what I want to do is find the rows in billboard that correspond to Britney Spears songs. The natural way to do this would be to pull out the â€œperformerâ€ column and then try to find entries that refer to Britney. The slightly tricky aspect to this is that Britney doesnâ€™t appear solely as â€œBritney Spearsâ€. For example, â€œMe Against The Musicâ€ features Madonna, and the entry in the performer column is â€œBritney Spears Featuring Madonnaâ€. So weâ€™re going to have to search in a slightly smarter way. Before turning this into R code, I can sketch out my plan like this:\nget the billboard data, THEN\n  pull out the performer column, THEN\n  search for britney, THEN\n  tidy up a bit\nThis kind of workflow is naturally suited to the â€œpipeâ€, which is written %>%. Youâ€™ll see referred to either as the â€œmagrittr pipeâ€ (referring to the magrittr package where it originally came from) or the â€œdplyr pipeâ€ (because dplyr made it famous!). Iâ€™m sure youâ€™ve seen it before, but since one goal of this post is to be a refresher, Iâ€™ll explain it again. The pipe does the same job as the word â€œTHENâ€ in the pseudo-code I wrote above. Its job is to take the output of one function (whatever is on the left) and then pass it on as the input to the next one (on the right). So hereâ€™s that plan re-written in an â€œR-likeâ€ format:\nthe_billboard_data %>% \n  pull_out_the_performer_column() %>% \n  search_for_britney() %>% \n  tidy_it_up()\nIn fact thatâ€™s pretty close to what the actual R code is going to look like! The dplyr package has a function dplyr::pull() that will extract a column from the data set (e.g., all 327,895 listings in the performer column), and base R has a function called unique() that will ignore repeat entries, showing you only the unique elements of a column. So our code is going to look almost exactly like this\nbillboard %>% \n  pull(performer) %>% \n  search_for_britney() %>% \n  unique()\nPattern matching for text data\nInexcusably, however, R does not come with a search_for_britney() function, so weâ€™re going to have to do it manually. This is where the stringr package is very helpful. It contains a lot of functions that are very helpful in searching for text and manipulating text. The actual function Iâ€™m going to use here is stringr::str_subset() which will return the subset of values that â€œmatchâ€ a particular pattern. Hereâ€™s a very simple example, where the â€œpatternâ€ is just the letter â€œaâ€. Iâ€™ll quickly define a variable animals containing the names of a few different animals:\n\n\nanimals <- c(\"cat\", \"dog\", \"rat\", \"ant\", \"bug\")\n\n\n\nTo retain only those strings that contain the letter \"a\" we do this:\n\n\nstr_subset(string = animals, pattern = \"a\")\n\n\n[1] \"cat\" \"rat\" \"ant\"\n\nAlternatively we could write this using the pipe:\n\n\nanimals %>% \n  str_subset(pattern = \"a\")\n\n\n[1] \"cat\" \"rat\" \"ant\"\n\nIâ€™m not sure this second version is any nicer than the first version, but it can be helpful to see the two versions side by side in order to remind yourself of what the pipe actually does!\nWe can use the same tool to find all the Britney songs. In real life, whenever youâ€™re working with text data you need to be wary of the possibility of mispellings and other errors in the raw data. Wild caught data are often very messy, but thankfully for us the Tidy Tuesday data sets tend to be a little kinder. With that in mind can safely assume that any song by Britney Spears will include the pattern â€œBritneyâ€ in it somewhere.\nSo letâ€™s do just try this and see what we get:\n\n\nbillboard %>% \n  pull(performer) %>% \n  str_subset(\"Britney\") %>% \n  unique()\n\n\n[1] \"Britney Spears\"                              \n[2] \"Rihanna Featuring Britney Spears\"            \n[3] \"will.i.am & Britney Spears\"                  \n[4] \"Britney Spears & Iggy Azalea\"                \n[5] \"Britney Spears Featuring G-Eazy\"             \n[6] \"Britney Spears Featuring Madonna\"            \n[7] \"Britney Spears Featuring Tinashe\"            \n[8] \"Britney Spears Featuring Nicki Minaj & Ke$ha\"\n\n\nAt this point I was sorely tempted to get distracted by Ke$ha and Rihanna, but somehow managed to stay on topic. Somehow\nOkay, so it turns out that Britney is listed in eight different ways. For the sake of this post, Iâ€™m happy to include cases where another artist features on a Britney track, but I donâ€™t want to include the two cases where Britney is the featuring artist. Looking at the output above, it seems like I can find those cases by keeping only those rows that start with the word â€œBritneyâ€.\nNow our question becomes â€œhow do we write down a pattern like that?â€ and the answer usually involves crying for a bit because the solution is to use a regular expression, or â€œregexâ€.\nRegular expressions are a tool used a lot in programming: they provide a compact way to represent patterns in text. Theyâ€™re very flexible, but can often be quite hard to wrap your head around because there are a lot of special characters that have particular meanings. Thankfully, for our purposes today we only need to know one of them: the ^ character is used to mean â€œthe start of the stringâ€. So when interpreted as a regular expression, \"^Britney\" translates to â€œany string that begins with â€˜Britneyâ€™â€. Now that we have our regular expression, this works nicely:\n\n\nbillboard %>% \n  pull(performer) %>% \n  str_subset(\"^Britney\") %>% \n  unique()\n\n\n[1] \"Britney Spears\"                              \n[2] \"Britney Spears & Iggy Azalea\"                \n[3] \"Britney Spears Featuring G-Eazy\"             \n[4] \"Britney Spears Featuring Madonna\"            \n[5] \"Britney Spears Featuring Tinashe\"            \n[6] \"Britney Spears Featuring Nicki Minaj & Ke$ha\"\n\nRegular expressions are one of those things youâ€™ll slowly pick up as you go along, and although they can be a huuuuuuge headache to learn, the reward is worth the effort.\n\nIn my mental list of â€œstuff I hatelove in data scienceâ€, git and regexes are tied for first place\nCreating the Britney data\nOkay so now weâ€™re in a position to filter the billboard data, keeping only the rows that correspond to Britney songs. Most people in our slack group have taken an introductory class before, so youâ€™ll be expecting that dplyr::filter() is the tool we need. The kind of filtering youâ€™ve seen before looks like this:\n\n\nbritney <- billboard %>% \n  filter(performer == \"Britney Spears\")\n\n\n\nHowever, this doesnâ€™t work the way we want. The bit of code that reads performer == \"Britney Spears\" is a logical expression (i.e., a code snippet that only returns TRUE and FALSE values) that will only detect exact matches. Itâ€™s too literal for our purposes. We canâ€™t use the == operator to detect our regular expression either: that will only detect cases where the performer is literally listed as â€œ^Britneyâ€. What we actually want is something that works like the == test, but uses a regular expression to determine if itâ€™s a match or not.\nThatâ€™s where the str_detect() function from the stringr package is really handy. Instead of using performer == \"Britney Spears\" to detect exact matches, weâ€™ll use str_detect(performer, \"^Britney\") to match using the regular expression:\n\n\nbritney <- billboard %>% \n  filter(str_detect(performer, \"^Britney\"))\n\n\n\n\nA confession. I didnâ€™t technically need to use a regex here, because stringr has a handy str_starts() function. But half the point of our slack group is to accidentally-on-purpose reveal new tools and also I forgot that str_starts() exists soâ€¦ regex it is\nThis version works the way we want it to, but itâ€™s usually a good idea in practice to check that we havenâ€™t made any mistakes. Perhaps I have forgotten what str_detect() actually does or Iâ€™ve made an error in my use of filter(), for example. So letâ€™s take a look at the performer column in the britney data and check that it contains the same six unique strings:\n\n\nbritney %>% \n  pull(performer) %>% \n  unique()\n\n\n[1] \"Britney Spears\"                              \n[2] \"Britney Spears & Iggy Azalea\"                \n[3] \"Britney Spears Featuring G-Eazy\"             \n[4] \"Britney Spears Featuring Madonna\"            \n[5] \"Britney Spears Featuring Tinashe\"            \n[6] \"Britney Spears Featuring Nicki Minaj & Ke$ha\"\n\nThatâ€™s reassuring. So letâ€™s take a quick peek at the results of our data wrangling:\n\n\nglimpse(britney)\n\n\nRows: 468\nColumns: 10\n$ url                    <chr> \"http://www.billboard.com/charts/hot-â€¦\n$ week_id                <chr> \"4/22/2000\", \"10/24/2009\", \"12/20/200â€¦\n$ week_position          <dbl> 67, 1, 3, 70, 70, 21, 17, 29, 76, 1, â€¦\n$ song                   <chr> \"Oops!...I Did It Again\", \"3\", \"Circuâ€¦\n$ performer              <chr> \"Britney Spears\", \"Britney Spears\", \"â€¦\n$ song_id                <chr> \"Oops!...I Did It AgainBritney Spearsâ€¦\n$ instance               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦\n$ previous_week_position <dbl> NA, NA, NA, NA, NA, 45, NA, NA, NA, Nâ€¦\n$ peak_position          <dbl> 67, 1, 3, 70, 70, 21, 17, 29, 76, 1, â€¦\n$ weeks_on_chart         <dbl> 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 8, 1, 1â€¦\n\nThat looks good to meâ€¦\nFixing the dates\nâ€¦or does it? Looking at the week_id column is enough to make any data analyst sigh in mild irritation. This column encodes the date, but the first two entries are \"4/22/2000\" and \"10/24/2009\". They are encoded in a â€œmonth/day/yearâ€ format. Nobody on this planet except Americans writes dates this way. Most countries use â€œday/month/yearâ€ as their standard way of writing dates, and most programming style guides strongly recommend â€œyear/month/dayâ€ (there are good reasons for this, mostly to do with sorting chronologically). Worse yet, itâ€™s just a character string. R doesnâ€™t know that this column corresponds to a date, and unlike Excel it is smart enough not to try. Trying to guess what is and is not a date is notoriously difficult, so R makes that your job as the data analyst. Thankfully, the lubridate package exists to make it a little bit easier. In this case, where we have data in month/day/year format, the lubridate::mdy() function will do the conversion for us. Youâ€™ll be completely unsurprised to learn that there are lubridate::dmy() and lubridate::ymd() functions that handle other kinds of date formats.\nSo letâ€™s do this. Iâ€™ll use the dplyr::mutate() function to modify the britney data, like so:\n\n\nbritney <- britney %>% \n  mutate(week_id = lubridate::mdy(week_id))\n\nglimpse(britney)\n\n\nRows: 468\nColumns: 10\n$ url                    <chr> \"http://www.billboard.com/charts/hot-â€¦\n$ week_id                <date> 2000-04-22, 2009-10-24, 2008-12-20, â€¦\n$ week_position          <dbl> 67, 1, 3, 70, 70, 21, 17, 29, 76, 1, â€¦\n$ song                   <chr> \"Oops!...I Did It Again\", \"3\", \"Circuâ€¦\n$ performer              <chr> \"Britney Spears\", \"Britney Spears\", \"â€¦\n$ song_id                <chr> \"Oops!...I Did It AgainBritney Spearsâ€¦\n$ instance               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦\n$ previous_week_position <dbl> NA, NA, NA, NA, NA, 45, NA, NA, NA, Nâ€¦\n$ peak_position          <dbl> 67, 1, 3, 70, 70, 21, 17, 29, 76, 1, â€¦\n$ weeks_on_chart         <dbl> 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 8, 1, 1â€¦\n\nMuch better!\nVisualising a queen\nIâ€™m now at the point that I have a britney data set I can visualise. However, being the queen she is, Britney has quite a few songs that appear in the Billboard Top 100, so the first thing Iâ€™ll do is specify a few favourites that weâ€™ll highlight in the plots:\n\n\nhighlights <- c(\"Work B**ch!\", \"...Baby One More Time\", \"Toxic\")\n\n\n\nMost people in our slack will probably have encountered the ggplot2 package before, and at least have some experience in creating data visualisations using it. So we might write some code like this, which draws a plot showing the date on the horizontal axis (the mapping x = week_id) and the position of the song on the vertical axis (represented by the mapping y = week_position). Weâ€™ll also map the colour to the song by setting colour = song. Then weâ€™ll add some points and lines:\n\n\nggplot(\n  data = britney,\n  mapping = aes(\n    x = week_id,\n    y = week_position,\n    colour = song\n  )\n) + \n  geom_line(show.legend = FALSE) + \n  geom_point(show.legend = FALSE)\n\n\n\n\nThe reason Iâ€™ve included show.legend = FALSE here is that there are quite a few different songs in the data, and if they were all added to a legend it wouldnâ€™t leave any room for the data!\nWe can improve on this in a couple of ways. First up, letâ€™s use scale_y_reverse() to flip the y-axis. That way, a top ranked song appears at the top, and a 100th ranked song appears at the bottom:\n\n\nbritney %>% \n  ggplot(aes(\n    x = week_id, \n    y = week_position, \n    colour = song\n  )) + \n  geom_line(show.legend = FALSE) + \n  geom_point(show.legend = FALSE) + \n  scale_y_reverse()\n\n\n\n\nNotice that Iâ€™ve switched to using the pipe here. I take the britney data, pipe it with %>% to the ggplot() function where I set up the mapping, and then add things to the plot with +. Itâ€™s a matter of personal style though. Other people write their code differently!\nOkay, itâ€™s time to do something about the lack of labels. My real interest here is in the three songs I listed in the highlights so Iâ€™m going to use the gghighlight package, like this:\n\n\nbritney %>% \n  ggplot(aes(\n    x = week_id, \n    y = week_position, \n    colour = song\n  )) + \n  geom_line() + \n  geom_point() + \n  scale_y_reverse() + \n  gghighlight::gghighlight(song %in% highlights)\n\n\n\n\nWhen the data are plotted like this, you get a strong sense of the chronology of Britneyâ€™s career, but the downside is that you canâ€™t easily see how the chart performance of â€œâ€¦Baby One More Timeâ€ compares to the performance of â€œToxicâ€ and \"Work B**ch!\". To give a better sense of that, itâ€™s better to plot weeks_on_chart on the horizontal axis:\n\n\nbritney %>% \n  ggplot(aes(\n    x = weeks_on_chart, \n    y = week_position, \n    group = song,\n    colour = song\n  )) + \n  geom_line() + \n  geom_point() + \n  scale_y_reverse() + \n  gghighlight::gghighlight(song %in% highlights)\n\n\n\n\nShown this way, you get a really strong sense of just how much of an impact â€œâ€¦Baby One More Timeâ€ had. It wasnâ€™t just Britneyâ€™s first hit, it was also her biggest. Itâ€™s quite an outlier on the chart!\nIf weâ€™re doing exploratory data analysis, and the only goal is to have a picture to show a colleague, thatâ€™s good enough. However, if we wanted to share it more widely, youâ€™d probably want to spend a little more time fiddling with the details, adding text, colour and other things that actually matter a lot in real life!\n\n\nbritney %>% \n  ggplot(aes(\n    x = weeks_on_chart, \n    y = week_position, \n    group = song,\n    colour = song\n  )) + \n  geom_line(size = 1.5) + \n  scale_y_reverse() + \n  scale_color_brewer(palette = \"Dark2\") + \n  gghighlight::gghighlight(song %in% highlights, \n    unhighlighted_params = list(size = .5)) + \n  theme_minimal() +\n  labs(\n    title = \"Britney Spears' first hit was also her biggest\",\n    subtitle = \"Chart performance of Britney Spears' songs\",\n    x = \"Weeks in Billboard Top 100\",\n    y = \"Chart Position\"\n  )\n\n\n\n\nIf I were less lazy I would also make sure that the chart includes a reference to the original data source, and something that credits myself as the creator of the plot. Thatâ€™s generally good etiquette if youâ€™re planning on sharing the image on the interwebs. Thereâ€™s quite a lot you could do to tinker with the plot to get it to publication quality, but this is good enough for my goals today!\n\n\n\nFigure 1: Her Royal Highness Britney Spears, performing in Las Vegas, January 2014. Figure from wikimedia commons, released under a CC-BY-2.0 licence by Rhys Adams\n\n\n\n\n\nLast updated\n2021-09-17 16:58:36 AEST\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-09-14_tidy-tuesday-billboard/britney.jpg",
    "last_modified": "2021-09-30T15:08:53+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-07_water-colours/",
    "title": "Art, jasmines, and the water colours",
    "description": "An essay and tutorial covering a few useful art techniques in R",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-09-07",
    "categories": [],
    "contents": "\n\nContents\nPrelude\nThe water colours repository\nWhy use version control here?\nThe manifest file\nPreviewing the artwork\n\nDependencies\nArt from image processing\nFinding the image file\nImporting the image\nConverting the image to data\nArt from data visualisation\nExtracting the colour channels\nArt from channel manipulation\n\nIntermission\nArt from noise generators\nMultidimensional noise generation\nArt from the noise\nAccumulating art with purrr\n\nAssembling the parts\nAdding noise to jasmines coordinates\nJoining the noise with jasmine colours\nThe last chapter\n\nEpilogue\n\n\n\nPrelude\nIn recent weeks Iâ€™ve been posting generative art from the Water Colours series on twitter. The series has been popular, prompting requests that I sell prints, mint NFTs, or write a tutorial showing how they are made. For personal reasons I didnâ€™t want to commercialise this series. Instead, I chose to make the pieces freely available under a CC0 public domain licence and asked people to donate to a gofundme I set up for a charitable organisation I care about (the Louâ€™s Place womenâ€™s refuge here in Sydney). Iâ€™m not going to discuss the personal story behind this series, but it does matter. As Iâ€™ve mentioned previously, the art I make is inherently tied to moods. It is emotional in nature. In hindsight it is easy enough to describe how the system is implemented but this perspective is misleading. Although a clean and unemotional description of the code is useful for explanatory purposes, the actual process of creating the system is deeply tied to my life, my history, and my subjective experience. Those details are inextricably bound to the system. A friend described it better than I ever could:\n\nThe computer doesnâ€™t make this art any more than a camera makes a photograph; art is always intimate (Amy Patterson)\n\nIn this post Iâ€™ll describe the mechanistic processes involved in creating these pieces, but this is woefully inadequate as a description of the artistic process as a whole. The optical mechanics of a camera do not circumscribe the work of a skilled photographer. So it goes with generative art. The code describes the mechanics; it does not describe the art. There is a deeply personal story underneath these pieces (one that I wonâ€™t tell here), and I would no more mint an NFT from that story than I would sell a piece of my soul to a collector.\nThe water colours repository\nWhy use version control here?\nWhen I started making generative art I didnâ€™t think much about archiving my art or keeping it organised. I liked making pretty things, and that was as far as my thought process went. I didnâ€™t place the code under version control, and I stored everything in my Dropbox folder. Thereâ€™s nothing wrong with that: some things donâ€™t belong on GitHub. During the development phase of any art project thatâ€™s still what I do, and Iâ€™m perfectly happy with it.\nThings become a little trickier when you want to share the art. My art website is hosted on GitHub pages, and so my initial approach was to keep the art in the website repository. Huuuuge mistake. Sometimes the image files can be quite large and sometimes a series contains a large number of images. By the time Iâ€™d reached 40+ series, Hugo took a very long time to build the site (several minutes), and GitHub took even longer to deploy it (over half an hour).\nEventually I decided it made more sense to have one repository per series. Each one uses the â€œseries-â€ prefix to remind me itâ€™s an art repo. I donâ€™t use these repositories during development: they exist solely to snapshot the release. For example, the series-water-colours repository isnâ€™t going to be updated regularly, itâ€™s really just an archive combined with a â€œdocsâ€ folder that is used to host a minimal GitHub Pages site that makes the images public. Itâ€™s convenient for my purposes because my art website doesnâ€™t have to host any of the images: all it does is hotlink to the images that are exposed via the series repo.\nIt may seem surprising that Iâ€™ve used GitHub for this. Image files arenâ€™t exactly well suited to version control, but itâ€™s not like theyâ€™re going to be updated. Plus, there are a lot of advantages. I can explicitly include licencing information in the repository, I can release source code (when I want to), and I can include a readme file for anyone who wants to use it.\nThe manifest file\nOne nice feature of doing things this way is that it has encouraged me to include a manifest file. Because the image files belong to a completely different repository to the website, I need a way to automatically inspect the image repository and construct the links I need (because Iâ€™m waaaaaay too lazy to add the links by hand). Thatâ€™s the primary function of the manifest. The manifest.csv file is a plain csv file with one row per image, and one column for each piece of metadata I want to retain about the images. It might seem like organisational overkill to be this precise about the art, but Iâ€™m starting to realise that if I donâ€™t have a proper system in place Iâ€™ll forget minor details like â€œwhat the piece is calledâ€ or â€œwhen I made itâ€. That seems bad :-)\n\n\n\nI can use readr::read_csv() to download the manifest and do a little data wrangling to organise it into a format that is handy to me right now:\n\nThe data wrangling code is here\n\n\nmanifest\n\n\n# A tibble: 20 Ã— 9\n   series      sys_id img_id short_name  format long_name   date      \n   <chr>       <chr>  <chr>  <chr>       <chr>  <chr>       <date>    \n 1 watercolour sys02  img34  teacup-oceâ€¦ jpg    Ocean in aâ€¦ 2021-07-31\n 2 watercolour sys02  img31  incursions  jpg    Incursions  2021-08-14\n 3 watercolour sys02  img32  percolate   jpg    Percolate   2021-08-21\n 4 watercolour sys02  img37  gentle-desâ€¦ jpg    Gentle Desâ€¦ 2021-08-21\n 5 watercolour sys02  img41  stormy-seas jpg    Stormy Seas 2021-08-22\n 6 watercolour sys02  img42  turmeric    jpg    Turmeric Aâ€¦ 2021-08-24\n 7 watercolour sys02  img43  torn-and-fâ€¦ jpg    Torn and Fâ€¦ 2021-08-24\n 8 watercolour sys02  img47  inferno     jpg    Seventh Ciâ€¦ 2021-08-27\n 9 watercolour sys02  img48  storm-cell  jpg    Storm Cellâ€¦ 2021-08-27\n10 watercolour sys02  img49  tonal-earth jpg    Tonal Earth 2021-08-29\n11 watercolour sys02  img50  cold-front  jpg    Cold Front  2021-08-29\n12 watercolour sys02  img51  kintsugi-dâ€¦ jpg    Kintsugi Dâ€¦ 2021-08-29\n13 watercolour sys02  img53  departure   jpg    Departure   2021-08-29\n14 watercolour sys02  img54  echo        jpg    Echo        2021-08-30\n15 watercolour sys02  img57  portal      jpg    Portal      2021-08-31\n16 watercolour sys02  img60  salt-stoneâ€¦ jpg    Gods of Saâ€¦ 2021-08-31\n17 watercolour sys02  img61  amanecer-dâ€¦ jpg    El Ãšltimo â€¦ 2021-09-01\n18 watercolour sys02  img65  plume       jpg    Plume       2021-09-02\n19 watercolour sys02  img67  woodland-sâ€¦ jpg    Woodland Sâ€¦ 2021-09-02\n20 watercolour sys02  img68  below-the-â€¦ jpg    Below the â€¦ 2021-09-03\n# â€¦ with 2 more variables: path_2000 <chr>, path_500 <chr>\n\nPreviewing the artwork\nMore to the point, the manifest data frame is nicely suited for use with the bs4cards package, so I can display some of the pieces in a neat and tidy thumbnail grid. Here are the first eight pieces from the series, arranged by date of creation:\n\n\nmanifest[1:8, ] %>% \n  bs4cards::cards(\n    image = path_500,\n    link = path_2000,\n    title = long_name,\n    spacing = 3,\n    width = 2\n  )  \n\n\n\n\n\n\n\n\n\nOcean in a Teacup\n\n\n\n\n\n\n\n\nIncursions\n\n\n\n\n\n\n\n\nPercolate\n\n\n\n\n\n\n\n\nGentle Descent\n\n\n\n\n\n\n\n\nStormy Seas\n\n\n\n\n\n\n\n\nTurmeric Against Grey Tuesday\n\n\n\n\n\n\n\n\nTorn and Frayed\n\n\n\n\n\n\n\n\nSeventh Circle\n\n\n\n\n\n\nEach thumbnail image links to a medium resolution (2000 x 2000 pixels) jpg version of the corresponding piece, if youâ€™d like to see the images in a little more detail.\nDependencies\nIn the remainder of this post Iâ€™ll walk you through the process of creating pieces â€œin the style ofâ€ the water colours series. If you really want to, you can take a look at the actual source, but it may not be very helpful: the code is little opaque, poorly structured, and delegates a lot of the work to the halftoner and jasmines packages, neither of which is on CRAN. To make it a little easier on you, Iâ€™ll build a new system in this post that adopts the same core ideas.\nIn this post Iâ€™ll assume youâ€™re already familiar with data wrangling and visualisation with tidyverse tools. This is the subset of tidyverse packages that I have attached, and the code that follows relies on all these in some fashion:\n\n\nlibrary(magrittr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(dplyr)\n\n\n\n\nThe R environment is specified formally in the lockfile. Itâ€™s a story for another day, but for reproducibility purposes I have a separate renv configuration for every post\nIn addition to tidyverse and base R functions, Iâ€™ll use a few other packages as well. The magick, raster, rprojroot, fs, and ambient packages are all used in making the art. Because functions from those packages may not be as familiar to everyone, Iâ€™ll namespace the calls to them in the same way I did with bs4cards::cards() previously. Hopefully that will make it easier to see which functions belong to one of those packages.\nArt from image processing\nFinding the image file\nAs in life, the place to start is knowing where you are.\nThis post is part of my blog, and Iâ€™ll need to make use of an image file called \"jasmine.jpg\" stored alongside my R markdown. First, I can use rprojroot to find out where my blog is stored. Iâ€™ll do that by searching for a \"_site.yml\" file:\n\n\nblog <- rprojroot::find_root(\"_site.yml\")\nblog\n\n\n[1] \"/home/danielle/GitHub/sites/distill-blog\"\n\nI suspect that most people reading this would be more familiar with the here package that provides a simplified interface to rprojroot and will automatically detect the .Rproj or .here file associated with your project. In fact, because the here::here() function is so convenient, itâ€™s usually my preferred method for solving this problem. Sometimes, however, the additional flexibility provided by rprojroot is very useful. Some of my projects are comprised of partially independent sub-projects, each with a distinct root directory. That happens sometimes when blogging: there are contexts in which you might want to consider â€œthe blogâ€ to be the project, but other contexts in which â€œthe postâ€ might be the project. If youâ€™re not careful this can lead to chaos (e.g., RStudio projects nested inside other RStudio projects), and Iâ€™ve found rprojroot very helpful in avoiding ambiguity in these situations.\nHaving chosen â€œthe blogâ€ as the root folder, the next step in orientation is to find the post folder. Because this is a distill blog, all my posts are stored in the _posts folder, and Iâ€™ve adopted a consistent naming convention for organising the post folders. Every name begins with the post date in year-month-day format, followed by a human-readable â€œslugâ€:\n\n\npost <- paste(params$date, params$slug, sep = \"_\")\npost\n\n\n[1] \"2021-09-07_water-colours\"\n\nThis allows me to construct the path to the image file and display it here:\n\n\nfile <- fs::path(blog, \"_posts\", post, \"jasmine.jpg\")\nknitr::include_graphics(file)\n\n\n\n\n\nThe photo has an emotional resonance to me: it dates back to 2011 and appeared on the cover of Learning Statistics with R. Although 10 years separate the Water Colours series from the text and the photo, the two are linked by a shared connection to events from a decade ago\nImporting the image\nOur next step is to import the image into R at a suitable resolution. The original image size is 1000x600 pixels, which is a little more than we need. Hereâ€™s a simple import_image() function that does this:\n\n\nimport_image <- function(path, width, height) {\n  geometry <- paste0(width, \"x\", height) # e.g., \"100x60\"\n  path %>% \n    magick::image_read() %>% \n    magick::image_scale(geometry)\n}\n\n\n\nInternally, the work is being done by the fabulous magick package that provides bindings to the ImageMagick library. In truth, itâ€™s the ImageMagick library that is doing most the work here. R doesnâ€™t load the complete image, it lets ImageMagick take care of that. Generally thatâ€™s a good thing for performance reasons (you donâ€™t want to load large images into memory if you can avoid it), but in this case weâ€™re going to work with the raw image data inside R.\nThis brings us to the next taskâ€¦\nConverting the image to data\nConverting the image into a data structure we can use is a two step process. First, we create a matrix that represents the image in a format similar to the image itself. Thatâ€™s the job of the construct_matrix() function below. It takes the image as input, and first coerces it to a raster object and then to a regular matrix: in the code below, the matrix is named mat, and the pixel on the i-th row and j-th column of the image is represented by the contents of mat[i, j].\n\n\nconstruct_matrix <- function(image) {\n  \n  # read matrix\n  mat <- image %>% \n    as.raster() %>%\n    as.matrix()\n  \n  # use the row and column names to represent co-ordinates\n  rownames(mat) <- paste0(\"y\", nrow(mat):1) # <- flip y\n  colnames(mat) <- paste0(\"x\", 1:ncol(mat))\n  \n  return(mat)\n}\n\n\n\nA little care is needed when interpreting the rows of this matrix. When we think about graphs, the values on y-axis increase as we move our eyes upwards from the bottom, so our mental model has the small numbers at the bottom and the big numbers at the top. But thatâ€™s not the only mental model in play here. When we read a matrix or a table we donâ€™t look at it, we read it - and we read from top to bottom. A numbered list, for example, has the smallest numbers at the top, and the numbers get bigger as we read down the list. Both of those mental models are sensible, but itâ€™s hard to switch between them.\nThe tricky part here is that the raw image is encoded in â€œreading formatâ€. Itâ€™s supposed to be read like a table or a list, so the indices increase as we read down the image. The image data returned by construct_matrix() is organised this format. However, when we draw pictures with ggplot2 later on, weâ€™re going to need to switch to a â€œgraph formatâ€ convention with the small numbers at the bottom. Thatâ€™s the reason why the code above flips the order of the row names. Our next task will be to convert this (reading-formatted) matrix into a tidy tibble, and those row and column names will become become our (graph-formatted) x- and y-coordinates, so the row names need to be labelled in reverse order.\nTo transform the image matrix into a tidy tibble, Iâ€™ve written a handy construct_tibble() function:\n\n\nconstruct_tibble <- function(mat) {\n  \n  # convert to tibble\n  tbl <- mat %>%\n    as.data.frame() %>%\n    rownames_to_column(\"y\") %>%\n    as_tibble() \n  \n  # reshape\n  tbl <- tbl %>%\n    pivot_longer(\n      cols = starts_with(\"x\"),\n      names_to = \"x\",\n      values_to = \"shade\"\n    ) \n  \n  # tidy\n  tbl <- tbl %>%\n    arrange(x, y) %>% \n    mutate(\n      x = x %>% str_remove_all(\"x\") %>% as.numeric(),\n      y = y %>% str_remove_all(\"y\") %>% as.numeric(),\n      id = row_number()\n    )\n  \n  return(tbl)\n}\n\n\n\nThe code has the following strucure:\nThe first part of this code coerces the matrix to a plain data frame, then uses rownames_to_columns() to extract the row names before coercing it to a tibble. This step is necessary because tibbles donâ€™t have row names, and we need those row names: our end goal is to have a variable y to store those co-ordinate values.\nThe second part of the code uses pivot_longer() to capture all the other variables (currently named x1, x2, etc) and pull them down into a single column that specifies the x co-ordinate. At this stage, the tbl tibble contains three variables: an x value, a y value, and a shade that contains the hex code for a colour.\nThe last step is to tidy up the values. After pivot_longer() does its job, the x variable contains strings like \"x1\", \"x2\", etc, but weâ€™d prefer them to be actual numbers like 1, 2, etc. The same is true for the y variable. To fix this, the last part of the code does a tiny bit of string manipulation using str_remove_all() to get rid of the unwanted prefixes, and then coerces the result to a number.\n\nThe names_prefix argument to pivot_longer() can transform x without the third step, but I prefer the verbose form. I find it easier to read and it treats x and y the same\nTaken together, the import_image(), construct_matrix(), and construct_tibble() functions provide us with everything we need to pull the data from the image file and wrangle it into a format that ggplot2 is expecting:\n\n\njas <- file %>% \n  import_image(width = 100, height = 60) %>% \n  construct_matrix() %>% \n  construct_tibble()\n\njas\n\n\n# A tibble: 6,000 Ã— 4\n       y     x shade        id\n   <dbl> <dbl> <chr>     <int>\n 1     1     1 #838c70ff     1\n 2    10     1 #3c3123ff     2\n 3    11     1 #503d3dff     3\n 4    12     1 #363126ff     4\n 5    13     1 #443a30ff     5\n 6    14     1 #8a6860ff     6\n 7    15     1 #665859ff     7\n 8    16     1 #5a5d51ff     8\n 9    17     1 #535c4cff     9\n10    18     1 #944b61ff    10\n# â€¦ with 5,990 more rows\n\nA little unusually, the hex codes here are specified in RGBA format: the first two alphanumeric characters specify the hexadecimal code for the red level, the second two represent the green level (or â€œchannelâ€), the third two are the blue channel, and the last two are the opacity level (the alpha channel). Iâ€™m going to ignore the alpha channel for this exercise though.\nThereâ€™s one last thing to point out before turning to the fun art part. Notice that jas also contains an id column (added by the third part of the construct_tibble() function). Itâ€™s generally good practice to have an id column that uniquely identifies each row, and will turn out to be useful later when we need to join this data set with other data sets that weâ€™ll generate.\nArt from data visualisation\nLet the art begin!\nThe first step is to define a helper function ggplot_themed() that provides a template that weâ€™ll reuse in every plot. Mostly this involves preventing ggplot2 from doing things it wants to do. When weâ€™re doing data visualisation itâ€™s great that ggplot2 automatically provides things like â€œlegendsâ€, â€œaxesâ€, and â€œscalesâ€ to map from data to visual aesthetics, but from an artistic perspective theyâ€™re just clutter. I donâ€™t want to manually strip that out every time I make a plot, so it makes sense to have a function that gets rid of all those things:\n\n\nggplot_themed <- function(data) {\n  data %>% \n    ggplot(aes(x, y)) +\n    coord_equal() + \n    scale_size_identity() + \n    scale_colour_identity() + \n    scale_fill_identity() + \n    theme_void() \n}\n\n\n\nThis â€œtemplate functionâ€ allows us to start with a clean slate, and it makes our subsequent coding task easier. The x and y aesthetics are already specified, ggplot2 wonâ€™t try to â€œinterpretâ€ our colours and sizes for us, and it wonâ€™t mess with the aspect ratio. In a sense, this function turns off the autopilot: weâ€™re flying this thing manuallyâ€¦\nThere are many ways to plot the jas data in ggplot2. The least imaginative possibility is geom_tile(), which produces a pixellated version of the jasmines photo:\n\n\njas %>% \n  ggplot_themed() + \n  geom_tile(aes(fill = shade)) \n\n\n\n\nOf course, if you are like me you always forget to use the fill aesthetic. The muscle memory tells me to use the colour aesthetic, so I often end up drawing something where only the borders of the tiles are coloured:\n\n\njas %>% \n  ggplot_themed() + \n  geom_tile(aes(colour = shade)) \n\n\n\n\nItâ€™s surprisingly pretty, and a cute demonstration of how good the visual system is at reconstructing images from low-quality input: remarkably, the jasmines are still perceptible despite the fact that most of the plot area is black. I didnâ€™t end up pursuing this (yet!) but I think thereâ€™s a lot of artistic potential here. It might be worth playing with at a later date. In that sense generative art is a lot like any other kind of art (or, for that matter, science). It is as much about exploration and discovery as it is about technical prowess.\nThe path I did follow is based on geom_point(). Each pixel in the original image is plotted as a circular marker in the appropriate colour. Hereâ€™s the simplest version of this idea applied to the jas data:\n\n\njas %>% \n  ggplot_themed() + \n  geom_point(aes(colour = shade)) \n\n\n\n\nItâ€™s simple, but I like it.\nExtracting the colour channels\nUp to this point we havenâ€™t been manipulating the colours in any of the plots: the hex code in the shade variable is left intact. Thereâ€™s no inherent reason we should limit ourselves to such boring visualisations. All we need to do is extract the different â€œcolour channelsâ€ and start playing around.\nItâ€™s not too difficult to do this: base R provides the col2rgb() function that separates the hex code into red, green, blue channels, and represents each channel with integers between 0 and 255. It also provides the rgb2hsv() function that converts this RGB format into hue, saturation, and value format, represented as numeric values between 0 and 1.\nThis technique is illustrated by the extract_channels() helper function shown below. It looks at the shade column in the data frame, and adds six new columns, one for each channel. Iâ€™m a sucker for variable names that are all the same length (often unwisely), and Iâ€™ve named them red, grn, blu, hue, sat, and val:\n\n\nextract_channels <- function(tbl) {\n  rgb <- with(tbl, col2rgb(shade))\n  hsv <- rgb2hsv(rgb)\n  tbl <- tbl %>% \n    mutate(\n      red = rgb[1, ],\n      grn = rgb[2, ],\n      blu = rgb[3, ],\n      hue = hsv[1, ],\n      sat = hsv[2, ],\n      val = hsv[3, ]\n    )\n  return(tbl)\n}\n\n\n\nHereâ€™s what that looks like applied to the jas data:\n\n\njas <- extract_channels(jas)\njas\n\n\n# A tibble: 6,000 Ã— 10\n       y     x shade        id   red   grn   blu    hue   sat   val\n   <dbl> <dbl> <chr>     <int> <int> <int> <int>  <dbl> <dbl> <dbl>\n 1     1     1 #838c70ff     1   131   140   112 0.220  0.200 0.549\n 2    10     1 #3c3123ff     2    60    49    35 0.0933 0.417 0.235\n 3    11     1 #503d3dff     3    80    61    61 0      0.237 0.314\n 4    12     1 #363126ff     4    54    49    38 0.115  0.296 0.212\n 5    13     1 #443a30ff     5    68    58    48 0.0833 0.294 0.267\n 6    14     1 #8a6860ff     6   138   104    96 0.0317 0.304 0.541\n 7    15     1 #665859ff     7   102    88    89 0.988  0.137 0.4  \n 8    16     1 #5a5d51ff     8    90    93    81 0.208  0.129 0.365\n 9    17     1 #535c4cff     9    83    92    76 0.260  0.174 0.361\n10    18     1 #944b61ff    10   148    75    97 0.950  0.493 0.580\n# â€¦ with 5,990 more rows\n\nA whole new world of artistic possibilities has just emerged!\nArt from channel manipulation\nOne way to use this representation is in halftone images. If you have a printer that contains only black ink, you can approximate shades of grey by using the size of each dot to represent how dark that pixel should be:\n\n\nmap_size <- function(x) {\n  ambient::normalise(1-x, to = c(0, 2))\n}\n\njas %>% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(size = map_size(val)),\n    colour = \"black\", \n    show.legend = FALSE\n  )\n\n\n\n\n\nIn this code the ambient::normalise() function is used to rescale the input to fall within a specified range. Usually ggplot2 handles this automatically, but as I mentioned, weâ€™ve turned off the autopilotâ€¦\nFor real world printers, this approach is very convenient because it allows us to construct any shade we like using only a few different colours of ink. In the halftone world shades of grey are merely blacks of different size, pinks are merely sizes of red (sort of), and so on.\nBut weâ€™re not using real printers, and in any case the image above is not a very good example of a halftone format: Iâ€™m crudely mapping 1-val to the size aesthetic, and thatâ€™s not actually the right way to do this (if you want to see this done properly, look at the halftoner package). The image above is â€œinspired byâ€ the halftone concept, not the real thing. Iâ€™m okay with that, and abandoning the idea of fidelity opens up new possibilities. For example, thereâ€™s nothing stopping us retaining the original hue and saturation, while using dot size to represent the intensity value. That allows us to produce â€œhalftonesqueâ€ images like this:\n\n\njas %>% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(\n      colour = hsv(hue, sat, .5), \n      size = map_size(val)\n    ), \n    show.legend = FALSE\n  )\n\n\n\n\nIn this code, the hsv() function takes the hue and saturation channels from the original image, but combines them with a constant intensity value: the output is a new colour specified as a hex code that ggplot2 can display in the output. Because we have stripped out the value channel, we can reuse the halftone trick. Much like a halftone image, the image above uses the size aesthetic to represent the intensity at the corresponding pixel.\nIntermission\nUp to this point Iâ€™ve talked about image manipulation, and I hope you can see the artistic potential created when we pair image processing tools like magick with data visualisation tools like ggplot2. What I havenâ€™t talked about is how to choose (or generate!) the images to manipulate, and I havenâ€™t talked about how we might introduce a probabilistic component to the process. Iâ€™m not going to say much about how to choose images. The possibilities are endless. For this post Iâ€™ve used a photo I took in my garden many years ago, but the pieces in Water Colours series have a different origin: I dripped some food colouring into a glass of water and took some photos of the dye diffusing. Small sections were cropped out of these photos and often preprocessed in some fashion by changing the hue, saturation etc. These manipulated photos were then passed into a noise generation process, and the output produced images like this:\n\n\n\n\n\n\n\n\nStorm Cell / Air Elemental\n\n\n\n\n\n\n\n\nTonal Earth\n\n\n\n\n\n\n\n\nCold Front\n\n\n\n\n\n\n\n\nKintsugi Dreams\n\n\n\n\n\n\nArt from noise generators\nMultidimensional noise generation\nHow can we generate interesting noise patterns in R? As usual, there are many different ways you can do this, but my favourite method is to use the ambient package that provides bindings to the FastNoise C++ library. A proper description of what you can do with ambient is beyond what I can accomplish here. There are a lot of things you can do with a tool like this, and Iâ€™ve explored only a small subset of the possibilities in my art. Rather than make a long post even longer, what Iâ€™ll do is link to a lovely essay on flow fields and encourage you to play around yourself.\nTo give you a sense of what the possibilities are, Iâ€™ve written a field() function that uses the ambient package to generate noise. At its heart is ambient::gen_simplex(), a function that generates simplex noise (examples here), a useful form of multidimensional noise that has applications in computer graphics. In the code below, the simplex noise is then modified by a billow fractal that makes it â€œlumpierâ€: thatâ€™s the job of ambient::gen_billow() and ambient::fracture(). This is then modified one last time by the ambient::curl_noise() function to avoid some undesirable properties of the flow fields created by simplex noise.\nIn any case, here is the code. Youâ€™ll probably need to read through the ambient documentation to understand all the moving parts here, but for our purposes the main things to note are the arguments. The points argument takes a data frame or tibble that contains the x and y coordinates of a set of points (e.g., something like the jas data!). The frequency argument controls the overall â€œscaleâ€ of the noise: does it change quickly or slowly as you move across the image? The octaves argument controls the amount of fractal-ness (hush, I know thatâ€™s not a word) in the image. How many times do you apply the underlying transformation?\n\n\nfield <- function(points, frequency = .1, octaves = 1) {\n  ambient::curl_noise(\n    generator = ambient::fracture,\n    fractal = ambient::billow,\n    noise = ambient::gen_simplex,\n    x = points$x,\n    y = points$y,\n    frequency = frequency,\n    octaves = octaves,\n    seed = 1\n  )\n}\n\n\n\nInterpreting the output of the field() function requires a little care. The result isnâ€™t a new set of points. Rather, it is a collection of directional vectors that tell you â€œhow fastâ€ the x- and y-components are flowing at each of the locations specified in the points input. If we want to compute a new set of points (which is usually true), we need something like the shift() function below. It takes a set of points as input, computes the directional vectors at each of the locations, and then moves each point by a specified amount, using the flow vectors to work out how far to move and what direction to move. The result is a new data frame with the same columns and the same number of rows:\n\n\nshift <- function(points, amount, ...) {\n  vectors <- field(points, ...)\n  points <- points %>%\n    mutate(\n      x = x + vectors$x * amount,\n      y = y + vectors$y * amount,\n      time = time + 1,\n      id = id\n    )\n  return(points)\n}\n\n\n\nItâ€™s worth noting that the shift() function assumes that points contains an id column as well as the x and y columns. This will be crucial later when we want to merge the output with the jas data. Because the positions of each point are changing, the id column will be the method we use to join the two data sets. Itâ€™s also worth noting that shift() keeps track of time for you. It assumes that the input data contains a time column, and the output data contains the same column with every value incremented by one. In other words, it keeps the id constant so we know which point is referred to by the row, but modifies its position in time and space (x and y). Neat.\nArt from the noise\nTo illustrate how this all works, Iâ€™ll start by creating a regular 50x30 grid of points:\n\n\npoints_time0 <- expand_grid(x = 1:50, y = 1:30) %>% \n  mutate(time = 0, id = row_number())\n\nggplot_themed(points_time0) + \n  geom_point(size = .5)\n\n\n\n\nNext, Iâ€™ll apply the shift() function three times in succession, and bind the results into a single tibble that contains the the data at each point in time:\n\n\npoints_time1 <- shift(points_time0, amount = 1)\npoints_time2 <- shift(points_time1, amount = 1)\npoints_time3 <- shift(points_time2, amount = 1)\n\npts <- bind_rows(\n  points_time0, \n  points_time1, \n  points_time2,\n  points_time3\n)\n\n\n\nThen Iâ€™ll quickly write a couple of boring wrapper functions that will control how the size and transparency of the markers changes as a function of timeâ€¦\n\n\nmap_size <- function(x) {\n  ambient::normalise(x, to = c(0, 2))\n}\nmap_alpha <- function(x) {\n  ambient::normalise(-x, to = c(0, .5))\n}\n\n\n\nâ€¦and now we can create some art:\n\n\npts %>% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(\n      size = map_size(time), \n      alpha = map_alpha(time)\n    ),\n    show.legend = FALSE\n  )\n\n\n\n\nSo pretty!\nAccumulating art with purrr\nâ€¦ but also so ugly. The code I used above is awfully inelegant: Iâ€™ve â€œiterativelyâ€ created a sequence of data frames by writing the same line of code several times. Thatâ€™s almost never the right answer, especially when the code doesnâ€™t know in advance how many times we want to shift() the points! To fix this I could write a loop (and contrary to folklore, thereâ€™s nothing wrong with loops in R so long as youâ€™re careful to avoid unnecessary copying). However, Iâ€™ve become addicted to functional programming tools in the purrr package, so Iâ€™m going to use those rather than write a loop.\nTo solve my problem Iâ€™m going to use the purrr::accumulate() function, which I personally feel is an underappreciated gem in the functional programming toolkit. It does precisely the thing we want to do here: it takes one object (e.g., points) as input together with a second quantity (e.g., an amount), and uses the user-supplied function (e.g., shift()) to produce a new object that can, once again, be passed to the user-supplied function (yielding new points). It continues with this process, taking the output of the last iteration of shift() and using it as input to the next iteration, until it runs out of amount values. It is very similar to the better-known purrr::reduce() function, except that it doesnâ€™t throw away the intermediate values. The reduce() function is only interested in the destination; accumulate() is a whole journey.\nSo letâ€™s use it. The iterate() function below gives a convenient interface:\n\n\niterate <- function(pts, time, step, ...) {\n  bind_rows(accumulate(\n    .x = rep(step, time), \n    .f = shift, \n    .init = pts,\n    ...\n  ))\n}\n\n\n\nHereâ€™s the code to recreate the pts data from the previous section:\n\n\npts <- points_time0 %>% \n  iterate(time = 3, step = 1)\n\n\n\nIt produces the same image, but the code is nicer!\n\n\n\nAssembling the parts\nAdding noise to jasmines coordinates\nThe time has come to start assembling the pieces of the jigsaw puzzle, by applying the flow fields from the previous section to the data associated with the jasmines image. The first step in doing so is to write a small extract_points() function that will take a data frame (like jas) as input, extract the positional information (x and y) and the identifier column (id), and add a time column so that we can modify positions over time:\n\n\nextract_points <- function(data) {\n  data %>% \n    select(x, y, id) %>% \n    mutate(time = 0)\n}\n\n\n\nHereâ€™s how we can use this. The code below extracts the positional information from jas and then use the iterate() function to iteratively shift those positions along the paths traced out by a flow field:\n\n\npts <- jas %>% \n  extract_points() %>% \n  iterate(time = 20, step = .1)\n\n\n\nThe pts tibble doesnâ€™t contain any of the colour information from jas, but it does have the â€œright kindâ€ of positional information. Itâ€™s also rather pretty in its own right:\n\n\nmap_size <- function(x) {\n  ambient::normalise(x^2, to = c(0, 3.5))\n}\n\npts %>% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(size = map_size(time)),\n    alpha = .01,\n    show.legend = FALSE\n  ) \n\n\n\n\nJoining the noise with jasmine colours\nWe can now take the pixels from the jasmines image and make them â€œflowâ€ across the image. To do this, weâ€™ll need to reintroduce the colour information. We can do this using full_join() from the dplyr package. Iâ€™ve written a small convenience function restore_points() that performs the join only after removing the original x and y coordinates from the jas data. The reason for this is that the pts data now contains the positional information we need, so we want the x and y values from that data set. Thatâ€™s easy enough: we drop those coordinates with select() and then join the two tables using only the id column. See? I promised it would be useful!\n\n\nrestore_points <- function(jas, pts) {\n  jas %>% \n    select(-x, -y) %>% \n    full_join(pts, by = \"id\") %>% \n    arrange(time, id) \n}\n\n\n\nThe result is a tibble that looks like this:\n\n\njas <- restore_points(jas, pts)\njas\n\n\n# A tibble: 126,000 Ã— 11\n   shade     id   red   grn   blu    hue   sat   val     x     y  time\n   <chr>  <int> <int> <int> <int>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 #838câ€¦     1   131   140   112 0.220  0.200 0.549     1     1     0\n 2 #3c31â€¦     2    60    49    35 0.0933 0.417 0.235     1    10     0\n 3 #503dâ€¦     3    80    61    61 0      0.237 0.314     1    11     0\n 4 #3631â€¦     4    54    49    38 0.115  0.296 0.212     1    12     0\n 5 #443aâ€¦     5    68    58    48 0.0833 0.294 0.267     1    13     0\n 6 #8a68â€¦     6   138   104    96 0.0317 0.304 0.541     1    14     0\n 7 #6658â€¦     7   102    88    89 0.988  0.137 0.4       1    15     0\n 8 #5a5dâ€¦     8    90    93    81 0.208  0.129 0.365     1    16     0\n 9 #535câ€¦     9    83    92    76 0.260  0.174 0.361     1    17     0\n10 #944bâ€¦    10   148    75    97 0.950  0.493 0.580     1    18     0\n# â€¦ with 125,990 more rows\n\nMore importantly though, it produces images like this:\n\n\nmap_size <- function(x, y) {\n  ambient::normalise((1 - x) * y^2, to = c(0, 5))\n}\n\njas %>% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(\n      colour = hsv(hue, sat, .5), \n      size = map_size(val, time)\n    ), \n    alpha = .03,\n    show.legend = FALSE\n  )\n\n\n\n\nWhen colouring the image, weâ€™re using the same â€œhalftonesqueâ€ trick from earlier. The colours vary only in hue and saturation. The intensity values are mapped to the size aesthetic, much like we did earlier, but this time around the size aesthetic is a function of two variables: it depends on time as well as val. The way Iâ€™ve set it up here is to have the points get larger as time increases, but thereâ€™s no reason we have to do it that way. There are endless ways in which you could combine the positional, temporal, and shading data to create interesting generative art. This is only one example.\nThe last chapter\nAt last we have the tools we need to create images in a style similar (though not identical) to those produced by the Water Colours system. We can import, reorganise, and separate the data:\n\n\njas <- file %>% \n  import_image(width = 200, height = 120) %>% \n  construct_matrix() %>% \n  construct_tibble() %>% \n  extract_channels()\n\n\n\nWe can define flow fields with different properties, move the pixels through the fields, and rejoin the modified positions with the colour information\n\n\npts <- jas %>% \n  extract_points() %>% \n  iterate(\n    time = 40, \n    step = .2, \n    octaves = 10, \n    frequency = .05\n  )\n\njas <- jas %>%\n  restore_points(pts)\n\njas\n\n\n# A tibble: 984,000 Ã— 11\n   shade    id   red   grn   blu    hue    sat   val     x     y  time\n   <chr> <int> <int> <int> <int>  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 #9c8â€¦     1   156   129   120 0.0417 0.231  0.612     1     1     0\n 2 #81bâ€¦     2   129   181   100 0.274  0.448  0.710     1    10     0\n 3 #8b7â€¦     3   139   120   112 0.0494 0.194  0.545     1   100     0\n 4 #eedâ€¦     4   238   223   219 0.0351 0.0798 0.933     1   101     0\n 5 #c29â€¦     5   194   154   163 0.962  0.206  0.761     1   102     0\n 6 #d5eâ€¦     6   213   225   195 0.233  0.133  0.882     1   103     0\n 7 #bdeâ€¦     7   189   232   190 0.337  0.185  0.910     1   104     0\n 8 #b3dâ€¦     8   179   223   188 0.367  0.197  0.875     1   105     0\n 9 #b2dâ€¦     9   178   220   189 0.377  0.191  0.863     1   106     0\n10 #b3dâ€¦    10   179   217   191 0.386  0.175  0.851     1   107     0\n# â€¦ with 983,990 more rows\n\nWe can write customised helpers to guide how information is used:\n\n\nmap_size <- function(x, y) {\n  12 * (1 - x) * (max(y)^2 - y^2) / y^2\n}\n\n\n\nAnd we can render the images with ggplot2:\n\n\npic <- jas %>% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(\n      colour = shade, \n      size = map_size(val, time)\n    ), \n    alpha = 1,\n    stroke = 0,\n    show.legend = FALSE\n  ) \n\npic\n\n\n\n\nThe colour bleeding over the edges here is to be expected. Some of the points created with geom_point() are quite large, and they extend some distance beyond the boundaries of the original jasmines photograph. The result doesnâ€™t appeal to my artistic sensibilities, so Iâ€™ll adjust the scale limits in ggplot2 so that we donâ€™t get that strange border:\n\n\npic +\n  scale_x_continuous(limits = c(11, 190), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(7, 114), expand = c(0, 0))\n\n\n\n\nThe end result is something that has a qualitative similarity to the Water Colours pieces, but is also possessed of a style that is very much its own. This is as it should be. It may be true that â€œall art is theftâ€ â€“ as Picasso is often misquoted as saying â€“ but a good artistic theft is no mere replication. It can also be growth, change, and reconstruction.\nA happy ending after all.\nEpilogue\n\nI find it so amazing when people tell me that electronic music has no soul. You canâ€™t blame the computer. If thereâ€™s no soul in the music, itâ€™s because nobody put it there (BjÃ¶rk, via Tim de Sousa)\n\n\n\n\n\n\n\n\n\nDeparture\n\n\n\n\n\n\n\n\nEcho\n\n\n\n\n\n\n\n\nPortal\n\n\n\n\n\n\n\n\nGods of Salt, Stone, and Storm\n\n\n\n\n\n\n\n\nEl Ãšltimo Amanecer de Invierno\n\n\n\n\n\n\n\n\nPlume\n\n\n\n\n\n\n\n\nWoodland Spirits\n\n\n\n\n\n\n\n\nBelow the Horizon\n\n\n\n\n\n\n\n\nLast updated\n2021-09-17 16:55:04 AEST\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-09-07_water-colours/jasmine-recollected.png",
    "last_modified": "2021-09-30T15:08:53+10:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-08_git-credential-helpers/",
    "title": "Managing GitHub credentials from R, difficulty level linux",
    "description": "A sick sad story in which a humble R user was forced to learn something about\nhow linux stores passwords and, more importantly, got R to use her GitHub\ncredentials properly",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-08-08",
    "categories": [],
    "contents": "\n\nContents\nThe story is quite shortâ€¦\nUsing GitHub credentials with R\nSetting up the credentials\n\nâ€¦ unless youâ€™re on linux\nWhere did I leave my config?\nDonâ€™t forget to update git\n\nThree solutions\n1. Set a long timeout for the git cache\n2. Use libsecret credential manager\n3. Use GCM core\n\n\n\n\nThere are days when I regret switching to linux as an R user. Itâ€™s not that Iâ€™m particularly enamoured of Apple or Microsoft, and I do enjoy the freedom to tinker that linux systems provide, but without the same resourcing that underpins Windows or Mac OS, I do spent a disproportionate amount my time trying to make my long-suffering Ubuntu laptop do something that would â€œjust workâ€ if Iâ€™d gone with one of the more traditional options. But such is life, and besides, thereâ€™s a case to be made that the time I spend on these things is not wasted: usually, I end up learning something useful.\n La la la la la. (Figure from giphy.com)\nThis is one of those stories.\nThe story is quite shortâ€¦\nUsing GitHub credentials with R\nFor some years now I have been using git repositories for version control, with some ambivalence to my feelings. I absolutely love version control, and I think GitHub is a fabulous tool, but git itself gives me headaches. It feels counterintuitive and untidy, and I am resistant to learning new git tricks because of that. However, now that GitHub is moving to end password authentication for git operations, I find myself needing to do precisely that. Sigh.\nLike many R users, whenever I encounter a git problem my first impulse is to see whether Happy Git and GitHub for the useR (Bryan 2018) can help me out, and true to form, it can. Having decided that I will revert to being an https girl, renouncing my flirtation with ssh, Iâ€™ve found the chapter on caching https credentials extremely useful. The usethis article on git credentials is also worth the read.\nThe problem can be broken into three parts:\nHow do I set up an authentication token on my GitHub account?\nHow do I configure my git installation to use the authentication token?\nHow do I ensure that R detects these credentials?\nThanks to the fabulous work of the tidyverse team, itâ€™s possible for R users to solve the problem in a fairly painless way. The solution has been documented repeatedly, but for the sake of completeness Iâ€™ll repeat the advice here.\nSetting up the credentials\nThe first thing youâ€™ll need to do is set up a GitHub token. You can do this on the GitHub website, but for an R user itâ€™s probably easiest to use the usethis package (Wickham and Bryan 2021):\n\n\nusethis::create_github_token()\n\n\n\nThis will open GitHub in a browser window, take you to the â€œcreate a new token page,â€ and pre-populate all the fields with sensible default values. After accepting these values, the token is created and youâ€™ll be given a PAT, a â€œpersonal authentication token.â€ Itâ€™ll look something like thisâ€¦\nghp_dgdfasdklfjsdklfjsadfDKFJASDLKFJ3453\nâ€¦and you should immediately save this in a secure password manager, like 1password, lastpass, etc, because GitHub will only show it to you this one time. You did save it to your password manager, right? Right? I mean, you might need it again. You really might. Yes, you. All right then. Iâ€™ll trust youâ€™ve taken sensible precautions now, so letâ€™s keep going. The next step in the process is to configure your git installation to use your token. This is, once again, quite easy to do with gitcreds (CsÃ¡rdi 2020):\n\n\ngitcreds::gitcreds_set()\n\n\n\nWhen you call this function interactively, R will ask for your PAT. Paste it into the console, hit enter, and you are done. Your git installation is now configured to use the token. Yay! Letâ€™s move onto the third step, which is to ensure that R will recognise and use these credentials. As it turns out, step three doesnâ€™t require you to do anything, because it happens automatically! Functions like usethis::pr_push() recognise your credentials as soon as gitcreds sets them up, and everything works perfectlyâ€¦\n Quinn. (Figure from giphy.com)\nâ€¦ unless youâ€™re on linux\nIf youâ€™re on linux, you might find yourself in the same boat I was. The credentials you just set up work flawlessly for about 15 minutes, at which time R complains that it cannot find any credentials and you spend the next 15 minutes crying melodramatically.\nWhen this happened to me I assumed the problem was my R environment. I tried updating gitcreds, usethis, and every other R package I could think of that might possibly be involved in communicating with git. Nothing worked. The reason nothing worked is that the problem wasnâ€™t with R at allâ€¦ it was git, and in hindsight I realise that the problem is specific to git on linux. All those beautiful people with their fancy Windows and Mac machines wonâ€™t run into the problem I encountered. They wonâ€™t spend an entire Saturday trying to teach themselves git credential management. They will never know my pain. Curse them and their superior purchasing decisions.\n Daria. (Figure from giphy.com)\nJust kidding. I love my quirky little Ubuntu box and I have a lot of fun learning how to fix her up every time she sets herself on fire.\nWhere did I leave my config?\nOkay, Iâ€™m going to need to make changes to my git configuration. Although git makes it possible to store configuration locally, at the repository level, I rarely need this flexibility. The relevant information is stored in the global configuration file: on my machine, this is located at /home/danielle/.gitconfig. I can use git config to list these configuration settings, like this\n\ngit config --global --list\n\nand at the start of this exercise the output would have looked like this:\nuser.name=Danielle Navarro\nuser.email=d.navarro@unsw.edu.au\nIâ€™m not sure why this is, but I always feel slightly more reassured when Iâ€™m able to inspect the configuration file itself. Opening my .gitconfig file shows the same information, but the formatting is slightly different in the raw file:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\nTo solve the git credential problem, weâ€™re going to need to edit this configuration information. Depending on which solution you go with, you might need to install new software too.\nDonâ€™t forget to update git\nBefore starting, itâ€™s a good idea to make sure you have the latest version of git: older versions may not have the tools you need. As it happens, I had already updated git to the most recent version (2.32.0 at the time of writing), but in case anyone ends up relying on this post, hereâ€™s how you do it:\nsudo add-apt-repository ppa:git-core/ppa\nsudo apt update\nsudo apt install git\nThree solutions\n1. Set a long timeout for the git cache\nRecent versions of git are released with a credential cache that retains your credentials in memory temporarily. The information is never written to disk, and it expires after a time. You can tell git to use this cache as your â€œcredential helperâ€ by typing the following command at the terminal:\n\ngit config --global credential.helper cache\n\nAfter doing this, my .gitconfig file now looks like this:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = cache\nUnfortunately this isnâ€™t an ideal solution, because the cache expires after 900 seconds (15 minutes). As soon as the cache expires, git loses track of your GitHub credentials and so does R. So you have to set the credentials again by calling gitcreds::gitcreds_set() and entering the PAT again. Thatâ€™s annoying, but you did store the PAT in a password manager right? You were smart. You definitely arenâ€™t going to be foolish like me, forget to store your PAT every time, and end up needing to create a new GitHub token every 15 minutes.\nA simple solution to this problem is to ask git to store information in the cache for just a teeny tiny little bit longer. Instead of having the cache expire after the default 900 seconds, maybe set it to expire after 10 million seconds. That way, youâ€™ll only have to refresh the cache using gitcreds::gitcreds_set() once every four months instead of four times an hour. Implementing this solution requires only one line of code at the terminal:\n\ngit config --global credential.helper 'cache --timeout=10000000'\n\nAfter typing this, my .gitconfig file looks like this:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = cache --timeout=10000000\nIn some ways this is a bit of a hack. If cache expiry normally happens every 15 minutes, thereâ€™s something a little odd about dragging it out and making it hang around for 16 weeks. That being said, Iâ€™ve done many stranger things than this in my life. It may not be the most elegant way to solve the problem, but it works.\n Trent. (Figure from giphy.com)\n2. Use libsecret credential manager\nIt puzzled me slightly that this problem only exists for linux computers, so I did a little more reading on how git manages credentials. It turns out you donâ€™t have to rely on the in-memory cache: you can tell git to use some other program to supply the credentials. This is what all those swanky Mac and Windows people have been doing all along. On Macs, for example, git defaults to using the OS X keychain to store credentials safely on disk. Itâ€™s possible to do the same thing on linux using libsecret (source on gitlab) and thankfully itâ€™s not much harder to set this up than to use the â€œlong cacheâ€ trick described in the previous section.\nThe first step is ensuring libsecret is installed on your machine. It probably is (or at least, it was on my Ubuntu 20.04 box), but in case it isnâ€™t hereâ€™s the command you need\n\nsudo apt install libsecret-1-0 libsecret-1-dev\n\nIt helps to realise that libsecret isnâ€™t an application designed to work with git (i.e., itâ€™s not the credential manager), nor is it the keyring where the passwords are stored. Rather, itâ€™s a library that communicates with the keyring: I found this post useful for making sense of it. So if we want to use libsecret to access the keyring, weâ€™re going to need a git credential manager that knows how to talk to libsecret. As it turns out, git comes with one already, you just have to build it using make:\n\ncd /usr/share/doc/git/contrib/credential/libsecret\nsudo make\n\nThis will build the git-credential-libsecret application for you and now all you have to do is tell git to use this as the â€œcredential helperâ€ application that supplies the GitHub credentials:\n\ngit config --global credential.helper \\\n  /usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret\n\nAfter typing that, my .gitconfig file looks like thisâ€¦\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = /usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret\nâ€¦ and Iâ€™m all set and ready to go.\nOne thing I found handy during this step is to check that R was reading the correct configuration information. Itâ€™s possible to do this with gitcreds:\n\n\ngitcreds::gitcreds_list_helpers()\n\n\n\n\n[1] \"/usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret\"\n\nIn any case, if all the applications are talking to each other properly, the next time you call gitcreds::gitcreds_set() theyâ€™ll all send the message along: R will pass your PAT to git, git will pass it to git-credential-libsecret, git-credential-libsecret will pass it to libsecret, and the PAT will end up in your linux keychain. Whenever you need to authenticate and push some commits up to GitHub from R, it should find the credentials using the same communication channel. Everything should work swimmingly.\n Quinn et al.Â (Figure from giphy.com)\n3. Use GCM core\nAs far as I can tell, the libsecret credential manager is a perfectly good solution to the problem, but in the end I made a different choice: I decided to go with â€œgit credential manager core,â€ or GCM Core. Itâ€™s developed by Microsoft and, perhaps unsurprisingly, it is what GitHub currently recommends. Itâ€™s slightly more painful to set up, and the installation instructions are different depending on what flavour of linux youâ€™re running. Because Iâ€™m on Ubuntu 20.04, I downloaded the .deb file associated with the most recent release of GCM core, and then installed the application using the dpkg command:\n\nsudo dpkg -i <path-to-deb-file>\n\nThis will build GCM core on your system, and once thatâ€™s done you can ask it to take care of the git configuration for you:\n\ngit-credential-manager-core configure\n\nThis will edit the .gitconfig file, so for me it now looks like this:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = \n    helper = /usr/bin/git-credential-manager-core\n[credential \"https://dev.azure.com\"]\n    useHttpPath = true\nIn a happier world you would be done at this point, but we donâ€™t live in a happy world. We live in a sick sad world that has global pandemics and pineapple on pizzas. So thereâ€™s still one job left to do.\nMuch like the libsecret credential manager I built in the previous section, GCM core is â€œjustâ€ a git credential manager: it communicates with git, but it isnâ€™t a password manager or a keyring, and it doesnâ€™t store the PAT itself. Instead, it offers you several different options for how the PAT is to be stored. If you click through and take a look at the list, the first suggested option is to connect to a secret service API. As far as I can tell â€œsecret serviceâ€ isnâ€™t an application, itâ€™s a specification, and in practice itâ€™s just a fancy way of referring to a linux keychain. Just as the libsecret credential manager needs some way of communicating with the keychain (i.e., the libsecret library itself), GCM core needs an intermediary. In fact, it turns out GCM core also uses libsecret to talk to the keychain. So thatâ€™s the option I went with. The terminal command to set this up is this:\n\ngit config --global credential.credentialStore secretservice\n\nAfter running the command, my .gitconfig file looks like this:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = \n    helper = /usr/bin/git-credential-manager-core\n    credentialStore = secretservice\n[credential \"https://dev.azure.com\"]\n    useHttpPath = true\n Jane. (Figure from giphy.com)\nAs before, I can check that R is reading the correct configuration informationâ€¦\n\n\ngitcreds::gitcreds_list_helpers()\n\n\n[1] \"/usr/bin/git-credential-manager-core\"\n\nâ€¦and now Iâ€™m ready to go. My problems are solved. The sun is shining, the birds are singing, and git is working properly from R again. All is well in heaven and earth. Oh the sheer excitement of it all. I hope I can contain my boundless enthusiasm and joy.\n Daria. (Figure from giphy.com)\n\nLast updated\n2021-09-17 17:00:16 AEST\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\nBryan, Jennifer. 2018. Happy Git and GitHub for the useR. GitHub.\n\n\nCsÃ¡rdi, GÃ¡bor. 2020. Gitcreds: Query â€™Gitâ€™ Credentials from â€™râ€™. https://CRAN.R-project.org/package=gitcreds.\n\n\nWickham, Hadley, and Jennifer Bryan. 2021. Usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\n\n\n",
    "preview": "posts/2021-08-08_git-credential-helpers/credentials.jpg",
    "last_modified": "2021-09-30T15:08:53+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-08_generative-art-in-r/",
    "title": "Generative art in R",
    "description": "Comments on an exhibit I contributed to as part of useR!2021",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-07-08",
    "categories": [],
    "contents": "\n\n\nA little while ago I was invited by Sara Mortara to contribute art as part of an exhibit to be presented at the 2021 useR! conference, along with several artists who I admire greatly. I could hardly say no to that, now could I? So I sent some pieces that Iâ€™m fond of, most of which are posted somewhere on my art website. I realised later though that I was going to have to talk a little about my art too, and Sara suggested an informal Q&A during the timeslot allocated to the exhibit. Naturally, I agreed since that meant I didnâ€™t have to prepare anything formal, and like all artists I am extremely lazy. Later though, it occurred to me that it actually wouldnâ€™t be terrible if I wrote a blog post to accompany my contribution to the exhibit, loosely based on the questions Sara suggested. And so here we areâ€¦\nWhen did you start using R for art? Do you remember your first piece?\nI started making art in R some time in late 2019. Iâ€™d discovered some of the art that Thomas Lin Pedersen had been making â€“ at the time he was posting pieces from his Genesis series â€“ and at the same time I found the ambient package that he was using to create the pieces. Thomas famously does not post source code for his art, and being stubborn and curious I wanted to work out how he was doing it, so I started playing with ambient to see if I could reverse engineer his system. My very first piece was Constellations, shown below. Itâ€™s certainly not the prettiest thing Iâ€™ve created, and there are a lot of things Iâ€™d like to change about it now, but itâ€™s nice to have your early work lying around to see how youâ€™ve changed since then:\n\nConstellations\n\nIf you follow the link above and look at Thomasâ€™ Genesis pieces you can tell that itâ€™s not even remotely close to the mark, but I did eventually get the hang of it and managed to produce a few pieces like Rainbow Prisms which are closer to the kind of work he was producing:\n\nRainbow Prisms\n\nItâ€™s still not quite the same as Thomasâ€™ in style, but by the time Iâ€™d worked out how to produce these I decided it was time to change my approach and branch out a bit. I love Thomasâ€™ work of course, but I didnâ€™t want my art to be just a low quality imitation of his! And besides, by that point Iâ€™d started discovering a whole lot of other people making generative art in R, such as Will Chase, Antonio SÃ¡nchez ChinchÃ³n, Marcus Volz, and (somewhat later) Ijeamaka Anyene. Each has their own style and â€“ following the famous advice that art is theft â€“ have shamelessly taken ideas and inspiration from each at different times.\nSome of those early pieces are still around, as part of the Rosemary gallery.\nWere you an artist before making generative art in R?\nNot really. I always wanted to do more artistic and creative things, but the only thing Iâ€™d ever done that required any kind of mix of aesthetic sensibility and craftwork was gardening. I used to have a lovely garden in Adelaide with a mix of Mediterranean and Australian native plants, and I had the same kind of enthusiasm for gardening then as I do for art now. Maybe one day Iâ€™ll garden again but thereâ€™s no space for that in my Sydney apartment!\nCan you talk about your creative process? Do you begin from code or from the outcome you are looking for? Do you start with the color palette in mind, or is it an iterative process?\nIâ€™m honestly not sure I have a consistent process? I spend a lot of time browsing artwork by other people on twitter and instagram, and from time to time I read posts about the techniques that they use. Whenever I do this I end up thinking a bit about how I might use this technique or wondering what methods other artists use to create their work, but I donâ€™t usually act on that information until I think of something I want to do with it. That kind of technical or stylistic information is like background knowledge that lies dormant until I need it.\nMost of the time the starting point for my art is an emotion. I might be angry or lonely or tired, or just in need of something to occupy my mind and distract me from something else. When I start implementing a new system itâ€™s often (though not always) a modification of a previous one. In principle this modification process could go in any direction, but my aesthetic sensibilities depend a lot on my state of mind, and that imposes a bias. I tweak the code one way, and see what it produces. If I like it, I keep the change, if I donâ€™t I reject it. Itâ€™s a lot like a Metropolis-Hastings sampler that way, but my mood strongly shapes the accept/reject decision, so the same starting point can lead to different outcomes. As a concrete example, the Pollen, Bursts and Embers series are all based on the same underlying engine, the fractal flame algorithm created by Scott Draves, but my emotional state was very different at the time I coded each version. For example, the Pollen Cloud piece I contributed to the useR exhibit is soft and gentle largely because I was feeling peaceful and relaxed at the time:\n\nPollen Cloud\n\nBy way of contrast, the Geometry in a Hurricane piece from Bursts is layered in jagged textures with a chaotic energy because I was angry at the time I was coding:\n\nGeometry in a Hurricane\n\nThe Soft Ember piece below (also included in the exhibit) has a different feel again. Thereâ€™s more energy to it than the pollen pieces, but itâ€™s not as chaotic as the bursts series. Again, thatâ€™s very much a reflection of my mood. I wasnâ€™t angry when I coded this system, but I wasnâ€™t relaxed either. At the time, something exciting had happened in my life that I wasnâ€™t quite able to do anything about, but I was indulging in the anticipation of a new thing, and some of that emotion ended up showing through in the pieces that I made at the time:\n\nSoft Ember\n\nTo bring all this back to the question, itâ€™s very much an iterative process. The driver behind the process is usually an emotion, and the colour choices, the shapes, and the code are all adapted on the fly to meet with how Iâ€™m feeling.\nWhat is your inspiration?\nTo the extent that my art is driven by emotion, the inspiration for it tends to be tied to sources of strong emotion in my life. Sometimes that emotion comes from the sources of love and joy: family, intimate partners, and so on. The Heartbleed series is one of those. The background texture to these images is generated by simulating a simple Turing machine known as a turmite and the swirly hearts in the foreground are generated using the toolkit provided by the ambient package. This system is very much motivated from emotional responses to the loved ones in my life. One of the pieces in the exhibit is from this series:\n\nTurmite 59 in Red\n\nOther times the emotional motivation comes from sources of pain - sometimes things that were physically painful, sometimes that were psychologically painful. The Orchid Thorn piece I included in the exhibit is one of those, linked to an intense physically painful experience.\n\nOrchid Thorn\n\nThe Bitterness piece below, which I havenâ€™t done much with other than post to my instagram, is strongly tied to the psychological stresses associated with my gender transition. Yes, thereâ€™s a softness to the piece, but thereâ€™s also a sandpaper-like texture there that makes me think of abrasion. The colour shifts make me think about transitions, but the roughness at some of the boundaries reminds me that change is often painful.\n\nBitterness\n\nOne odd property of the art, at least from my point of view, is that looking at a given piece recalls to mind the events and emotions that inspired the work, and to some extent that recollection becomes a way of re-experiencing the events. Sometimes thatâ€™s a good thing. Not always though.\nWhat is your advice for people who wants to create art in R?\nI think Iâ€™d suggest three things. Find artists you like, read about their processes. Sometimes theyâ€™ll show source code or link to algorithms like Iâ€™ve done in a few places in this piece, and it can be really valuable to try to retrace their steps. Thereâ€™s nothing wrong with learning technique by initially copying other artists and then developing your own style as you go.\nThe second thing Iâ€™d suggest, for R folks specifically, is to take advantage of the skills you already have. Most of us have skills in simulation, data wrangling, and data visualisation, and those skills can be repurposed for artistic work quite easily. A lot of my pieces are created using that specific combination. Iâ€™ll often define a stochastic process and sample data from it using tools in base R, use dplyr to transform and manipulate it, then use ggplot2 to map the data structure onto a visualisation. One of the nice things about dplyr and ggplot2 being compositional grammars is the fact that you can â€œreuseâ€ their parts for different purposes. I get a lot of artistic mileage out of geom_point() and geom_polygon(), and quite frankly purrr is an absolute godsend when the generative process youâ€™re working with is iterative in nature.\nThe other thing would be try not to put pressure on yourself to be good at it immediately. I wasnâ€™t, and I donâ€™t think anyone else was either. Earlier I showed the Constellations piece and referred to it as the first piece I created. In a way thatâ€™s true, because it was the first time I reached a level that I felt comfortable showing to other people. But I made a lot of junk before that, and I made a lot of junk after that. I make some good art now (or so people tell me) precisely because I made a lot of bad art before. Even now, though, I canâ€™t tell which systems will end up good and which will end up bad. Itâ€™s a bit of a lottery, and Iâ€™m trying my best not to worry too much about how the lottery works. I like to have fun playing with visual tools, and sometimes the tinkering takes me interesting places.\nAnything to add about your pieces in the exhibit?\nNot a lot. Several of the pieces Iâ€™ve contributed are already linked above, but I might just say a little about the other pieces and how they were made. The Silhouette in Teal piece uses the flametree generative art package to create the tree shown in silhouette in the foreground, and a simple random walk to generate the texture in the background:\n\nSilhouette in Teal\n\nIt has also been surprisingly popular on my Society6 store, which you can visit if you want some of my art on random objects. I am not sure why, but I have sold a lot more shower curtains and yoga mats than I would have expected to sell in my lifetime.\nLeviathan emerged from my first attempt to create simulated watercolours in R using this guide written by Tyler Hobbs. I was in a dark mood at the time and the ominous mood to the piece seems quite fitting to me.\n\nLeviathan\n\nThe Floral Effect piece is an odd one. Itâ€™s part of the Viewports series that I created by applying Thomas Lin Pedersenâ€™s ggfx package over the top of the output of the same system I used to create the Native Flora series, which in turn is an application of the flametree system I mentioned earlier. I quite like it when these systems build on top of one another.\n\nFloral Effect\n\nThe last piece I included, Fire and Ice, is a little different from the others in that itâ€™s not a â€œpureâ€ generative system. It works by reading an image file into R, using Chris Marcumâ€™s halftoner package to convert it to a halftone image, and then manipulate that image using the tools provided in the ambient package. The end result is something that still resembles the original image but has more of a painted feel:\n\nFire and Ice\n\n\n\nLast updated\n2021-09-17 17:01:46 AEST\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-07-08_generative-art-in-r/turmite59-in-red.jpg",
    "last_modified": "2021-09-30T15:08:53+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-19_bs4cards-in-distill/",
    "title": "Bootstrap cards in distill",
    "description": "How to enable bootstrap 4 on a distill website, even though you probably \ndon't need to. I like it though because I get to add pretty bootstrap cards",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-04-19",
    "categories": [],
    "contents": "\n\nContents\nEnabling bootstrap 4\nVanilla R markdown\nPkgdown\nDistill\n\nTesting with pretty pictures\n\n\n\nWhen creating R markdown websites, I often find myself wanting to organise content into a nice-looking grid of links. For example, in a recent project I wanted to be able to create something like this:\n\n\n\n\n\n\n\n\nStarting R markdown\n\nAn introduction to R markdown. The target audience is a novice R user with no previous experience with markdown.\n\n\n\n\n\n\n\nStarting ggplot2\n\nAn introduction to ggplot2. The target audience is a novice user with no previous experience with R or ggplot2.\n\n\n\n\n\n\n\nStarting programming\n\nThis is primarily a tutorial on making generative art in R, but in doing so introduces core programming constructs and data structures. It is assumed the user has some previous experience with ggplot2.\n\n\n\n\n\nIt bothered me that this wasnâ€™t as straightforward as I was expecting, so for one of my side projects Iâ€™ve been putting together a small package called bs4cards to make this a little easier inside an R markdown document or website. There are some introductory articles posted on the bs4cards package website showing how the package works, and thereâ€™s no need to duplicate that content here. However, because this website uses the distill package (Allaire et al. 2021) and the package website is built using pkgdown (Wickham, Hesselberth, and Salmon 2021), it seems like a good idea to have at least one post on both sites that uses bs4cards.\nEnabling bootstrap 4\nThe reason for doing this is that the first step in using the package is to make sure that your R markdown document uses version 4 of bootstrap: the bs4cards package takes its name from the cards system introduced in bootstrap version 4, and will not work properly if used in R markdown documents that rely on bootstrap version 3, or donâ€™t use bootstrap at all. To ensure that you are using bootstrap 4, you need to edit the YAML header for your document to specify which version of bootstrap you want to use. The instructions are slightly different depending on what kind of document youâ€™re creating:\nVanilla R markdown\nFor a plain R markdown document or website (i.e., one where the output format is html_document) here is the relevant section of YAML you might use:\noutput:\n  html_document:\n    theme:\n      version: 4\nThis overrides the R markdown defaults (Xie, Dervieux, and Riederer 2020) to ensure that the output is built using bootstrap 4.5.\nPkgdown\nTo enable bootstrap 4 in a pkgdown site, the process is similar but not identical. Edit the _pkgdown.yml file to include the following\ntemplate:\n  bootstrap: 4\nNote that this relies on a currently-in-development feature, so you may need to update to the development version of pkgdown to make this work.\nDistill\nDistill R markdown does not use bootstrap, which is a little inconvenient if you want to use bs4cards with distill. With a little effort it is possible to enable the entire bootstrap library in a distill site, but this can lead to undesirable side-effects because bootstrap has a lot of styling that doesnâ€™t look visually appealing when mixed with the istill styling. The solution Iâ€™ve adopted for this is to use a custom bootstrap build that includes a minimal number of bootstrap components. If you want to try the same approach, you can download the strapless.css file to the same folder as the distill post you want to enable it for, and include the following YAML in the post header:\noutput:\n  distill::distill_article:\n    css: \"strapless.css\"\nIf you want to enable strapless for the entire site, this markup goes in the _site.yml file and the css file should go in the home folder for the project. Once thatâ€™s done you should be ready to go. That being said, youâ€™d be wise to be careful when adopting this approach: the strapless build is a crude hack, and I havenâ€™t tested it very thoroughly.\nTesting with pretty pictures\nJust to make certain, letâ€™s check that it does what we want by generating cards using the galleries data that comes bundled with the bs4cards package:\n\n\nlibrary(bs4cards)\ngalleries %>% \n  cards(title = long_name, image = image_url)\n\n\n\n\n\n\nAsh Cloud and Blood\n\n\n\n\nGhosts on Marble Paper\n\n\n\n\nIce Floes\n\n\n\n\nNative Flora\n\n\n\n\nSilhouettes\n\n\n\n\nTrack Marks\n\n\n\n\nViewports\n\n\n\n\n\n\nLooks about right to me?\n\n\nLast updated\n2021-09-17 17:03:00 AEST\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\nAllaire, JJ, Rich Iannone, Alison Presmanes Hill, and Yihui Xie. 2021. Distill: â€™R Markdownâ€™ Format for Scientific and Technical Writing. https://CRAN.R-project.org/package=distill.\n\n\nWickham, Hadley, Jay Hesselberth, and MaÃ«lle Salmon. 2021. Pkgdown: Make Static HTML Documentation for a Package.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.\n\n\n\n\n",
    "preview": "posts/2021-04-19_bs4cards-in-distill/bs4cards-logo.png",
    "last_modified": "2021-09-30T15:08:53+10:00",
    "input_file": {},
    "preview_width": 2820,
    "preview_height": 1620
  },
  {
    "path": "posts/2021-04-18_pretty-little-clis/",
    "title": "Pretty little CLIs",
    "description": "How to make a gorgeous command line interface in R using the cli package.\nSomewhere along the way I accidentally learned about ANSI control codes,\nwhich strikes me as unfortunate",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-04-18",
    "categories": [],
    "contents": "\n\nContents\nMeet the cli package\nUsing the status bar\nCreating spinners\nShowing cli messages in R markdown\nWriting longer messages\nCreating structured messages\nEpilogue\n\n\n\n\n\n\n\nLyrics to the title theme of the US TV show, Pretty Little Liars. The song is called Secrets, taken from the fabulous Thirteen Tales of Love and Revenge album by The Pierces\nAnytime you write R code whose output needs to be understood by a human being, it is an act of kindness to spend a little time making sure that the output shown to the human being properly communicates with that human. As a consequence of this, you often find yourself needing to write information to the R console, just to cater to those precious human sensibilities. Perhaps the simplest way to do this is to use the cat() function. Itâ€™s a simple tool and it gets the job done in most cases.\nFor example, consider the use case for the antagonist character â€œAâ€ from Pretty Little Liars, whose stalking and threats were delivered mostly via text message. Had she used R to craft her threatening text messages, she could have written code like this:\n\n\nwait <- function(seconds = 2) {\n  Sys.sleep(seconds)\n}\n\nsend_cat_threat <- function() {\n  cat(\"Dead girls walking.\\n\"); wait()\n  cat(\"--A.\\n\")\n}\n\n\n\nEquipped with a function that specifies her threat, complete with a dramatic pause for effect, sheâ€™s ready to go. When her unwitting victim does something to trigger the send_cat_threat() function, a two part message is displayed on the console. The first part shows up immediately\n\nDead girls walking.\n\nand after a two second delay, her call sign is revealed\n\nDead girls walking.\n--A.\n\nItâ€™s not too difficult to imagine what this message might look like at the R console, but whereâ€™s the fun in that? Thanks to the asciicast package (CsÃ¡rdi et al. 2019), thereâ€™s no need to leave anything to the imagination, and we can see the malevolent message in screencast form:\n\n\n\n\nThe ominous text messages used in this post are taken from Pretty Little Liars. This one is from episode two in season one. Itâ€™s important that one documents ones sources, right?\nUsing cat() to craft messages works perfectly well for simple text communication, but sometimes you want something that looks a little fancier. After all, if the big picture plan here is to impersonate a dead teenager and terrorise her friends - and for some reason youâ€™ve chosen R to do so - you might as well put a little effort into the details, right?\nMeet the cli package\nOne thing I love about the R community is that if you search long enough youâ€™ll find that someone else has already written a package that solves the problem youâ€™re facing. If your problem is â€œhow to craft nicely formatted messagesâ€ then youâ€™ll be delighted to learn that many wonderful things become possible if you have the cli package (CsÃ¡rdi 2021a) as your talented assistant. To craft a beautiful command line interface (CLI) of her very own, the first thing A will need to do is load the package:\n\n\nlibrary(cli)\n\n\n\nOnce this is done, it is a very trivial task for A to write the same threatening text message using cli_text()â€¦\n\n\nsend_cli_threat <- function() {\n  cli_text(\"Dead girls walking.\"); wait()\n  cli_text(\"--A.\")\n}\nsend_cli_threat()\n\n\n\n\n\n\nâ€¦which is nice and all, but it doesnâ€™t make much of a case for using cli. Stalking and threatening is busy work, and Iâ€™d imagine that A would want a more compelling justification before deciding to switch her evil workflow. However - much like A herself - the R console has many dark secrets, and fancier tricks than this are possible once you know how to expose them using cli.\nUsing the status bar\nOne piece of magic that I have wondered about for a long time is how fancy progress bars work: often when youâ€™re doing something that takes a long time, youâ€™ll see an ASCII progress bar rendered in text on the screen, which suddenly vanishes once the process is complete. How exactly does this work? Normally you canâ€™t â€œunprintâ€ a message from the console, so how is it possible for the progress bar to update without leaving an ugly trail of earlier messages behind it?\nWhile teaching myself cli, I found the answer. The most recent line of text generated at the terminal is speciall. Itâ€™s called the status bar: the state of the status bar can be manipulated, and the cli package provides a neat toolkit for doing so. So letâ€™s say I were trying to convince A to switch to the cli tools. Right now, sheâ€™s writing a function that will send a four-part message, using cli_text() because Iâ€™ve at least convinced her to try the new tools:\n\n\nmessage_scroll <- function() {\n  cli_text(\"You found my bracelet.\"); wait()\n  cli_text(\"Now come find me.\"); wait()\n  cli_text(\"Good luck bitches.\"); wait()\n  cli_text(\"-A\"); wait()\n}\nmessage_scroll()\n\n\n\nWhen her victim triggers this message the lines will appear on screen, one after the other with an appropriate dramatic pause separating them. The victim might see something that looks like this:\n\n\n\nThe problem â€“ when viewed from an evil point of view â€“ is that this message stays on screen after delivery.1 The victim has time to think about it, take a screenshot to show her friends, that kind of thing. Wouldnâ€™t the gaslighting be so much more effective if she were to send the message piece by piece, each part disappearing as the next one appears, only to have the whole thing vanish without a trace and leaving the victim wondering if she imagined the whole thing? This is where the status bar comes in handy. Hereâ€™s how it would work:\n\n\nmessage_inline <- function() {\n  id <- cli_status(\"\")\n  cli_status_update(id, \"You found my bracelet.\"); wait()\n  cli_status_update(id, \"Now come find me.\"); wait()\n  cli_status_update(id, \"Good luck bitches.\"); wait()\n  cli_status_update(id, \"-A\"); wait()\n  cli_status_clear(id)\n}\n\n\n\nThe first line in this function uses cli_status() to create a blank message on the status bar, and returns an identifier that refers to the status bar. The next four lines all use cli_status_update() to overwrite the current state of the status bar, and then pause dramatically for two seconds. In a final act of malice, the last line in the function clears the status bar using cli_status_clear(), leaving nothing except a blank space behind. So what the victim sees is something more like this:\n\n\nmessage_inline()\n\n\n\n\n\n\n\nThis message was sent to Aria in episode 10 of season one. Iâ€™m sure it is deeply important to everyone that I mention this.\nCreating spinners\nThe ability to control the status bar opens up a world of new possibilities. Progress bars are one such possibility, but the progress package (CsÃ¡rdi and FitzJohn 2019) already does this nicely, and in any case I suspect that A might be more intrigued by the possibility of spinners, since they just spin and spin and give the victim no clue about when the process is going to end. Much more appealing when the developer doesnâ€™t know (or doesnâ€™t want to reveal) when the wait will end. The cli package has a nice makes_spinner function that serves this purpose. Hereâ€™s an example:\n\n\nspinny <- make_spinner(\n  which = \"dots2\",\n  template = \"{spin} It's not over until I say it is.\"\n)\n\n\n\nThe which argument is used to choose how the spinner would look, and the template argument is used to define how the â€œspinny bitâ€ is placed relative to the rest of the text. The spinny object includes functions to update the state of the spinner (in this case spinny$spin() would be that function), and a function to clear the spinner from the status bar. So hereâ€™s how A might define a function that uses a spinner to keep the victim in suspenseâ€¦\n\n\ntheatrics <- function(which) {\n  \n  # define the spinner\n  spinny <- make_spinner(\n    which = which,\n    template = \"{spin} It's not over until I say it is.\"\n  )\n  \n  # update the spinner 100 times\n  for(i in 1:100) {\n    spinny$spin()\n    wait(.05)\n  }\n  \n  # clear the spinner from the status bar\n  spinny$finish()\n  \n  # send the final part of the message\n  cli_alert_success(\"Sleep tight while you still can, bitches. -A\")\n}\n\n\n\nHereâ€™s what happens:\n\n\ntheatrics(\"dots2\")\n\n\n\n\n\n\n\nThis message was sent to all four of the liars in the final episode of season one. I donâ€™t think A used a spinner though, which feels like a missed opportunity to me\nSetting which = \"dots2\" is only one possibility. There are quite a lot of different spinner types that come bundled with the cli package, and Iâ€™d imagine A would want to look around to see which one suits her needs. Personally, Iâ€™m a fan of hearts:\n\n\ntheatrics(\"hearts\")\n\n\n\n\n\n\nTo see the full list use the list_spinners() function:\n\n\nlist_spinners()\n\n\n [1] \"dots\"                \"dots2\"               \"dots3\"              \n [4] \"dots4\"               \"dots5\"               \"dots6\"              \n [7] \"dots7\"               \"dots8\"               \"dots9\"              \n[10] \"dots10\"              \"dots11\"              \"dots12\"             \n[13] \"line\"                \"line2\"               \"pipe\"               \n[16] \"simpleDots\"          \"simpleDotsScrolling\" \"star\"               \n[19] \"star2\"               \"flip\"                \"hamburger\"          \n[22] \"growVertical\"        \"growHorizontal\"      \"balloon\"            \n[25] \"balloon2\"            \"noise\"               \"bounce\"             \n[28] \"boxBounce\"           \"boxBounce2\"          \"triangle\"           \n[31] \"arc\"                 \"circle\"              \"squareCorners\"      \n[34] \"circleQuarters\"      \"circleHalves\"        \"squish\"             \n[37] \"toggle\"              \"toggle2\"             \"toggle3\"            \n[40] \"toggle4\"             \"toggle5\"             \"toggle6\"            \n[43] \"toggle7\"             \"toggle8\"             \"toggle9\"            \n[46] \"toggle10\"            \"toggle11\"            \"toggle12\"           \n[49] \"toggle13\"            \"arrow\"               \"arrow2\"             \n[52] \"arrow3\"              \"bouncingBar\"         \"bouncingBall\"       \n[55] \"smiley\"              \"monkey\"              \"hearts\"             \n[58] \"clock\"               \"earth\"               \"moon\"               \n[61] \"runner\"              \"pong\"                \"shark\"              \n[64] \"dqpb\"               \n\nShowing cli messages in R markdown\nThroughout this post Iâ€™ve relied on asciicast to display screencasts of the R console as animated SVG files, rather than what I might normally do and rely on regular R markdown code chunks to do the work. Thereâ€™s a reason for this: the R console is a terminal, and its behaviour doesnâ€™t always translate nicely to HTML. Part of the magic of the rmarkdown package (Xie, Allaire, and Grolemund 2018) is that most of the time it is able to capture terminal output and translate it seamlessly into HTML, and we mere mortal users never notice how clever this is. However, when dealing with cli output, we run into cases where this breaks down and the law of leaky abstractions comes into play: text generated at the R console does not follow the same rules as text inserted into an HTML document, and R Markdown sometimes needs a little help when transforming one to the other.\nAn important thing to remember about cli is that the text it produces is a message, so its visibility in R Markdown depends on the chunk option for messages. As long as the message option is set to TRUE, R Markdown will include them as part of the output.2 In the simplest case, R Markdown works nicely, so as long as all A wants to do is send an unformatted threat within an R Markdown document, then this works:\n\n\ncli_text(\"I'm still here bitches, and I know everything. -A\")\n\n\nI'm still here bitches, and I know everything. -A\n\nHowever, the moment A tries to use any fancy formatting, things will go haywire for her. For example, suppose she wanted to send the message above as a simple â€œalertâ€ message using cli_alert(), which uses fancy symbols and colours in the output. It is at this point that the cracks in the R Markdown pipeline start to leak. In this case, the leak would result in the document failing to knit and an error message complaining about\nPCDATA invalid Char value\nIntuitively she might guess that somewhere in the R Markdown pipeline, an invalid or malformed character has been created.3 The reason this happens is that the colours and symbols used by cli, and supported in the R console, rely on ANSI escape codes, but those escape codes arenâ€™t recognised in HTML and â€“ apparently â€“ they can wreak havoc when R markdown writes those characters into the HTML document. ANSI colours in R are usually generated with the help of the crayon package (CsÃ¡rdi 2021b), and per the issue #24 thread that I encounter on a semi-regular basis, it can be tricky to manage the process of translating these to HTML via R Markdown.\nSolving this issue requires A to jump through a few hoops. Itâ€™s annoying I know, but no-one ever said that running an unhinged stalking campaign via text messages was easy, right? Her first task is to make sure that the R Markdown document turns on crayon support:\n\n\noptions(crayon.enabled = TRUE)\n\n\n\nThis isnâ€™t the whole solution, however, because while that tells R Markdown to stop ignoring all the ANSI stuff, it doesnâ€™t necessarily allow it to render ANSI sequences properly. To fix this she needs to specify the knit hooks that explicitly tell R Markdown what to do. She can do this with the help of the fansi package (Gaslam 2021), which contains an obscurely-named function sgr_to_html() that translates a subset of the ANSI control sequences to HTML, and strips out all the others. Using this, she can write an ansi_aware_handler() function that will take an input string x and return HTML output appropriate for the R Markdown context:\n\n\nansi_aware_handler <- function(x, options) {\n  paste0(\n    \"<pre class=\\\"r-output\\\"><code>\",\n    fansi::sgr_to_html(x = x, warn = FALSE, term.cap = \"256\"),\n    \"<\/code><\/pre>\"\n  )\n}\n\n\n\nFrom there, itâ€™s relatively easy. All she needs to do is tell knitr (Xie 2021) to use this function whenever it needs to handle output. Just for good measure she might do the same for messages, errors, and warnings:\n\n\nknitr::knit_hooks$set(\n  output = ansi_aware_handler, \n  message = ansi_aware_handler, \n  warning = ansi_aware_handler,\n  error = ansi_aware_handler\n)\n\n\n\nAt long last she is done.4 Her campaign of bullying and cruelty can continue:\n\n\ncli_alert(\"I'm still here bitches, and I know everything. -A\")\n\n\nâ†’ I'm still here bitches, and I know everything. -A\n\n\n\nThis message was sent in the pilot episode. Yes, the quotes Iâ€™ve used are all from season one: Iâ€™ve just started a rewatch of the show, so the early episodes are quite fresh in my memory!\nWriting longer messages\nUp to this point the threatening messages that A has been sending have been short, only one line long. In several cases the messages have been cleverly constructed so that the same line (the status bar) is used to display multiple pieces of text, but ultimately itâ€™s still one line messaging. A needs to take a little care when she wants to branch out. Conceptually, a message should correspond to â€œone semantically meaningful bundle of informationâ€ that might be split over several lines. However, as far as R is concerned, each call to cli_text() creates a distinct message. To see how this might cause A some grief, hereâ€™s the letter that she sent to Ariaâ€™s mother announcing the infidelity of Ariaâ€™s father:\n\n\nsend_cruel_letter_piecewise <- function() {\n  cli_text('Your husband, Byron, is involved with another woman')\n  cli_text('and when I say involved I mean in a \"romantic\" way.')\n  cli_text('This is not something recent. It started before your')\n  cli_text('family went away to Iceland and from the look of')\n  cli_text('things, it may be starting up again now that you\\'re')\n  cli_text('back. I know this is hard to hear, but it is the')\n  cli_text('truth. If you don\\'t believe this about your husband,')\n  cli_text('ask your daughter. She knows all about it.')\n  cli_text('Sincerely,')\n  cli_text('A')\n}\n\nsend_cruel_letter_piecewise()\n\n\nYour husband, Byron, is involved with another woman\n\nand when I say involved I mean in a \"romantic\" way.\n\nThis is not something recent. It started before your\n\nfamily went away to Iceland and from the look of\n\nthings, it may be starting up again now that you're\n\nback. I know this is hard to hear, but it is the\n\ntruth. If you don't believe this about your husband,\n\nask your daughter. She knows all about it.\n\nSincerely,\n\nA\n\n\nThis is not an ideal implementation. What A wants to send is one message spanning 10 lines not 10 separate one-line messages, but itâ€™s the latter that she has actually implemented here. This is where the cli() function is handy: to takes an expression as input and collects all the constituent parts into a single message. This version of the function now sends a single message:\n\n\nsend_cruel_letter_singly <- function() {\n  cli({\n    cli_text('Your husband, Byron, is involved with another woman')\n    cli_text('and when I say involved I mean in a \"romantic\" way.')\n    cli_text('This is not something recent. It started before your')\n    cli_text('family went away to Iceland and from the look of')\n    cli_text('things, it may be starting up again now that you\\'re')\n    cli_text('back. I know this is hard to hear, but it is the')\n    cli_text('truth. If you don\\'t believe this about your husband,')\n    cli_text('ask your daughter. She knows all about it.')\n    cli_text('Sincerely,')\n    cli_text('A')\n  })\n}\n\nsend_cruel_letter_singly()\n\n\nYour husband, Byron, is involved with another woman\nand when I say involved I mean in a \"romantic\" way.\nThis is not something recent. It started before your\nfamily went away to Iceland and from the look of\nthings, it may be starting up again now that you're\nback. I know this is hard to hear, but it is the\ntruth. If you don't believe this about your husband,\nask your daughter. She knows all about it.\nSincerely,\nA\n\n\n\nThe letter was sent to Ella in episode four season one. Even on a rewatch Iâ€™m finding it impossible to imagine Holly Marie Combs as anyone other than Piper from Charmed and I keep expecting â€œEllaâ€ to stop time and, idk, shave off her husbands eyebrows or something?\nMuch nicer. As every would-be tormenter knows, itâ€™s important to pay attention to the details.\nCreating structured messages\nWriting long messages when sending a threatening letter is a simple enough thing, but at some point A will likely find herself wanting to add some structure to these missives. Lists are nice. Stalkers like keeping lists, I hear. With that in mind, a nice property of cli is that it allows you to separate style from structure using an HTML-like syntax. Top level headings are specified using cli_h1(), and second level headings are produced by cli_h2(). Unordered lists are produced using cli_ul() and ordered lists by cli_ol(). This make it easy to write structured messages to the R console:\n\n\ncli({\n  cli_h1(\"Characters\")\n  cli_h2(\"The Liars\")\n  cli_ul(c(\n    \"Alison DiLaurentis\",\n    \"Spencer Hastings\",\n    \"Aria Montgomery\",\n    \"Hanna Marin\",\n    \"Emily Fields\"\n  ))\n  cli_h2(\"The A-Team\")\n  cli_ul(c(\n    \"Mona Vanderwaal\",\n    \"Lucas Gottesman\",\n    \"Melissa Hastings\"\n  ))\n})\n\n\n\nâ”€â”€ Characters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nâ”€â”€ The Liars â”€â”€\n\nâ€¢ Alison DiLaurentis\nâ€¢ Spencer Hastings\nâ€¢ Aria Montgomery\nâ€¢ Hanna Marin\nâ€¢ Emily Fields\n\nâ”€â”€ The A-Team â”€â”€\n\nâ€¢ Mona Vanderwaal\nâ€¢ Lucas Gottesman\nâ€¢ Melissa Hastings\n\n\nBetter yet, the cli package has a whole swathe of other utilities that follow this same HTML-like naming scheme, making it possible to send elaborate and disturbing messages in so many different ways.\nEpilogue\nThere is a lot more to the cli package that I havenâ€™t talked about in this post. Iâ€™ve not talked about how to modify the themes, how to create custom cli â€œappsâ€ that use different themes or send output to different connections. Iâ€™ve not talked about how to use conditional logic within a cli call, displaying different messages depending on whether a process succeeds or fails. Those will have to remain secret for now, because this post is quite long enough already and quite frankly Iâ€™m still learning myself. Besides, these powers would no doubt would be put to terrible purposes in an R-themed Pretty Little Liars spinoff show, and Iâ€™m not entirely sure that all secrets need sharingâ€¦\n\n\ncli(\n  cli_blockquote(\n    quote = \"Friends share secrets, that's what keeps us close\",\n    citation = \"Alison\"\n  )\n)\n\n\n\n    â€œFriends share secrets, that's what keeps us closeâ€\n    â€” Alison\n\n\n\n\nLast updated\n2021-09-17 17:03:57 AEST\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\nCsÃ¡rdi, GÃ¡bor. 2021a. Cli: Helpers for Developing Command Line Interfaces. https://CRAN.R-project.org/package=cli.\n\n\nâ€”â€”â€”. 2021b. Crayon: Colored Terminal Output. https://CRAN.R-project.org/package=crayon.\n\n\nCsÃ¡rdi, GÃ¡bor, and Rich FitzJohn. 2019. Progress: Terminal Progress Bars. https://CRAN.R-project.org/package=progress.\n\n\nCsÃ¡rdi, GÃ¡bor, Romain Francois, Mario Nebl, and Marcin Kulik. 2019. Asciicast: Create â€™Asciiâ€™ Screen Casts from r Scripts. https://CRAN.R-project.org/package=asciicast.\n\n\nGaslam, Brodie. 2021. Fansi: ANSI Control Sequence Aware String Functions. https://CRAN.R-project.org/package=fansi.\n\n\nXie, Yihui. 2021. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown.\n\n\nYes, it does disappear in this screencast, but thatâ€™s just the screencast. If it were the R console it would stay on screen the whole time.â†©ï¸\nSomewhat counterintuitively, although cli emits messages that can be suppressed by suppressMessages(), they donâ€™t behave precisely the same as the messages produced by message(). The default handler for base R messages sends the output to the stderr() connection and so they are often shown as the dreaded â€œred textâ€ that users learn to fear. To avoid this, the default behaviour in cli sends messages to the stdout() connection, thereby avoiding this issue. However, cli does allow you to control this behaviour: see the start_app() and stop_app() functions for more information.â†©ï¸\nAs an aside, if youâ€™re running a site with an RSS feed it may also write malformed characters into the index.xml file as well as any generated .html file. When I encountered this problem I found that even when I â€œfixedâ€ my .Rmd file the document wouldnâ€™t re-knit, because of the problems with the xml file. Eventually I realised that I could solve the problem by deleting the index.xml file for the RSS feed and then knitting again. Sighâ†©ï¸\nNote that there is also the fansi::set_knit_hooks() function which will set the hooks in a more user-friendly way. I donâ€™t think thereâ€™s any reason not to use it: the only reason I didnâ€™t is that I found it convenient to write things from scratch here so that I understood what was happening.â†©ï¸\n",
    "preview": "posts/2021-04-18_pretty-little-clis/pretty-little-clis.jpg",
    "last_modified": "2021-09-30T15:08:53+10:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-05_welcome/",
    "title": "Welcome to the jungle",
    "description": "I have reluctantly decided to create a new blog. Some thoughts on \nwhat I hope to achieve, having tried my hand at blogging so very many times \nbefore",
    "author": [
      {
        "name": "Danielle Navarro",
        "url": "https://djnavarro.net"
      }
    ],
    "date": "2021-04-05",
    "categories": [],
    "contents": "\n\n\nIâ€™ve decided the time has come to restart my blog. Iâ€™ve tried blogging many times before with mixed success, and this time around Iâ€™d like to avoid the mistakes of the past. Iâ€™ve set up this blog with a few principles in mind:\nSimplicity. One mistake Iâ€™ve often made is to create blogs using the fanciest tools I could find. For example, Iâ€™ve previously used Hugo based packages like blogdown and hugodown, and much as I love those tools (and use them on other sites) I want this blog to be as low-maintenance as possible. To that end Iâ€™m using distill for R markdown, and Iâ€™m keeping the default settings in most respects.\nEncapsulation. There was a time when I really liked the idea of having my blog integrated nicely with my homepage (djnavarro.net). Iâ€™ve become less keen on this because the aesthetic and technical demands of a blog arenâ€™t always aligned with the needs of my homepage. This time Iâ€™ve set it up so that the blog.djnavarro.net subdomain corresponds to a different repository from my homepage. Iâ€™m hoping this will make blogging simpler from a technical standpoint.\nFocus. Another mistake I have made in the past is letting blogs â€œsprawlâ€, mixing personal essays with technical posts. My intention with this blog is to write technical posts only, mostly on R and data science. Iâ€™ve moved my personal writing to essays.djnavarro.net and my artwork to art.djnavarro.net. My hope is that this will make blogging easier from an emotional standpoint.\nReproducibility. A frustration Iâ€™ve had with my previous blogs is that my posts were not particularly reproducible. Source code was often missing, information about the R session was not provided, and so on. This time, Iâ€™ve set up the blog so that there is a â€œdetailsâ€ section at the bottom of each post containing links to the source code, the R session information, and a lockfile generated using renv::snapshot(). (Edit: from 2021-08-23 Iâ€™ve extended this approach so that every post actually uses the previously stored R environment)\nAt this stage Iâ€™m not entirely certain how Iâ€™ll use the blog. There are a lot of possibilities, and I have some thoughts on which ones Iâ€™d like to explore. A self-contained blog such as this seems nicely suited to teaching materials. An obvious example would be to write blog posts to accompany the data science slides and youtube videos Iâ€™ve been making. The lack of written material to go with those talks has bothered me for some time. Another possibility might be to write tutorials on generative art. I use my art website to post the art itself, but the site functions as a gallery rather than a classroom. I get a lot of people asking questions about how I make my art, and this blog might be a good place to provide answers. Those arenâ€™t the only possibilities, of course, but they are appealing ones.\nNot sure how this will go, but fingers crossed!\n\n\nLast updated\n2021-09-17 17:05:09 AEST\nDetails\nsource code, R environment\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-04-05_welcome/welcome.jpg",
    "last_modified": "2021-09-30T15:08:53+10:00",
    "input_file": {}
  }
]
